{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Pytorch] *",
      "language": "python",
      "name": "conda-env-Pytorch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "276.108px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "347.713px",
        "left": "1100.43px",
        "right": "20px",
        "top": "67.9716px",
        "width": "601.008px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "RoBERTa_TextClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douzujun/Discourse-Relation-Code/blob/main/RoBERTa_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Mid_hU_N2p",
        "outputId": "a118318a-9956-4231-d99e-1d2f0c5ca3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "# 安装必要的包\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozb-d52U_RV0"
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjd1NiuX_RrX"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/Colab/RoBERTa/')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUM-I3SX_rCm",
        "outputId": "f525aed5-baf4-4cb7-8fe3-abfb00a31672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Colab/RoBERTa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeV8sOJe_zvh",
        "outputId": "fc16a8d6-fdf5-480b-d309-3c428e732643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:12.009763Z",
          "start_time": "2020-10-23T04:35:09.466749Z"
        },
        "id": "HkX0c7jp_Ch3",
        "outputId": "a4c21808-b0fa-4203-a4d4-9602f3c6433c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pylab as plt \n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F \n",
        "\n",
        "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from transformers import RobertaModel, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "SEED = 123\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "hidden_dropout_prob = 0.2\n",
        "learning_rate = 5e-6\n",
        "weight_decay = 1e-2\n",
        "epsilon = 1e-8\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zmaOHL_CiH"
      },
      "source": [
        "# 预处理数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdxjASf0_CiI"
      },
      "source": [
        "## 读取文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:25.290387Z",
          "start_time": "2020-10-23T04:35:25.175651Z"
        },
        "id": "f8uAVWpq_CiJ"
      },
      "source": [
        "train = pd.read_csv('./pdtb3/train.tsv', sep='\\t')\n",
        "dev = pd.read_csv('./pdtb3/dev.tsv', sep='\\t')\n",
        "test = pd.read_csv('./pdtb3/test.tsv', sep='\\t') "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:25.602713Z",
          "start_time": "2020-10-23T04:35:25.577583Z"
        },
        "id": "rB2smbzR_CiT",
        "outputId": "d5678eda-4a69-4c68-d8a1-e51728083e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>split</th>\n",
              "      <th>section</th>\n",
              "      <th>file_number</th>\n",
              "      <th>label</th>\n",
              "      <th>category</th>\n",
              "      <th>arg1</th>\n",
              "      <th>arg2</th>\n",
              "      <th>conn</th>\n",
              "      <th>full_sense</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>2</td>\n",
              "      <td>wsj_0288</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>The two leaders will meet on Dec. 2 and 3,</td>\n",
              "      <td>alternating the two days of meetings between a...</td>\n",
              "      <td>in particular</td>\n",
              "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>2</td>\n",
              "      <td>wsj_0288</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>Despite the informal nature of the session and...</td>\n",
              "      <td>Mr. Gorbachev badly needs a diversion from the...</td>\n",
              "      <td>because</td>\n",
              "      <td>Contingency.Cause.Reason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "      <td>2</td>\n",
              "      <td>wsj_0288</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>Mr. Gorbachev badly needs a diversion from the...</td>\n",
              "      <td>that a meeting with the leader of the U.S. cou...</td>\n",
              "      <td>thus</td>\n",
              "      <td>Contingency.Cause.Result</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>train</td>\n",
              "      <td>2</td>\n",
              "      <td>wsj_0288</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>For his part, Mr. Bush has been criticized reg...</td>\n",
              "      <td>A face-to-face meeting with Mr. Gorbachev shou...</td>\n",
              "      <td>so</td>\n",
              "      <td>Contingency.Cause.Result</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>train</td>\n",
              "      <td>2</td>\n",
              "      <td>wsj_0288</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>that the December meeting, will be held in the...</td>\n",
              "      <td>to hold down the \"fanfare\" and force the two s...</td>\n",
              "      <td>in order</td>\n",
              "      <td>Contingency.Purpose.Arg2-as-goal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx  split  ...           conn                                full_sense\n",
              "0    0  train  ...  in particular  Expansion.Level-of-detail.Arg2-as-detail\n",
              "1    1  train  ...        because                  Contingency.Cause.Reason\n",
              "2    2  train  ...           thus                  Contingency.Cause.Result\n",
              "3    3  train  ...             so                  Contingency.Cause.Result\n",
              "4    4  train  ...       in order          Contingency.Purpose.Arg2-as-goal\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:26.120157Z",
          "start_time": "2020-10-23T04:35:26.094745Z"
        },
        "id": "_2U4jtv-_Cic",
        "outputId": "b153aaeb-80a1-4628-9621-9bff246b006b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>split</th>\n",
              "      <th>section</th>\n",
              "      <th>file_number</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>category</th>\n",
              "      <th>arg1</th>\n",
              "      <th>arg2</th>\n",
              "      <th>conn1</th>\n",
              "      <th>full_sense1</th>\n",
              "      <th>conn2</th>\n",
              "      <th>full_sense2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>dev</td>\n",
              "      <td>0</td>\n",
              "      <td>wsj_0068</td>\n",
              "      <td>Comparison</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>The Kearny, N.J.-based maker of hair accessori...</td>\n",
              "      <td>In the year-ago quarter, the company reported ...</td>\n",
              "      <td>by contrast</td>\n",
              "      <td>Comparison.Contrast</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dev</td>\n",
              "      <td>0</td>\n",
              "      <td>wsj_0018</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>Not only is development of the new company's i...</td>\n",
              "      <td>that Cray Research Inc. will withdraw the almo...</td>\n",
              "      <td>specifically</td>\n",
              "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>dev</td>\n",
              "      <td>0</td>\n",
              "      <td>wsj_0018</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>that Seymour is the chief designer of the Cray-3,</td>\n",
              "      <td>and without him it could not be completed.</td>\n",
              "      <td>as a result</td>\n",
              "      <td>Contingency.Cause.Result</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>dev</td>\n",
              "      <td>0</td>\n",
              "      <td>wsj_0018</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>and without him it could not be completed.</td>\n",
              "      <td>Cray Research did not want to fund a project t...</td>\n",
              "      <td>so</td>\n",
              "      <td>Contingency.Cause.Result</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>dev</td>\n",
              "      <td>0</td>\n",
              "      <td>wsj_0018</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>that Cray Research's decision to link its $98....</td>\n",
              "      <td>It has to be considered as an additional risk ...</td>\n",
              "      <td>in fact</td>\n",
              "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx split  ...  conn2 full_sense2\n",
              "0    0   dev  ...   None        None\n",
              "1    1   dev  ...   None        None\n",
              "2    2   dev  ...   None        None\n",
              "3    3   dev  ...   None        None\n",
              "4    4   dev  ...   None        None\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:26.434451Z",
          "start_time": "2020-10-23T04:35:26.417480Z"
        },
        "id": "_e0lhhjt_Cil",
        "outputId": "4911ece6-8e27-4157-a74b-7c1369e4ec58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>split</th>\n",
              "      <th>section</th>\n",
              "      <th>file_number</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>category</th>\n",
              "      <th>arg1</th>\n",
              "      <th>arg2</th>\n",
              "      <th>conn1</th>\n",
              "      <th>full_sense1</th>\n",
              "      <th>conn2</th>\n",
              "      <th>full_sense2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>21</td>\n",
              "      <td>wsj_2131</td>\n",
              "      <td>Comparison</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>Among other things, the restructured facilitie...</td>\n",
              "      <td>Certain details of the restructured facilities...</td>\n",
              "      <td>although</td>\n",
              "      <td>Comparison.Concession.Arg2-as-denier</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>21</td>\n",
              "      <td>wsj_2131</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>Certain details of the restructured facilities...</td>\n",
              "      <td>The agreement is subject to completion of a de...</td>\n",
              "      <td>also</td>\n",
              "      <td>Expansion.Conjunction</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>21</td>\n",
              "      <td>wsj_2140</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>They are enjoying domestic sales that are more...</td>\n",
              "      <td>South Korean consumers are expected to buy alm...</td>\n",
              "      <td>in fact</td>\n",
              "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>21</td>\n",
              "      <td>wsj_2140</td>\n",
              "      <td>Contingency</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>that slackened demand for their cars in the U....</td>\n",
              "      <td>otherwise they wouldn't be able to keep up wit...</td>\n",
              "      <td>because</td>\n",
              "      <td>Contingency.Cause.Reason</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>21</td>\n",
              "      <td>wsj_2140</td>\n",
              "      <td>Expansion</td>\n",
              "      <td>None</td>\n",
              "      <td>Implicit</td>\n",
              "      <td>As it is, waiting lists of a month aren't unus...</td>\n",
              "      <td>Demand is so strong that all of the domestic m...</td>\n",
              "      <td>in fact</td>\n",
              "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx split  ...  conn2 full_sense2\n",
              "0    0  test  ...   None        None\n",
              "1    1  test  ...   None        None\n",
              "2    2  test  ...   None        None\n",
              "3    3  test  ...   None        None\n",
              "4    4  test  ...   None        None\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:26.776932Z",
          "start_time": "2020-10-23T04:35:26.731232Z"
        },
        "id": "Y1ONgmtz_Ciu",
        "outputId": "c47e377f-2671-463d-d91e-94b9e7cfe656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# ============================ 设置标签 ================================\n",
        "def set_train_labels(data_set):\n",
        "    data_set['multi_relation'] = None\n",
        "    \n",
        "    comp_idx = data_set[data_set['label'] == 'Comparison'].index    # Comparison:  0\n",
        "    exp_idx = data_set[data_set['label'] == 'Expansion'].index      # Expansion:   1\n",
        "    cont_idx = data_set[data_set['label'] == 'Contingency'].index   # Contingency: 2\n",
        "    temp_idx = data_set[data_set['label'] == 'Temporal'].index      # Temporal:    3\n",
        "\n",
        "    data_set.loc[:, 'multi_relation'].iloc[comp_idx] = 0\n",
        "    data_set.loc[:, 'multi_relation'].iloc[exp_idx] = 1 \n",
        "    data_set.loc[:, 'multi_relation'].iloc[cont_idx] = 2\n",
        "    data_set.loc[:, 'multi_relation'].iloc[temp_idx] = 3 \n",
        "\n",
        "def set_dev_or_test_labels(data_set):\n",
        "    data_set['multi_relation'] = None\n",
        "    \n",
        "    comp_idx = data_set[data_set['label1'] == 'Comparison'].index    # Comparison:  0\n",
        "    exp_idx = data_set[data_set['label1'] == 'Expansion'].index      # Expansion:   1\n",
        "    cont_idx = data_set[data_set['label1'] == 'Contingency'].index   # Contingency: 2\n",
        "    temp_idx = data_set[data_set['label1'] == 'Temporal'].index      # Temporal:    3\n",
        "\n",
        "    data_set.loc[:, 'multi_relation'].iloc[comp_idx] = 0\n",
        "    data_set.loc[:, 'multi_relation'].iloc[exp_idx] = 1 \n",
        "    data_set.loc[:, 'multi_relation'].iloc[cont_idx] = 2\n",
        "    data_set.loc[:, 'multi_relation'].iloc[temp_idx] = 3 \n",
        "    \n",
        "# train\n",
        "set_train_labels(train)\n",
        "# dev\n",
        "set_dev_or_test_labels(dev)\n",
        "# test\n",
        "set_dev_or_test_labels(test)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:27.026817Z",
          "start_time": "2020-10-23T04:35:27.021035Z"
        },
        "id": "WcMDw2Gc_Ci2",
        "outputId": "aaa0baa6-dcfb-4fb3-fb28-c002b8fd64eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Example: 四分类，把标签0,1,2,3 --> 换成 one-hot\n",
        "F.one_hot(torch.LongTensor(test['multi_relation']), num_classes = 4)[:10, :]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0],\n",
              "        [0, 1, 0, 0],\n",
              "        [0, 1, 0, 0],\n",
              "        [0, 0, 1, 0],\n",
              "        [0, 1, 0, 0],\n",
              "        [0, 1, 0, 0],\n",
              "        [0, 0, 1, 0],\n",
              "        [0, 0, 0, 1],\n",
              "        [0, 1, 0, 0],\n",
              "        [0, 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-21T12:38:34.539371Z",
          "start_time": "2020-10-21T12:38:34.497357Z"
        },
        "id": "HAmND512_Ci_"
      },
      "source": [
        "## BertTokenizer进行编码，将每一句转成数字"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VdP54VL_CjA"
      },
      "source": [
        "- 例子：\n",
        "\n",
        "```python\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name, cache_dir = cache_dir)\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name, return_dict=True)\n",
        "\n",
        "inputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\n",
        "labels = torch.tensor([1]).unsqueeze(0)   # batch size 1, [1, 1]\n",
        "outputs = model(**inputs, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:31.298787Z",
          "start_time": "2020-10-23T04:35:28.125223Z"
        },
        "id": "KarfgaDI_CjC"
      },
      "source": [
        "model_name = 'roberta-base'\n",
        "cache_dir = './Transformer-Bert-Cache/'\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name, cache_dir = cache_dir)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:31.550453Z",
          "start_time": "2020-10-23T04:35:31.542625Z"
        },
        "id": "3rFLWGO4_CjI"
      },
      "source": [
        "train_arg1, train_arg2, train_label = train['arg1'], train['arg2'], torch.LongTensor(train['multi_relation'])\n",
        "\n",
        "dev_arg1, dev_arg2, dev_label = dev['arg1'], dev['arg2'], torch.LongTensor(dev['multi_relation'])\n",
        "\n",
        "test_arg1, test_arg2, test_label = test['arg1'], test['arg2'], torch.LongTensor(test['multi_relation'])"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:31.817115Z",
          "start_time": "2020-10-23T04:35:31.813346Z"
        },
        "id": "EHhe8JKS_CjO",
        "outputId": "3bd67621-18c1-44cf-a07b-b692ea48ad60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_label"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 2,  ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:32.090563Z",
          "start_time": "2020-10-23T04:35:32.088496Z"
        },
        "id": "2YxOLY0l_CjW"
      },
      "source": [
        "# ===================== Test =========================\n",
        "# t_arg1, t_arg2 = train_arg1[0], train_arg2[0]\n",
        "# print(tokenizer.tokenize(t_arg1))\n",
        "# print(tokenizer.tokenize(t_arg2))\n",
        "# print(tokenizer.encode(t_arg1))\n",
        "# print(tokenizer.encode_plus(t_arg1, t_arg2))\n",
        "# print(tokenizer.convert_ids_to_tokens(tokenizer.encode(t_arg1)))\n",
        "# print(tokenizer.convert_ids_to_tokens(tokenizer.encode_plus(t_arg1, t_arg2)['input_ids']))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:32.348955Z",
          "start_time": "2020-10-23T04:35:32.345110Z"
        },
        "id": "IVG26Wbp_Cjh"
      },
      "source": [
        "# BertTokenizer进行编码，将每一句转成数字，长度不够补0\n",
        "def convert_text_to_token(tokenizer, arg1, arg2, limit_size = 126):\n",
        "    tokens = tokenizer.encode_plus(arg1[:limit_size], arg2[:limit_size])['input_ids']\n",
        "    if len(tokens) < limit_size*2 + 4:\n",
        "        tokens.extend([0] * (limit_size*2 + 4 - len(tokens)))\n",
        "    return tokens"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G8DC6Xl_Cjp"
      },
      "source": [
        "## attention_masks, 在一个文本中，如果是PAD符号则是0，否则就是1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:32.594431Z",
          "start_time": "2020-10-23T04:35:32.591333Z"
        },
        "id": "xFGiNA5e_Cjr"
      },
      "source": [
        "def attention_masks(input_ids):\n",
        "    atten_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        atten_masks.append(seq_mask)\n",
        "    return atten_masks"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:32.883018Z",
          "start_time": "2020-10-23T04:35:32.879112Z"
        },
        "id": "vC5LEX-h_Cj0"
      },
      "source": [
        "def get_inputTokens_and_attentionTokens(data_arg1, data_arg2, data_type):\n",
        "    slen = len(data_arg1)\n",
        "    input_ids = [convert_text_to_token(tokenizer, data_arg1[i], data_arg2[i]) for i in range(slen)]\n",
        "    input_tokens = torch.tensor(input_ids)\n",
        "    print(data_type, ':', input_tokens.shape)\n",
        "    \n",
        "    # attention_tokens\n",
        "    atten_masks = attention_masks(input_ids)\n",
        "    attention_tokens = torch.tensor(atten_masks)\n",
        "    print(data_type, ':', attention_tokens.shape)\n",
        "    \n",
        "    return input_tokens, attention_tokens"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:47.360559Z",
          "start_time": "2020-10-23T04:35:33.178384Z"
        },
        "id": "0ibl5WDO_Cj9",
        "outputId": "a164defd-cd02-4663-8af1-eda17ccfb8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# =================== train =====================\n",
        "# train\n",
        "train_input_tokens, train_attention_tokens = get_inputTokens_and_attentionTokens(train_arg1, train_arg2, train['split'][0])\n",
        "# label\n",
        "print('train: ', train_label.shape)\n",
        "\n",
        "# =================== dev ===================\n",
        "dev_input_tokens, dev_attention_tokens = get_inputTokens_and_attentionTokens(dev_arg1, dev_arg2, dev['split'][0])\n",
        "# label\n",
        "print('dev: ', dev_label.shape)\n",
        "\n",
        "# ==================== test ==================\n",
        "test_input_tokens, test_attention_tokens = get_inputTokens_and_attentionTokens(test_arg1, test_arg2, test['split'][0])\n",
        "# label\n",
        "print('test: ', test_label.shape)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : torch.Size([17867, 256])\n",
            "train : torch.Size([17867, 256])\n",
            "train:  torch.Size([17867])\n",
            "dev : torch.Size([1653, 256])\n",
            "dev : torch.Size([1653, 256])\n",
            "dev:  torch.Size([1653])\n",
            "test : torch.Size([1474, 256])\n",
            "test : torch.Size([1474, 256])\n",
            "test:  torch.Size([1474])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:47.654659Z",
          "start_time": "2020-10-23T04:35:47.646575Z"
        },
        "scrolled": true,
        "id": "2rcOL3fY_CkE",
        "outputId": "df4c77e5-39c3-4a14-bb81-23d702c4ce37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "# ================ Test ======================\n",
        "print(tokenizer.tokenize(train_arg1[0]))\n",
        "print(tokenizer.tokenize(train_arg2[0]))\n",
        "print(tokenizer.encode_plus(train_arg1[0], train_arg2[0])['input_ids'])\n",
        "print(train_attention_tokens[0], len(train_attention_tokens[0]))\n",
        "\n",
        "print(train_label[0])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Ġtwo', 'Ġleaders', 'Ġwill', 'Ġmeet', 'Ġon', 'ĠDec', '.', 'Ġ2', 'Ġand', 'Ġ3', ',']\n",
            "['altern', 'ating', 'Ġthe', 'Ġtwo', 'Ġdays', 'Ġof', 'Ġmeetings', 'Ġbetween', 'Ġa', 'ĠU', '.', 'S', '.', 'Ġand', 'Ġa', 'ĠSoviet', 'Ġnaval', 'Ġvessel', 'Ġin', 'Ġthe', 'ĠMediterranean', 'ĠSea', '.']\n",
            "[0, 133, 80, 917, 40, 972, 15, 1502, 4, 132, 8, 155, 6, 2, 2, 34268, 1295, 5, 80, 360, 9, 2891, 227, 10, 121, 4, 104, 4, 8, 10, 8297, 15272, 9190, 11, 5, 11965, 3939, 4, 2]\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.]) 256\n",
            "tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLgCJz4W_CkJ"
      },
      "source": [
        "## 创建DataLoader，用来取出一个batch的数据\n",
        "\n",
        "- TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。<br>\n",
        "\n",
        "- 该类通过每一个 tensor 的第一个维度进行索引，所以该类中的 tensor 第一维度必须相等，且TensorDataset 中的参数必须是 tensor类型。<br>\n",
        "\n",
        "- RandomSampler：对数据集随机采样。<br>\n",
        "\n",
        "- SequentialSampler：按顺序对数据集采样。<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:48.008553Z",
          "start_time": "2020-10-23T04:35:48.001638Z"
        },
        "id": "T-BLX77Z_CkL"
      },
      "source": [
        "train_data = TensorDataset(train_input_tokens, train_attention_tokens, train_label)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "dev_data = TensorDataset(dev_input_tokens, dev_attention_tokens, dev_label)\n",
        "dev_sampler = RandomSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(test_input_tokens, test_attention_tokens, test_label)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:35:48.323759Z",
          "start_time": "2020-10-23T04:35:48.317524Z"
        },
        "id": "y6TwB0ES_CkU",
        "outputId": "fbd49abf-dd38-4e32-837c-53d99b9cb1a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# ================ Test =====================\n",
        "for i, (train, mask, label) in enumerate(train_dataloader): \n",
        "    # torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 1])\n",
        "    print(train.shape, mask.shape, label.shape)\n",
        "    break\n",
        "    \n",
        "print('len(train_dataloader) = ', len(train_dataloader))    # 1117"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 256]) torch.Size([8, 256]) torch.Size([8])\n",
            "len(train_dataloader) =  2234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlEmczK__Ckg"
      },
      "source": [
        "# 创建模型、优化器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csznz7IE_Ckh"
      },
      "source": [
        "## 创建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:39:03.296941Z",
          "start_time": "2020-10-23T04:38:53.606409Z"
        },
        "id": "AJng04oT_Ckk",
        "outputId": "49e41c78-9e45-471a-b8bb-47da90bd87a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = RobertaConfig.from_pretrained(model_name, num_labels = 4, \n",
        "                                       hidden_dropout_prob = hidden_dropout_prob, cache_dir=cache_dir)          \n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp9DVZqx_Cky"
      },
      "source": [
        "## 定义优化器\n",
        "\n",
        "参数eps是为了 提高数值稳定性 而添加到分母的一个项(默认: 1e-8)。\n",
        "\n",
        "更通用的写法：bias和LayNorm.weight没有用权重衰减"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.253260Z",
          "start_time": "2020-10-23T04:32:21.348Z"
        },
        "id": "EJj_kuAm_Ck0"
      },
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params' : [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay' : weight_decay\n",
        "    },\n",
        "    {'params' : [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay' : 0.0\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = epsilon)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf3e9Fa6_ClB"
      },
      "source": [
        "## 学习率预热，训练时先从小的学习率开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.257129Z",
          "start_time": "2020-10-23T04:32:22.034Z"
        },
        "id": "RJUgpP5q_ClF"
      },
      "source": [
        "epochs = 15\n",
        "\n",
        "# training steps 的数量: [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 设计 learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 50, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0mYzFAI_Cle"
      },
      "source": [
        "# 训练、评估模型\n",
        "\n",
        "## 模型准确率, precision, recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.260773Z",
          "start_time": "2020-10-23T04:32:22.717Z"
        },
        "id": "gmXGNgE7_Clg"
      },
      "source": [
        "def binary_acc(preds, labels):\n",
        "    # print(torch.max(preds, dim=1)[1], labels)\n",
        "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()\n",
        "    acc = correct.sum().item() / len(labels)\n",
        "    # print('acc:', acc)\n",
        "    return acc \n",
        "\n",
        "# Comparison:  0\n",
        "# Expansion:   1\n",
        "# Contingency: 2\n",
        "# Temporal:    3\n",
        "def report_Evaluation(y_preds, y_labels, labels=[0, 1, 2, 3], \n",
        "                      target_names=['Comparison', 'Expansion', 'Contingency', 'Temporal'],\n",
        "                      digits = 3):\n",
        "    # print(\"eval: \", y_preds, y_labels, labels)\n",
        "    return classification_report(y_preds, y_labels, labels=labels, \n",
        "                                 target_names=target_names, digits=digits)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.264743Z",
          "start_time": "2020-10-23T04:32:23.042Z"
        },
        "id": "gk5nXi38_Cl3",
        "outputId": "3ac55deb-058f-4b2f-9b2f-233ed35e2d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# =========== Test ============\n",
        "y_true = [1, 2, 0, 0, 3, 0, 3]\n",
        "y_pred = [1, 2, 0, 0, 2, 0, 3]\n",
        "\n",
        "print(report_Evaluation(y_pred, y_true))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      1.000     1.000     1.000         3\n",
            "   Expansion      1.000     1.000     1.000         1\n",
            " Contingency      1.000     0.500     0.667         2\n",
            "    Temporal      0.500     1.000     0.667         1\n",
            "\n",
            "    accuracy                          0.857         7\n",
            "   macro avg      0.875     0.875     0.833         7\n",
            "weighted avg      0.929     0.857     0.857         7\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-2cddg_CnH"
      },
      "source": [
        "## 计算模型运行时间"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.344834Z",
          "start_time": "2020-10-23T04:32:23.901Z"
        },
        "id": "bv3yAtCw_CnK"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "    return str(datetime.timedelta(seconds = elapsed_rounded)) # 返回 hh:mm:ss 形式的时间"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOU4oJPW_Cnn"
      },
      "source": [
        "## 保存和加载模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T05:40:00.583531Z",
          "start_time": "2020-10-23T05:40:00.577267Z"
        },
        "id": "7ur1-hdD_Cnr"
      },
      "source": [
        "def saveModel(model, name):\n",
        "    torch.save(model.state_dict(), './model/' + name + '_model.pt')\n",
        "\n",
        "def loadModel(model, name):\n",
        "    model.load_state_dict(torch.load('./model/' + name + '_model.pt', map_location=device))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs8I2C5l_Cn4"
      },
      "source": [
        "## 训练模型\n",
        "\n",
        "传入model的参数必须是tensor类型的；\n",
        "\n",
        "`nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)` 用于解决神经网络训练过拟合的方法；\n",
        "\n",
        " - 输入是（NN参数，最大梯度范数，范数类型=2) 一般默认为L2 范数；\n",
        " \n",
        " - Tip： 注意这个方法只在训练的时候使用，在测试的时候不用；\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.351420Z",
          "start_time": "2020-10-23T04:32:24.526Z"
        },
        "id": "hqn1UUHb_Cn7"
      },
      "source": [
        "def train_one_step(model, optimizer):\n",
        "    t0 = time.time()\n",
        "    avg_loss, avg_acc = [], []\n",
        "    \n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # 每隔40个batch 输出一下所用时间\n",
        "        if step % 10 == 0 and not (step == 0):\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
        "\n",
        "#         print(b_input_ids.shape, b_input_mask.shape, b_labels.shape)\n",
        "        \n",
        "        # loss: 损失，logits：predict\n",
        "        output = model(input_ids=b_input_ids, token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask, labels = b_labels)\n",
        "        loss, logits = output[0], output[1]\n",
        "        \n",
        "        avg = binary_acc(logits, b_labels)        # (predict, label)\n",
        "        avg_acc.append(avg)\n",
        "\n",
        "        avg_loss.append(loss.item())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)  # 大于1的梯度将其设置为1.0，以防止梯度爆炸\n",
        "        optimizer.step()                          # 更新模型参数\n",
        "        scheduler.step()                          # 更新learning_rate\n",
        "        \n",
        "        # print(\"loss: \", loss.item())\n",
        "\n",
        "        # print('logits index: ', torch.max(logits, dim=1)[1])\n",
        "        # print(\"labels: \", b_labels)\n",
        "        # print('eva: ', report_Evaluation(torch.max(logits, dim=1)[1].cpu(), b_labels.cpu()))\n",
        "    \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    avg_loss = np.array(avg_loss).mean()\n",
        "    \n",
        "    return avg_loss, avg_acc   "
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngTJ-ndH_CoD"
      },
      "source": [
        "## 评估模型\n",
        "\n",
        "调用model模型时不传入label值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.357485Z",
          "start_time": "2020-10-23T04:32:25.683Z"
        },
        "id": "3YnLKAC7_CoG"
      },
      "source": [
        "def evaluate(model, dataloader):\n",
        "    avg_acc = []\n",
        "    \n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:    # dev\n",
        "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device),                                                   batch[2].long().to(device)\n",
        "            \n",
        "            output = model(input_ids = b_input_ids, token_type_ids = None,\n",
        "                           attention_mask = b_input_mask)\n",
        "            \n",
        "            logits = output[0]\n",
        "            acc = binary_acc(logits, b_labels)\n",
        "            avg_acc.append(acc)\n",
        "\n",
        "            y_true.extend(b_labels.cpu().numpy())\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    \n",
        "    print(\"dev集性能: \\n\", report_Evaluation(y_true, y_pred))\n",
        "    return avg_acc, f1_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOj_sNMZ_Coe"
      },
      "source": [
        "## 测试模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOSvKoCn_Cog"
      },
      "source": [
        "# test model\n",
        "def test_model(model, dataloader):\n",
        "    avg_acc = []\n",
        "    \n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:    # dev, test\n",
        "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device),                                                   batch[2].long().to(device)\n",
        "            \n",
        "            output = model(input_ids = b_input_ids, token_type_ids = None,\n",
        "                           attention_mask = b_input_mask)\n",
        "            \n",
        "            logits = output[0]\n",
        "            acc = binary_acc(logits, b_labels)\n",
        "            avg_acc.append(acc)\n",
        "\n",
        "            y_true.extend(b_labels.cpu().numpy())\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    \n",
        "    print(\"Test集性能: \\n\", report_Evaluation(y_true, y_pred))\n",
        "    return avg_acc, f1_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vid8qXr_Co1"
      },
      "source": [
        "# 训练模型和评估模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T04:32:53.362895Z",
          "start_time": "2020-10-23T04:32:26.434Z"
        },
        "id": "r0eWJlt1_Co7"
      },
      "source": [
        "def Train():\n",
        "    best_dev_f1 = float(\"-inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_one_step(model, optimizer)\n",
        "        print('epoch={}, 训练准确率={}，损失={}'.format(epoch, train_acc, train_loss))\n",
        "        \n",
        "        dev_acc, dev_f1 = evaluate(model, dev_dataloader)\n",
        "        print(\"epoch={}, 开发集准确率={}, F1值={}\".format(epoch, dev_acc, dev_f1))\n",
        "\n",
        "        if dev_f1 > best_dev_f1:\n",
        "            best_dev_f1 = dev_f1\n",
        "            saveModel(model, 'roberta')"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-23T05:38:37.587842Z",
          "start_time": "2020-10-23T05:38:37.582251Z"
        },
        "id": "2czNpIEw_CpF"
      },
      "source": [
        "def Test():\n",
        "    loadModel(model, 'roberta')\n",
        "    test_acc, test_F1 = test_model(model, test_dataloader)\n",
        "    print(\"Test. Acc: {0:.2f}%.\\nTest. macro f1-score: {1:.2f}.\".format(test_acc * 100, test_F1))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqu80I85_CpU",
        "outputId": "8000f6eb-71f7-4991-fa62-62a81164f5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Train()\n",
        "\n",
        "Test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:23.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:27.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:31.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:35.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:39.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:43.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:48.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:52.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:56.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:00.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:04.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:08.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:12.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:16.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:20.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:24.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:28.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:32.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:36.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:40.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:44.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:48.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:52.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:56.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:00.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:04.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:08.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:12.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:16.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:20.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:25.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:29.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:33.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:41.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:45.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:49.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:53.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:57.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:01.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:05.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:09.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:13.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:17.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:21.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:25.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:29.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:33.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:37.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:41.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:45.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:49.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:53.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:57.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:01.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:05.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:09.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:13.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:26.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:30.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:34.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:38.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:42.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:46.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:50.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:54.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:58.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:02.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:06.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:10.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:14.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:18.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:22.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:26.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:30.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:34.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:38.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:42.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:46.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:50.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:35.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:39.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:43.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:47.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:51.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:55.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:06:59.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:03.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:07.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:11.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:15.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:19.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:23.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:28.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:32.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:36.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:40.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:44.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:48.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:52.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:56.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:00.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:04.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:08.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:12.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:16.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:20.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:24.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:28.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:32.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:36.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:40.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:44.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:48.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:52.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:56.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:00.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:04.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:08.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:13.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:17.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:21.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:25.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:29.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:33.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:37.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:41.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:45.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:49.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:53.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:57.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:01.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:05.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:09.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:13.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:17.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:21.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:25.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:29.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:33.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:37.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:42.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:46.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:50.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:54.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:58.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:02.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:06.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:10.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:14.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:18.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:22.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:26.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:30.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:34.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:38.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:42.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:46.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:50.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:54.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:58.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:02.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:06.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:10.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:14.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:18.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:22.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:26.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:30.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:35.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:39.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:43.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:47.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:51.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:55.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:12:59.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:03.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:07.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:11.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:15.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:19.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:23.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:27.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:31.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:35.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:39.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:43.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:47.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:51.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:55.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:13:59.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:03.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:08.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:12.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:16.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:20.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:24.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:28.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:32.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:36.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:40.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:44.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:48.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:52.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:56.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:00.\n",
            "epoch=0, 训练准确率=0.5273239331542823，损失=1.0940592367412154\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.452     0.200     0.277       190\n",
            "   Expansion      0.683     0.551     0.610       748\n",
            " Contingency      0.528     0.786     0.632       579\n",
            "    Temporal      0.467     0.360     0.407       136\n",
            "\n",
            "    accuracy                          0.577      1653\n",
            "   macro avg      0.533     0.474     0.481      1653\n",
            "weighted avg      0.585     0.577     0.563      1653\n",
            "\n",
            "epoch=0, 开发集准确率=0.5768115942028986, F1值=0.4814685725495102\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:14.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:18.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:22.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:47.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:51.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:55.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:59.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:03.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:07.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:11.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:15.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:19.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:23.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:24.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:28.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:32.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:36.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:40.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:44.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:48.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:52.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:56.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:06:00.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:04.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:08.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:12.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:16.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:20.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:24.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:28.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:32.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:36.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:40.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:44.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:48.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:52.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:56.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:00.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:05.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:09.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:13.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:17.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:21.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:25.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:29.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:33.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:37.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:41.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:45.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:49.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:53.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:57.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:01.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:05.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:09.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:13.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:17.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:21.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:25.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:29.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:33.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:38.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:42.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:46.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:50.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:54.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:58.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:02.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:06.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:10.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:14.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:18.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:22.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:26.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:30.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:34.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:38.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:42.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:46.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:50.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:54.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:58.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:02.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:06.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:10.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:14.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:19.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:23.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:27.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:31.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:35.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:39.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:43.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:47.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:51.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:55.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:59.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:03.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:07.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:11.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:15.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:19.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:23.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:27.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:31.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:35.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:39.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:43.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:47.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:51.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:55.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:12:00.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:04.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:08.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:12.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:16.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:20.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:24.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:28.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:32.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:36.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:40.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:44.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:48.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:52.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:56.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:13:00.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:04.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:08.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:12.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:16.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:20.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:24.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:28.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:32.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:36.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:40.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:44.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:49.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:53.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:57.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:14:01.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:05.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:09.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:13.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:17.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:21.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:25.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:29.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:33.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:37.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:41.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:45.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:49.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:53.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:57.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:01.\n",
            "epoch=1, 训练准确率=0.6170359594151，损失=0.8981994266088988\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.543     0.495     0.518       190\n",
            "   Expansion      0.684     0.778     0.728       748\n",
            " Contingency      0.729     0.594     0.655       579\n",
            "    Temporal      0.484     0.559     0.519       136\n",
            "\n",
            "    accuracy                          0.663      1653\n",
            "   macro avg      0.610     0.606     0.605      1653\n",
            "weighted avg      0.667     0.663     0.661      1653\n",
            "\n",
            "epoch=1, 开发集准确率=0.663647342995169, F1值=0.6048118229288222\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:10.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:14.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:18.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:22.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:47.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:51.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:55.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:59.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:03.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:07.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:11.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:15.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:19.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:23.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:16.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:20.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:24.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:28.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:32.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:36.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:40.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:44.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:48.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:52.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:56.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:06:00.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:04.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:08.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:12.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:16.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:20.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:24.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:28.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:32.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:36.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:41.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:45.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:49.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:53.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:57.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:01.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:05.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:09.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:13.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:17.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:21.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:25.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:29.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:33.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:37.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:41.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:45.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:49.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:53.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:57.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:01.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:06.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:10.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:14.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:18.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:22.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:26.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:30.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:34.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:38.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:42.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:46.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:50.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:54.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:58.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:02.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:06.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:10.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:14.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:18.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:22.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:26.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:30.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:34.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:39.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:43.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:47.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:51.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:55.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:59.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:03.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:07.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:11.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:15.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:19.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:23.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:27.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:31.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:35.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:39.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:43.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:47.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:51.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:55.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:59.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:03.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:07.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:11.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:16.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:20.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:24.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:28.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:32.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:36.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:40.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:44.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:48.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:52.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:56.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:12:00.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:04.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:08.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:12.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:16.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:20.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:24.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:28.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:32.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:36.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:40.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:45.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:49.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:53.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:57.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:13:01.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:05.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:09.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:13.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:17.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:21.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:25.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:29.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:33.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:37.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:41.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:45.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:49.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:53.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:57.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:14:01.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:05.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:09.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:13.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:17.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:22.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:26.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:30.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:34.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:38.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:42.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:46.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:50.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:54.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:58.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:02.\n",
            "epoch=2, 训练准确率=0.6636265293942106，损失=0.8072048507717537\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.548     0.605     0.575       190\n",
            "   Expansion      0.749     0.703     0.726       748\n",
            " Contingency      0.654     0.729     0.690       579\n",
            "    Temporal      0.625     0.441     0.517       136\n",
            "\n",
            "    accuracy                          0.679      1653\n",
            "   macro avg      0.644     0.620     0.627      1653\n",
            "weighted avg      0.683     0.679     0.678      1653\n",
            "\n",
            "epoch=2, 开发集准确率=0.6792270531400966, F1值=0.6268252760874465\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:18.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:22.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:55.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:59.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:03.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:07.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:11.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:15.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:19.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:23.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:32.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:36.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:40.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:44.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:48.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:52.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:56.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:06:00.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:04.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:08.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:12.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:16.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:20.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:24.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:28.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:32.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:36.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:40.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:44.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:48.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:52.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:57.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:01.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:05.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:09.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:13.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:17.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:21.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:25.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:29.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:33.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:37.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:41.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:45.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:49.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:53.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:57.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:01.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:05.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:09.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:13.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:17.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:21.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:25.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:29.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:33.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:37.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:42.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:46.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:50.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:54.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:58.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:02.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:06.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:10.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:14.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:18.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:22.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:26.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:30.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:34.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:38.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:42.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:46.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:50.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:54.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:58.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:02.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:06.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:11.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:15.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:19.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:23.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:27.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:31.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:35.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:39.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:43.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:47.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:51.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:55.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:59.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:03.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:07.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:11.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:15.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:19.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:23.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:27.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:31.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:35.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:39.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:43.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:48.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:52.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:56.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:12:00.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:04.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:08.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:12.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:16.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:20.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:24.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:28.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:32.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:36.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:40.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:44.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:48.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:52.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:56.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:13:00.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:04.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:08.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:13.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:17.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:21.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:25.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:29.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:33.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:37.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:41.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:45.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:49.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:53.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:57.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:14:01.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:05.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:09.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:13.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:17.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:21.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:25.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:29.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:33.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:37.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:41.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:45.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:49.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:53.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:57.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:02.\n",
            "epoch=3, 训练准确率=0.6917524619516562，损失=0.7434641890721387\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.548     0.637     0.589       190\n",
            "   Expansion      0.731     0.745     0.738       748\n",
            " Contingency      0.700     0.689     0.695       579\n",
            "    Temporal      0.620     0.456     0.525       136\n",
            "\n",
            "    accuracy                          0.689      1653\n",
            "   macro avg      0.650     0.632     0.637      1653\n",
            "weighted avg      0.690     0.689     0.688      1653\n",
            "\n",
            "epoch=3, 开发集准确率=0.6892512077294687, F1值=0.6366242075879679\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:10.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:14.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:18.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:22.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:47.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:51.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:55.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:59.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:03.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:07.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:11.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:15.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:19.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:23.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:16.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:20.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:24.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:28.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:32.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:36.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:40.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:44.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:48.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:52.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:56.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:06:00.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:04.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:08.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:12.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:16.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:20.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:24.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:28.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:32.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:37.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:41.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:45.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:49.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:53.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:57.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:01.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:05.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:09.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:13.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:17.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:21.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:25.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:29.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:33.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:37.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:41.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:45.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:49.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:53.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:57.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:01.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:05.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:09.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:13.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:18.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:22.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:26.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:30.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:34.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:38.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:42.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:46.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:50.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:54.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:58.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:02.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:06.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:10.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:14.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:18.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:22.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:26.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:30.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:34.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:38.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:42.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:46.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:50.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:54.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:58.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:02.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:07.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:11.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:15.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:19.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:23.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:27.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:31.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:35.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:39.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:43.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:47.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:51.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:55.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:59.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:03.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:07.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:11.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:15.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:19.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:23.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:27.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:31.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:35.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:39.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:43.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:47.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:51.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:55.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:59.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:04.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:08.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:12.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:16.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:20.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:24.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:28.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:32.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:36.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:40.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:44.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:48.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:52.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:56.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:13:00.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:04.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:08.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:12.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:16.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:20.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:24.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:28.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:32.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:37.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:41.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:45.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:49.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:53.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:57.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:14:01.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:05.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:09.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:13.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:17.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:21.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:25.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:29.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:33.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:37.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:41.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:45.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:49.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:53.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:57.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:01.\n",
            "epoch=4, 训练准确率=0.7180132796180244，损失=0.685400193018527\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.555     0.616     0.584       190\n",
            "   Expansion      0.778     0.695     0.734       748\n",
            " Contingency      0.682     0.772     0.724       579\n",
            "    Temporal      0.605     0.529     0.565       136\n",
            "\n",
            "    accuracy                          0.699      1653\n",
            "   macro avg      0.655     0.653     0.652      1653\n",
            "weighted avg      0.705     0.699     0.700      1653\n",
            "\n",
            "epoch=4, 开发集准确率=0.698792270531401, F1值=0.6517958910049526\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:33.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:10.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:14.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:18.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:22.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:43.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:47.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:51.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:55.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:59.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:03.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:07.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:11.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:15.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:19.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:23.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:32.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:36.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:40.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:44.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:48.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:52.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:56.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:06:00.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:04.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:08.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:12.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:16.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:20.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:24.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:28.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:32.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:36.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:40.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:44.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:48.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:52.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:56.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:01.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:05.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:09.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:13.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:17.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:21.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:25.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:29.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:33.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:37.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:41.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:45.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:49.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:53.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:57.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:01.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:05.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:09.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:13.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:17.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:21.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:25.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:29.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:33.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:37.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:41.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:45.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:49.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:53.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:58.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:02.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:06.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:10.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:14.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:18.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:22.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:26.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:30.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:34.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:38.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:42.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:46.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:50.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:54.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:58.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:02.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:06.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:10.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:14.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:18.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:22.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:26.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:30.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:34.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:38.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:42.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:47.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:51.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:55.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:59.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:03.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:07.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:11.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:15.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:19.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:23.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:27.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:31.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:35.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:39.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:43.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:47.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:51.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:55.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:59.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:03.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:07.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:11.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:15.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:19.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:23.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:28.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:32.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:36.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:40.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:44.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:48.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:52.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:56.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:13:00.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:04.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:08.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:12.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:16.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:20.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:24.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:28.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:32.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:36.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:40.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:44.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:48.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:52.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:56.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:14:00.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:04.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:08.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:12.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:17.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:21.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:25.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:29.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:33.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:37.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:41.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:45.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:49.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:53.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:57.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:01.\n",
            "epoch=5, 训练准确率=0.7386414503133393，损失=0.6418505289267512\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.553     0.605     0.578       190\n",
            "   Expansion      0.745     0.750     0.748       748\n",
            " Contingency      0.730     0.696     0.713       579\n",
            "    Temporal      0.536     0.551     0.543       136\n",
            "\n",
            "    accuracy                          0.698      1653\n",
            "   macro avg      0.641     0.651     0.645      1653\n",
            "weighted avg      0.700     0.698     0.699      1653\n",
            "\n",
            "epoch=5, 开发集准确率=0.6986714975845411, F1值=0.6453782629557403\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:36.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:40.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:17.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:21.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:25.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:29.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:33.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:54.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:58.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:02.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:06.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:10.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:14.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:26.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:30.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:34.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:38.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:42.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:46.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:50.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:31.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:35.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:39.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:43.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:47.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:51.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:35.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:39.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:43.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:47.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:51.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:56.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:00.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:04.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:08.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:12.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:16.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:20.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:24.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:28.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:32.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:36.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:40.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:44.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:48.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:52.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:56.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:00.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:04.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:08.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:12.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:16.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:20.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:24.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:28.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:32.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:36.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:40.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:44.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:48.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:52.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:56.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:00.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:05.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:09.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:13.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:17.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:21.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:25.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:29.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:33.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:37.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:41.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:45.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:49.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:53.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:57.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:01.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:05.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:09.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:13.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:17.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:21.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:25.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:29.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:33.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:37.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:41.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:45.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:49.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:53.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:57.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:01.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:06.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:10.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:14.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:18.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:22.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:26.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:30.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:34.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:38.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:42.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:46.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:50.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:54.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:58.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:02.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:06.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:10.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:14.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:18.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:22.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:26.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:30.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:34.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:38.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:42.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:46.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:50.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:54.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:12:58.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:02.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:06.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:10.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:14.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:18.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:22.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:26.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:31.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:35.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:39.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:43.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:47.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:51.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:55.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:13:59.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:03.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:07.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:11.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:15.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:19.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:23.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:27.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:31.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:35.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:39.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:43.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:47.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:51.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:55.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:14:59.\n",
            "epoch=6, 训练准确率=0.759195016413011，损失=0.5972653584906643\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.599     0.589     0.594       190\n",
            "   Expansion      0.750     0.775     0.763       748\n",
            " Contingency      0.706     0.734     0.720       579\n",
            "    Temporal      0.692     0.463     0.555       136\n",
            "\n",
            "    accuracy                          0.714      1653\n",
            "   macro avg      0.687     0.641     0.658      1653\n",
            "weighted avg      0.713     0.714     0.711      1653\n",
            "\n",
            "epoch=6, 开发集准确率=0.7136473429951691, F1值=0.6579039314959627\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:37.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:41.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:45.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:49.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:17.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:21.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:26.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:30.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:34.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:38.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:42.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:46.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:50.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:54.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:58.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:02.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:06.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:10.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:14.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:27.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:31.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:35.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:39.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:43.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:47.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:51.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:55.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:59.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:31.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:35.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:39.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:43.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:47.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:51.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:36.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:40.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:44.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:48.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:52.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:56.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:07:00.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:04.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:08.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:12.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:16.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:20.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:24.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:28.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:32.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:36.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:40.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:44.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:48.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:52.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:56.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:00.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:04.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:08.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:12.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:16.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:20.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:24.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:28.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:33.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:37.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:41.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:45.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:49.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:53.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:57.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:01.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:05.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:09.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:13.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:17.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:21.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:25.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:29.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:33.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:37.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:41.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:45.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:49.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:53.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:57.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:01.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:05.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:09.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:13.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:17.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:21.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:25.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:29.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:33.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:38.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:42.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:46.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:50.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:54.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:58.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:02.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:06.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:10.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:14.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:18.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:22.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:26.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:30.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:34.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:38.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:42.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:46.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:50.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:54.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:58.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:02.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:06.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:10.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:14.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:18.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:22.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:26.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:30.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:34.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:39.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:43.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:47.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:51.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:55.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:12:59.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:03.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:07.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:11.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:15.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:19.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:23.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:27.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:31.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:35.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:39.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:43.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:47.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:51.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:55.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:13:59.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:03.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:07.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:11.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:15.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:19.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:23.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:27.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:31.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:35.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:39.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:44.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:48.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:52.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:56.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:15:00.\n",
            "epoch=7, 训练准确率=0.7722881229483736，损失=0.5635225266991926\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.601     0.611     0.606       190\n",
            "   Expansion      0.744     0.733     0.738       748\n",
            " Contingency      0.705     0.717     0.711       579\n",
            "    Temporal      0.545     0.537     0.541       136\n",
            "\n",
            "    accuracy                          0.697      1653\n",
            "   macro avg      0.648     0.649     0.649      1653\n",
            "weighted avg      0.697     0.697     0.697      1653\n",
            "\n",
            "epoch=7, 开发集准确率=0.697463768115942, F1值=0.6487871106176034\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:36.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:40.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:44.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:48.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:17.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:21.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:25.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:29.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:33.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:41.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:45.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:49.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:53.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:57.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:01.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:05.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:09.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:54.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:58.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:02.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:06.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:10.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:14.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:26.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:30.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:34.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:38.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:42.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:46.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:50.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:54.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:58.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:02.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:06.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:31.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:35.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:39.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:43.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:47.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:51.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:35.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:39.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:43.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:47.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:51.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:55.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:06:59.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:03.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:07.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:11.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:15.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:19.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:23.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:28.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:32.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:36.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:40.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:44.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:48.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:52.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:56.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:00.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:04.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:08.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:12.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:16.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:20.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:24.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:28.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:32.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:36.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:40.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:44.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:48.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:52.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:56.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:00.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:04.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:08.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:12.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:17.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:21.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:25.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:29.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:33.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:37.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:41.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:45.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:49.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:53.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:57.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:01.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:05.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:09.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:13.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:17.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:21.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:25.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:29.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:33.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:37.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:41.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:45.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:49.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:53.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:57.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:01.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:05.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:09.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:13.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:18.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:22.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:26.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:30.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:34.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:38.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:42.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:46.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:50.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:54.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:58.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:02.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:06.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:10.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:14.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:18.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:22.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:26.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:30.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:34.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:38.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:42.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:46.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:50.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:54.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:12:58.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:02.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:06.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:10.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:14.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:19.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:23.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:27.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:31.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:35.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:39.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:43.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:47.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:51.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:55.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:13:59.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:03.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:07.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:11.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:15.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:19.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:23.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:27.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:31.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:35.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:39.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:43.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:47.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:51.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:55.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:14:59.\n",
            "epoch=8, 训练准确率=0.7888876454789615，损失=0.5276704205695336\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.585     0.616     0.600       190\n",
            "   Expansion      0.729     0.771     0.749       748\n",
            " Contingency      0.739     0.668     0.702       579\n",
            "    Temporal      0.562     0.566     0.564       136\n",
            "\n",
            "    accuracy                          0.701      1653\n",
            "   macro avg      0.654     0.655     0.654      1653\n",
            "weighted avg      0.702     0.701     0.700      1653\n",
            "\n",
            "epoch=8, 开发集准确率=0.7010869565217391, F1值=0.6537939470623061\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:36.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:40.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:44.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:48.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:53.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:57.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:01.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:05.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:09.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:17.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:21.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:25.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:29.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:33.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:41.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:45.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:49.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:54.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:58.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:02.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:06.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:10.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:54.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:58.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:02.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:06.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:10.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:14.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:26.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:30.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:34.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:38.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:42.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:46.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:50.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:54.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:58.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:03.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:07.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:11.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:15.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:19.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:23.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:27.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:31.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:35.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:39.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:43.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:47.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:51.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:35.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:39.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:43.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:47.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:51.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:55.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:06:59.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:03.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:08.\n",
            "  Batch 1,070  of  2,234.    Elapsed: 0:07:12.\n",
            "  Batch 1,080  of  2,234.    Elapsed: 0:07:16.\n",
            "  Batch 1,090  of  2,234.    Elapsed: 0:07:20.\n",
            "  Batch 1,100  of  2,234.    Elapsed: 0:07:24.\n",
            "  Batch 1,110  of  2,234.    Elapsed: 0:07:28.\n",
            "  Batch 1,120  of  2,234.    Elapsed: 0:07:32.\n",
            "  Batch 1,130  of  2,234.    Elapsed: 0:07:36.\n",
            "  Batch 1,140  of  2,234.    Elapsed: 0:07:40.\n",
            "  Batch 1,150  of  2,234.    Elapsed: 0:07:44.\n",
            "  Batch 1,160  of  2,234.    Elapsed: 0:07:48.\n",
            "  Batch 1,170  of  2,234.    Elapsed: 0:07:52.\n",
            "  Batch 1,180  of  2,234.    Elapsed: 0:07:56.\n",
            "  Batch 1,190  of  2,234.    Elapsed: 0:08:00.\n",
            "  Batch 1,200  of  2,234.    Elapsed: 0:08:04.\n",
            "  Batch 1,210  of  2,234.    Elapsed: 0:08:08.\n",
            "  Batch 1,220  of  2,234.    Elapsed: 0:08:12.\n",
            "  Batch 1,230  of  2,234.    Elapsed: 0:08:16.\n",
            "  Batch 1,240  of  2,234.    Elapsed: 0:08:20.\n",
            "  Batch 1,250  of  2,234.    Elapsed: 0:08:24.\n",
            "  Batch 1,260  of  2,234.    Elapsed: 0:08:28.\n",
            "  Batch 1,270  of  2,234.    Elapsed: 0:08:32.\n",
            "  Batch 1,280  of  2,234.    Elapsed: 0:08:36.\n",
            "  Batch 1,290  of  2,234.    Elapsed: 0:08:40.\n",
            "  Batch 1,300  of  2,234.    Elapsed: 0:08:44.\n",
            "  Batch 1,310  of  2,234.    Elapsed: 0:08:48.\n",
            "  Batch 1,320  of  2,234.    Elapsed: 0:08:52.\n",
            "  Batch 1,330  of  2,234.    Elapsed: 0:08:56.\n",
            "  Batch 1,340  of  2,234.    Elapsed: 0:09:00.\n",
            "  Batch 1,350  of  2,234.    Elapsed: 0:09:04.\n",
            "  Batch 1,360  of  2,234.    Elapsed: 0:09:08.\n",
            "  Batch 1,370  of  2,234.    Elapsed: 0:09:13.\n",
            "  Batch 1,380  of  2,234.    Elapsed: 0:09:17.\n",
            "  Batch 1,390  of  2,234.    Elapsed: 0:09:21.\n",
            "  Batch 1,400  of  2,234.    Elapsed: 0:09:25.\n",
            "  Batch 1,410  of  2,234.    Elapsed: 0:09:29.\n",
            "  Batch 1,420  of  2,234.    Elapsed: 0:09:33.\n",
            "  Batch 1,430  of  2,234.    Elapsed: 0:09:37.\n",
            "  Batch 1,440  of  2,234.    Elapsed: 0:09:41.\n",
            "  Batch 1,450  of  2,234.    Elapsed: 0:09:45.\n",
            "  Batch 1,460  of  2,234.    Elapsed: 0:09:49.\n",
            "  Batch 1,470  of  2,234.    Elapsed: 0:09:53.\n",
            "  Batch 1,480  of  2,234.    Elapsed: 0:09:57.\n",
            "  Batch 1,490  of  2,234.    Elapsed: 0:10:01.\n",
            "  Batch 1,500  of  2,234.    Elapsed: 0:10:05.\n",
            "  Batch 1,510  of  2,234.    Elapsed: 0:10:09.\n",
            "  Batch 1,520  of  2,234.    Elapsed: 0:10:13.\n",
            "  Batch 1,530  of  2,234.    Elapsed: 0:10:17.\n",
            "  Batch 1,540  of  2,234.    Elapsed: 0:10:21.\n",
            "  Batch 1,550  of  2,234.    Elapsed: 0:10:25.\n",
            "  Batch 1,560  of  2,234.    Elapsed: 0:10:29.\n",
            "  Batch 1,570  of  2,234.    Elapsed: 0:10:33.\n",
            "  Batch 1,580  of  2,234.    Elapsed: 0:10:37.\n",
            "  Batch 1,590  of  2,234.    Elapsed: 0:10:41.\n",
            "  Batch 1,600  of  2,234.    Elapsed: 0:10:45.\n",
            "  Batch 1,610  of  2,234.    Elapsed: 0:10:49.\n",
            "  Batch 1,620  of  2,234.    Elapsed: 0:10:53.\n",
            "  Batch 1,630  of  2,234.    Elapsed: 0:10:57.\n",
            "  Batch 1,640  of  2,234.    Elapsed: 0:11:01.\n",
            "  Batch 1,650  of  2,234.    Elapsed: 0:11:05.\n",
            "  Batch 1,660  of  2,234.    Elapsed: 0:11:09.\n",
            "  Batch 1,670  of  2,234.    Elapsed: 0:11:13.\n",
            "  Batch 1,680  of  2,234.    Elapsed: 0:11:18.\n",
            "  Batch 1,690  of  2,234.    Elapsed: 0:11:22.\n",
            "  Batch 1,700  of  2,234.    Elapsed: 0:11:26.\n",
            "  Batch 1,710  of  2,234.    Elapsed: 0:11:30.\n",
            "  Batch 1,720  of  2,234.    Elapsed: 0:11:34.\n",
            "  Batch 1,730  of  2,234.    Elapsed: 0:11:38.\n",
            "  Batch 1,740  of  2,234.    Elapsed: 0:11:42.\n",
            "  Batch 1,750  of  2,234.    Elapsed: 0:11:46.\n",
            "  Batch 1,760  of  2,234.    Elapsed: 0:11:50.\n",
            "  Batch 1,770  of  2,234.    Elapsed: 0:11:54.\n",
            "  Batch 1,780  of  2,234.    Elapsed: 0:11:58.\n",
            "  Batch 1,790  of  2,234.    Elapsed: 0:12:02.\n",
            "  Batch 1,800  of  2,234.    Elapsed: 0:12:06.\n",
            "  Batch 1,810  of  2,234.    Elapsed: 0:12:10.\n",
            "  Batch 1,820  of  2,234.    Elapsed: 0:12:14.\n",
            "  Batch 1,830  of  2,234.    Elapsed: 0:12:18.\n",
            "  Batch 1,840  of  2,234.    Elapsed: 0:12:22.\n",
            "  Batch 1,850  of  2,234.    Elapsed: 0:12:26.\n",
            "  Batch 1,860  of  2,234.    Elapsed: 0:12:30.\n",
            "  Batch 1,870  of  2,234.    Elapsed: 0:12:34.\n",
            "  Batch 1,880  of  2,234.    Elapsed: 0:12:38.\n",
            "  Batch 1,890  of  2,234.    Elapsed: 0:12:42.\n",
            "  Batch 1,900  of  2,234.    Elapsed: 0:12:46.\n",
            "  Batch 1,910  of  2,234.    Elapsed: 0:12:50.\n",
            "  Batch 1,920  of  2,234.    Elapsed: 0:12:55.\n",
            "  Batch 1,930  of  2,234.    Elapsed: 0:12:59.\n",
            "  Batch 1,940  of  2,234.    Elapsed: 0:13:03.\n",
            "  Batch 1,950  of  2,234.    Elapsed: 0:13:07.\n",
            "  Batch 1,960  of  2,234.    Elapsed: 0:13:11.\n",
            "  Batch 1,970  of  2,234.    Elapsed: 0:13:15.\n",
            "  Batch 1,980  of  2,234.    Elapsed: 0:13:19.\n",
            "  Batch 1,990  of  2,234.    Elapsed: 0:13:23.\n",
            "  Batch 2,000  of  2,234.    Elapsed: 0:13:27.\n",
            "  Batch 2,010  of  2,234.    Elapsed: 0:13:31.\n",
            "  Batch 2,020  of  2,234.    Elapsed: 0:13:35.\n",
            "  Batch 2,030  of  2,234.    Elapsed: 0:13:39.\n",
            "  Batch 2,040  of  2,234.    Elapsed: 0:13:43.\n",
            "  Batch 2,050  of  2,234.    Elapsed: 0:13:47.\n",
            "  Batch 2,060  of  2,234.    Elapsed: 0:13:51.\n",
            "  Batch 2,070  of  2,234.    Elapsed: 0:13:55.\n",
            "  Batch 2,080  of  2,234.    Elapsed: 0:13:59.\n",
            "  Batch 2,090  of  2,234.    Elapsed: 0:14:03.\n",
            "  Batch 2,100  of  2,234.    Elapsed: 0:14:07.\n",
            "  Batch 2,110  of  2,234.    Elapsed: 0:14:11.\n",
            "  Batch 2,120  of  2,234.    Elapsed: 0:14:15.\n",
            "  Batch 2,130  of  2,234.    Elapsed: 0:14:19.\n",
            "  Batch 2,140  of  2,234.    Elapsed: 0:14:23.\n",
            "  Batch 2,150  of  2,234.    Elapsed: 0:14:27.\n",
            "  Batch 2,160  of  2,234.    Elapsed: 0:14:31.\n",
            "  Batch 2,170  of  2,234.    Elapsed: 0:14:35.\n",
            "  Batch 2,180  of  2,234.    Elapsed: 0:14:39.\n",
            "  Batch 2,190  of  2,234.    Elapsed: 0:14:43.\n",
            "  Batch 2,200  of  2,234.    Elapsed: 0:14:47.\n",
            "  Batch 2,210  of  2,234.    Elapsed: 0:14:51.\n",
            "  Batch 2,220  of  2,234.    Elapsed: 0:14:55.\n",
            "  Batch 2,230  of  2,234.    Elapsed: 0:14:59.\n",
            "epoch=9, 训练准确率=0.8028200537153088，损失=0.4984788682933767\n",
            "dev集性能: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Comparison      0.589     0.595     0.592       190\n",
            "   Expansion      0.716     0.767     0.741       748\n",
            " Contingency      0.745     0.660     0.700       579\n",
            "    Temporal      0.534     0.574     0.553       136\n",
            "\n",
            "    accuracy                          0.694      1653\n",
            "   macro avg      0.646     0.649     0.646      1653\n",
            "weighted avg      0.696     0.694     0.694      1653\n",
            "\n",
            "epoch=9, 开发集准确率=0.6940821256038648, F1值=0.6462733467337347\n",
            "  Batch    10  of  2,234.    Elapsed: 0:00:04.\n",
            "  Batch    20  of  2,234.    Elapsed: 0:00:08.\n",
            "  Batch    30  of  2,234.    Elapsed: 0:00:12.\n",
            "  Batch    40  of  2,234.    Elapsed: 0:00:16.\n",
            "  Batch    50  of  2,234.    Elapsed: 0:00:20.\n",
            "  Batch    60  of  2,234.    Elapsed: 0:00:24.\n",
            "  Batch    70  of  2,234.    Elapsed: 0:00:28.\n",
            "  Batch    80  of  2,234.    Elapsed: 0:00:32.\n",
            "  Batch    90  of  2,234.    Elapsed: 0:00:36.\n",
            "  Batch   100  of  2,234.    Elapsed: 0:00:40.\n",
            "  Batch   110  of  2,234.    Elapsed: 0:00:44.\n",
            "  Batch   120  of  2,234.    Elapsed: 0:00:48.\n",
            "  Batch   130  of  2,234.    Elapsed: 0:00:52.\n",
            "  Batch   140  of  2,234.    Elapsed: 0:00:56.\n",
            "  Batch   150  of  2,234.    Elapsed: 0:01:00.\n",
            "  Batch   160  of  2,234.    Elapsed: 0:01:04.\n",
            "  Batch   170  of  2,234.    Elapsed: 0:01:08.\n",
            "  Batch   180  of  2,234.    Elapsed: 0:01:13.\n",
            "  Batch   190  of  2,234.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  2,234.    Elapsed: 0:01:21.\n",
            "  Batch   210  of  2,234.    Elapsed: 0:01:25.\n",
            "  Batch   220  of  2,234.    Elapsed: 0:01:29.\n",
            "  Batch   230  of  2,234.    Elapsed: 0:01:33.\n",
            "  Batch   240  of  2,234.    Elapsed: 0:01:37.\n",
            "  Batch   250  of  2,234.    Elapsed: 0:01:41.\n",
            "  Batch   260  of  2,234.    Elapsed: 0:01:45.\n",
            "  Batch   270  of  2,234.    Elapsed: 0:01:49.\n",
            "  Batch   280  of  2,234.    Elapsed: 0:01:53.\n",
            "  Batch   290  of  2,234.    Elapsed: 0:01:57.\n",
            "  Batch   300  of  2,234.    Elapsed: 0:02:01.\n",
            "  Batch   310  of  2,234.    Elapsed: 0:02:05.\n",
            "  Batch   320  of  2,234.    Elapsed: 0:02:09.\n",
            "  Batch   330  of  2,234.    Elapsed: 0:02:13.\n",
            "  Batch   340  of  2,234.    Elapsed: 0:02:17.\n",
            "  Batch   350  of  2,234.    Elapsed: 0:02:21.\n",
            "  Batch   360  of  2,234.    Elapsed: 0:02:25.\n",
            "  Batch   370  of  2,234.    Elapsed: 0:02:29.\n",
            "  Batch   380  of  2,234.    Elapsed: 0:02:33.\n",
            "  Batch   390  of  2,234.    Elapsed: 0:02:37.\n",
            "  Batch   400  of  2,234.    Elapsed: 0:02:41.\n",
            "  Batch   410  of  2,234.    Elapsed: 0:02:45.\n",
            "  Batch   420  of  2,234.    Elapsed: 0:02:49.\n",
            "  Batch   430  of  2,234.    Elapsed: 0:02:53.\n",
            "  Batch   440  of  2,234.    Elapsed: 0:02:57.\n",
            "  Batch   450  of  2,234.    Elapsed: 0:03:01.\n",
            "  Batch   460  of  2,234.    Elapsed: 0:03:05.\n",
            "  Batch   470  of  2,234.    Elapsed: 0:03:09.\n",
            "  Batch   480  of  2,234.    Elapsed: 0:03:14.\n",
            "  Batch   490  of  2,234.    Elapsed: 0:03:18.\n",
            "  Batch   500  of  2,234.    Elapsed: 0:03:22.\n",
            "  Batch   510  of  2,234.    Elapsed: 0:03:26.\n",
            "  Batch   520  of  2,234.    Elapsed: 0:03:30.\n",
            "  Batch   530  of  2,234.    Elapsed: 0:03:34.\n",
            "  Batch   540  of  2,234.    Elapsed: 0:03:38.\n",
            "  Batch   550  of  2,234.    Elapsed: 0:03:42.\n",
            "  Batch   560  of  2,234.    Elapsed: 0:03:46.\n",
            "  Batch   570  of  2,234.    Elapsed: 0:03:50.\n",
            "  Batch   580  of  2,234.    Elapsed: 0:03:54.\n",
            "  Batch   590  of  2,234.    Elapsed: 0:03:58.\n",
            "  Batch   600  of  2,234.    Elapsed: 0:04:02.\n",
            "  Batch   610  of  2,234.    Elapsed: 0:04:06.\n",
            "  Batch   620  of  2,234.    Elapsed: 0:04:10.\n",
            "  Batch   630  of  2,234.    Elapsed: 0:04:14.\n",
            "  Batch   640  of  2,234.    Elapsed: 0:04:18.\n",
            "  Batch   650  of  2,234.    Elapsed: 0:04:22.\n",
            "  Batch   660  of  2,234.    Elapsed: 0:04:26.\n",
            "  Batch   670  of  2,234.    Elapsed: 0:04:30.\n",
            "  Batch   680  of  2,234.    Elapsed: 0:04:34.\n",
            "  Batch   690  of  2,234.    Elapsed: 0:04:38.\n",
            "  Batch   700  of  2,234.    Elapsed: 0:04:42.\n",
            "  Batch   710  of  2,234.    Elapsed: 0:04:46.\n",
            "  Batch   720  of  2,234.    Elapsed: 0:04:50.\n",
            "  Batch   730  of  2,234.    Elapsed: 0:04:54.\n",
            "  Batch   740  of  2,234.    Elapsed: 0:04:58.\n",
            "  Batch   750  of  2,234.    Elapsed: 0:05:02.\n",
            "  Batch   760  of  2,234.    Elapsed: 0:05:06.\n",
            "  Batch   770  of  2,234.    Elapsed: 0:05:10.\n",
            "  Batch   780  of  2,234.    Elapsed: 0:05:14.\n",
            "  Batch   790  of  2,234.    Elapsed: 0:05:18.\n",
            "  Batch   800  of  2,234.    Elapsed: 0:05:22.\n",
            "  Batch   810  of  2,234.    Elapsed: 0:05:26.\n",
            "  Batch   820  of  2,234.    Elapsed: 0:05:30.\n",
            "  Batch   830  of  2,234.    Elapsed: 0:05:34.\n",
            "  Batch   840  of  2,234.    Elapsed: 0:05:39.\n",
            "  Batch   850  of  2,234.    Elapsed: 0:05:43.\n",
            "  Batch   860  of  2,234.    Elapsed: 0:05:47.\n",
            "  Batch   870  of  2,234.    Elapsed: 0:05:51.\n",
            "  Batch   880  of  2,234.    Elapsed: 0:05:55.\n",
            "  Batch   890  of  2,234.    Elapsed: 0:05:59.\n",
            "  Batch   900  of  2,234.    Elapsed: 0:06:03.\n",
            "  Batch   910  of  2,234.    Elapsed: 0:06:07.\n",
            "  Batch   920  of  2,234.    Elapsed: 0:06:11.\n",
            "  Batch   930  of  2,234.    Elapsed: 0:06:15.\n",
            "  Batch   940  of  2,234.    Elapsed: 0:06:19.\n",
            "  Batch   950  of  2,234.    Elapsed: 0:06:23.\n",
            "  Batch   960  of  2,234.    Elapsed: 0:06:27.\n",
            "  Batch   970  of  2,234.    Elapsed: 0:06:31.\n",
            "  Batch   980  of  2,234.    Elapsed: 0:06:35.\n",
            "  Batch   990  of  2,234.    Elapsed: 0:06:39.\n",
            "  Batch 1,000  of  2,234.    Elapsed: 0:06:43.\n",
            "  Batch 1,010  of  2,234.    Elapsed: 0:06:47.\n",
            "  Batch 1,020  of  2,234.    Elapsed: 0:06:51.\n",
            "  Batch 1,030  of  2,234.    Elapsed: 0:06:55.\n",
            "  Batch 1,040  of  2,234.    Elapsed: 0:06:59.\n",
            "  Batch 1,050  of  2,234.    Elapsed: 0:07:03.\n",
            "  Batch 1,060  of  2,234.    Elapsed: 0:07:07.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sim_RdOQ_Cpi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}