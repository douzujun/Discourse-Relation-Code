{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDTB_load_json.py\n",
    "\n",
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:15:38.215811Z",
     "start_time": "2020-10-10T13:15:38.211085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:04:56.557786Z",
     "start_time": "2020-10-10T13:04:56.235000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pwd \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:19.193225Z",
     "start_time": "2020-10-10T13:10:19.067299Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_path = '/data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/'\n",
    "base_path = './'\n",
    "sense = \"Temporal\"\n",
    "\n",
    "# dicList_train = [json.loa ds(line) for line in open(\"D:/PDTB_r/02_20_Implicit.json\")]\n",
    "dicList_train = [json.loads(line) for line in open(base_path + r\"PDTB/one_vs_others_standard/Temporal_vs_others/train_sec_02_20\")]\n",
    "dicList_test = [json.loads(line) for line in open(base_path + r\"PDTB/one_vs_others_standard/Temporal_vs_others/test_sec_21_22\")]\n",
    "# dicList_al = [json.loads(line) for line in open(r'D:\\experiment\\PDTB\\PDTB语料\\00_24_explicit.json')]\n",
    "dicList_dev = [json.loads(line) for line in open(base_path + r'PDTB/one_vs_others_standard/Temporal_vs_others/dev_sec_00_01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:00:53.976900Z",
     "start_time": "2020-10-10T12:00:53.973318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocID': 'wsj_2100', 'Arg1': {'RawText': 'and viewers will be given a 900 number to call', 'Lemma': ['and', 'viewer', 'will', 'be', 'give', 'a', '900', 'number', 'to', 'call'], 'Word': ['and', 'viewers', 'will', 'be', 'given', 'a', '900', 'number', 'to', 'call'], 'POS': ['CC', 'NNS', 'MD', 'VB', 'VBN', 'DT', 'CD', 'NN', 'TO', 'VB'], 'NER': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, 'Arg2': {'RawText': \"Callers will be sent educational booklets, and the call's modest cost will be an immediate method of raising money\", 'Lemma': ['caller', 'will', 'be', 'send', 'educational', 'booklet', ',', 'and', 'the', 'call', \"'s\", 'modest', 'cost', 'will', 'be', 'a', 'immediate', 'method', 'of', 'raise', 'money'], 'Word': ['Callers', 'will', 'be', 'sent', 'educational', 'booklets', ',', 'and', 'the', 'call', \"'s\", 'modest', 'cost', 'will', 'be', 'an', 'immediate', 'method', 'of', 'raising', 'money'], 'POS': ['NNS', 'MD', 'VB', 'VBN', 'JJ', 'NNS', ',', 'CC', 'DT', 'NN', 'POS', 'JJ', 'NN', 'MD', 'VB', 'DT', 'JJ', 'NN', 'IN', 'VBG', 'NN'], 'NER': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, 'Connective': {'RawText': ['then']}, 'Label': '1', 'Sense': ['Temporal.Asynchronous.Precedence'], 'Type': 'Implicit', 'ID': '34607'}\n"
     ]
    }
   ],
   "source": [
    "print(dicList_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:00:54.746271Z",
     "start_time": "2020-10-10T12:00:54.742802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046\n"
     ]
    }
   ],
   "source": [
    "print(len(dicList_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:20.066593Z",
     "start_time": "2020-10-10T12:00:55.491478Z"
    }
   },
   "outputs": [],
   "source": [
    "# 词汇表\n",
    "# Google用word2vec预训练了300维的新闻语料的词向量googlenews-vecctors-negative300.bin\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(base_path + \"model/GoogleNews-vectors-negative300.bin\",\n",
    "                                                        unicode_errors=\"ignore\", binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:20.921535Z",
     "start_time": "2020-10-10T12:01:20.806917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out', 'can', 'It', 'all', 'also', 'two', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'new', 'other', 'her', 'people', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'U.S.', 'three', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'game', 'back', 'because', 'now', '#.#', 'before']\n",
      "[-2.94921875e-01  8.44726562e-02  1.03149414e-02  3.45703125e-01\n",
      " -9.37500000e-02  5.54199219e-02  1.31835938e-01 -2.73437500e-01\n",
      " -1.18164062e-01  3.12500000e-01 -2.96875000e-01 -3.69140625e-01\n",
      "  1.00585938e-01 -1.79687500e-01 -1.68945312e-01  1.12792969e-01\n",
      " -1.22558594e-01  2.75390625e-01 -2.15820312e-01 -4.53125000e-01\n",
      "  9.47265625e-02 -1.61132812e-01  3.20312500e-01 -3.28125000e-01\n",
      "  1.73828125e-01  5.97656250e-01 -2.20703125e-01  1.93359375e-01\n",
      " -4.46777344e-02 -6.25000000e-02 -1.34765625e-01 -1.07910156e-01\n",
      " -3.14453125e-01 -5.15136719e-02 -9.61914062e-02 -1.56250000e-01\n",
      " -2.38281250e-01  1.61132812e-02  6.13403320e-03  8.20312500e-02\n",
      " -6.78710938e-02  1.63085938e-01  2.04101562e-01 -8.83789062e-02\n",
      "  2.05078125e-01  1.46484375e-01 -4.29687500e-02  2.07031250e-01\n",
      " -2.34375000e-01 -1.04980469e-01  1.11328125e-01  2.00195312e-01\n",
      " -4.73632812e-02  7.27539062e-02 -2.91015625e-01  1.56250000e-01\n",
      " -1.96289062e-01 -7.61718750e-02  1.66992188e-01 -2.65625000e-01\n",
      " -7.86132812e-02  1.44531250e-01 -2.45117188e-01 -1.17187500e-01\n",
      " -2.55859375e-01  2.19726562e-01 -2.12890625e-01 -1.63085938e-01\n",
      " -4.08935547e-03  1.28906250e-01  4.37500000e-01  9.61914062e-02\n",
      "  2.83203125e-02 -4.78515625e-02 -1.18652344e-01 -9.17968750e-02\n",
      " -2.12890625e-01  2.19726562e-01 -9.52148438e-02 -1.78710938e-01\n",
      "  1.41906738e-03 -2.67578125e-01  7.66601562e-02 -2.01171875e-01\n",
      "  1.32812500e-01  1.58203125e-01 -2.29492188e-01 -4.10156250e-02\n",
      "  2.50244141e-02  7.81250000e-02 -2.06298828e-02 -5.34667969e-02\n",
      " -9.57031250e-02  1.44531250e-01 -2.23388672e-02 -5.78613281e-02\n",
      "  7.81250000e-02 -2.51953125e-01  2.11914062e-01 -2.69775391e-02\n",
      " -2.57812500e-01 -3.22265625e-02  1.67968750e-01 -1.61132812e-02\n",
      " -5.20019531e-02  7.03125000e-02  8.25195312e-02  2.09960938e-01\n",
      "  4.15039062e-02 -5.12695312e-02 -3.51562500e-01  8.39843750e-02\n",
      " -1.60156250e-01 -1.59179688e-01  1.75781250e-01 -1.39160156e-02\n",
      "  1.69921875e-01 -1.00097656e-01 -9.71679688e-02 -2.29492188e-02\n",
      "  1.19628906e-02  1.85546875e-01  5.27343750e-01  2.14843750e-01\n",
      " -5.90820312e-02  1.39648438e-01  7.37304688e-02 -1.53320312e-01\n",
      "  1.16210938e-01 -6.25000000e-02  4.17480469e-02  2.01416016e-02\n",
      "  2.36816406e-02  5.27343750e-01 -4.41894531e-02  3.80859375e-02\n",
      " -6.40625000e-01  5.20019531e-02 -2.10571289e-03 -8.66699219e-03\n",
      "  1.57226562e-01 -2.05078125e-01  7.42187500e-02  3.05175781e-02\n",
      "  7.56835938e-02  1.82617188e-01  1.19140625e-01 -6.05468750e-02\n",
      " -1.55273438e-01 -6.93359375e-02  2.81250000e-01 -2.34375000e-01\n",
      " -1.77001953e-02 -8.69750977e-04 -6.34765625e-02 -3.20312500e-01\n",
      " -2.43164062e-01  1.66015625e-01 -2.06054688e-01  9.37500000e-02\n",
      " -2.83203125e-01 -2.74658203e-03  4.44335938e-02 -7.66601562e-02\n",
      " -1.48437500e-01 -1.17675781e-01  2.01171875e-01 -3.68652344e-02\n",
      " -2.46093750e-01  2.29492188e-01 -3.28125000e-01  1.41601562e-01\n",
      "  1.63085938e-01  4.78515625e-02  3.36914062e-02 -6.59179688e-02\n",
      "  3.94531250e-01 -9.91210938e-02  3.47656250e-01 -1.55273438e-01\n",
      " -2.04101562e-01  3.60107422e-03 -6.59179688e-02  1.05957031e-01\n",
      "  2.69531250e-01 -5.29785156e-02  2.17773438e-01  5.17578125e-02\n",
      " -7.95898438e-02  1.03515625e-01  1.20605469e-01  1.41601562e-01\n",
      " -1.12792969e-01  2.04101562e-01  4.88281250e-02  1.36718750e-01\n",
      " -4.68750000e-02 -4.53125000e-01  3.90625000e-02 -2.19726562e-01\n",
      " -4.47265625e-01  3.39355469e-02 -6.68945312e-02 -1.37695312e-01\n",
      " -2.77343750e-01  1.75781250e-01  8.83789062e-02  3.06396484e-02\n",
      " -2.49023438e-02 -3.84521484e-03 -2.19726562e-01 -5.45501709e-04\n",
      " -1.66992188e-01  1.23046875e-01  3.98437500e-01  2.86865234e-02\n",
      " -1.10351562e-01  4.27246094e-03  1.37695312e-01  2.61718750e-01\n",
      " -4.39453125e-02  4.63867188e-02 -9.42382812e-02  2.85156250e-01\n",
      "  9.52148438e-02 -3.59375000e-01 -7.86132812e-02  5.02929688e-02\n",
      "  5.66406250e-02 -5.63964844e-02  9.15527344e-03 -1.58203125e-01\n",
      " -4.53125000e-01  1.64794922e-02 -2.51953125e-01 -4.31640625e-01\n",
      " -1.52343750e-01  3.00292969e-02 -1.23046875e-01  1.26953125e-01\n",
      "  2.00195312e-01  3.03955078e-02  3.06640625e-01  9.66796875e-02\n",
      " -2.71484375e-01  1.63085938e-01 -1.20605469e-01  4.63867188e-02\n",
      "  1.14257812e-01 -2.94921875e-01 -1.46484375e-01  2.92968750e-01\n",
      "  2.22656250e-01 -2.29492188e-02 -4.45312500e-01 -4.39453125e-02\n",
      " -1.98242188e-01  3.73535156e-02  2.17285156e-02 -7.37304688e-02\n",
      "  2.01171875e-01  6.44531250e-02  7.27539062e-02 -4.10156250e-01\n",
      " -1.66992188e-01  1.78710938e-01 -3.43750000e-01 -9.27734375e-02\n",
      "  1.81640625e-01  2.21679688e-01 -2.85156250e-01  1.88476562e-01\n",
      "  2.01171875e-01  2.46093750e-01  3.43750000e-01 -5.10253906e-02\n",
      "  3.94531250e-01 -1.55273438e-01 -4.32128906e-02 -1.22558594e-01\n",
      "  1.80664062e-02  1.73828125e-01  3.08593750e-01 -1.76757812e-01\n",
      " -3.41796875e-02  1.06933594e-01  2.85644531e-02  1.30859375e-01\n",
      " -3.41415405e-04  3.04687500e-01 -1.47460938e-01 -6.59179688e-02\n",
      " -1.56250000e-01  2.46093750e-01  2.44140625e-01 -1.01562500e-01\n",
      "  1.11816406e-01  1.14746094e-01  2.21679688e-01 -5.83496094e-02] 300\n"
     ]
    }
   ],
   "source": [
    "print(list(model.vocab.keys())[:100])      \n",
    "\n",
    "print(model['china'], len(model['china']))  # key是单词，value是300维的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "- 注意，train的数据，需要保持 sense 和 Not sense 样本数据量相同\n",
    "\n",
    "  - 下面是用 count1 < count 来控制 数据量相同的\n",
    "\n",
    "- 然而，dev 和 test 不需要保证数据量相同\n",
    "\n",
    "### train数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:21.765947Z",
     "start_time": "2020-10-10T12:01:21.692220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_count: 760\n",
      "neg_count: 760\n",
      "number of all words = 1520\n",
      "is and not\n",
      "\n",
      "760 760\n"
     ]
    }
   ],
   "source": [
    "arg1_train = []       # pdtb中sense为temporal的arg1的词，以[[1],[2], ...]的形式\n",
    "arg2_train = []       # pdtb中sense为temporal的arg2的词，以[[1],[2], ...]的形式\n",
    "\n",
    "arg1_train_pos = []   # 存放arg1词性\n",
    "arg2_train_pos = []   \n",
    "\n",
    "labels_train = []     # train的标签(sense)\n",
    "\n",
    "pos_count = 0         # 是temporal计数\n",
    "neg_count = 0         # 不是temporal计数\n",
    "count_all = 0         # 样本总数\n",
    "\n",
    "word_vocab = []       # 将是temporal的arg1，arg2的词以 ['', '', '', '', ] 的形式\n",
    "pos_vocab = []        # 将是temporal的arg1，arg2的pos标签(词性)以 ['', '', '', '', ]的形式\n",
    "\n",
    "word_embed = []       # 输入的所有单词的词向量(300维)\n",
    "pos_embed = []        # 输入的所有单词词性的词向量(300维)\n",
    " \n",
    "\n",
    "# temporal论元对计数\n",
    "for r in dicList_train:\n",
    "    a = 'False'\n",
    "    for s in r['Sense']:     # 'Sense': ['Temporal.Asynchronous.Precedence', ] (可以有多个)\n",
    "        if sense in s:       # sense = \"Temporal\"\n",
    "            a = \"True\"\n",
    "    \n",
    "    if a == \"True\":\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        neg_count += 1\n",
    "    \n",
    "print(\"pos_count:\", pos_count)\n",
    "print(\"neg_count:\", neg_count)\n",
    "\n",
    "count_all = pos_count + neg_count\n",
    "print(\"number of all words =\", count_all)\n",
    "\n",
    "# 这里是为了平衡 train数据，让正样本和负样本数量一致\n",
    "count = min(pos_count, neg_count)    # 是时序 和 不是时序 的数量的小的值\n",
    "count1 = 0\n",
    "count2 = 0 \n",
    "\n",
    "# 设置标签为 \"Temporal\" 和 \"Not Temporal\"\n",
    "for r in dicList_train:\n",
    "    a = \"False\"\n",
    "    for s in r[\"Sense\"]:\n",
    "        if sense in s:\n",
    "            a = \"True\"\n",
    "    \n",
    "    if a == \"True\" and count1 < count:  # 是时序，且count1数不到count\n",
    "        # 存放每个样本的标签(sense)\n",
    "        labels_train.append(sense)       # Temporal\n",
    "        \n",
    "        # 对每个w（r[\"Arg1\"][\"Word\"]]）如果w不在词向量表中，返回UNK（unknown），否则返回r[\"Arg1\"][\"Word\"]]小写\n",
    "        # [['clad', 'in', 'his', 'trademark', 'black', 'velvet', 'suit', '<UNK/>', 'the', '<UNK/>', 'clarinetist', 'announced', 'that', 'his', 'new', 'album', '<UNK/>', '<UNK/>', 'inner', 'voices', '<UNK/>', '<UNK/>', 'had', 'just', 'been', 'released', '<UNK/>', 'that', 'his', 'family', 'was', 'in', 'the', 'front', 'row', '<UNK/>', '<UNK/>', 'that', 'it', 'was', 'his', 'mother', '<UNK/>', 'birthday', '<UNK/>', 'so', 'he', 'was', 'going', '<UNK/>', 'play', 'her', 'favorite', 'tune', 'from', 'the', 'record'], ['<UNK/>', '<UNK/>', 'introduced', 'his', 'colleagues', '<UNK/>', 'bill', 'douglas', '<UNK/>', '<UNK/>', '<UNK/>', 'an', 'old', 'buddy', 'from', 'yale', '<UNK/>', '<UNK/>', 'jazz', 'bassist', 'eddie', 'gomez'], ['an', 'improvisational', 'section', 'was', 'built', 'around', 'pieces', 'by', '<UNK/>', 'douglas', '<UNK/>', 'beginning', 'with', '<UNK/>', 'golden', 'rain', '<UNK/>', '<UNK/>', '<UNK/>', 'lilting', '<UNK/>', '<UNK/>', 'lead', 'in', '<UNK/>', 'the', 'uptempo', '<UNK/>', 'sky', '<UNK/>', '<UNK/>', 'which', 'gave', '<UNK/>', '<UNK/>', 'the', 'opportunity', '<UNK/>', 'wail', 'in', '<UNK/>', 'high', 'register', '<UNK/>', 'show', 'off', 'his', 'fleet', 'fingers']]\n",
    "        arg1_train.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r['Arg1'][\"Word\"]])\n",
    "        arg2_train.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r['Arg2'][\"Word\"]])\n",
    "        # 存放 Arg1,Arg2的所有单词\n",
    "        # word_vocab: ['clad', 'in', 'his', 'trademark', 'black', 'velvet', 'suit', ',', 'the', 'soft-spoken', 'clarinetist', 'announced', 'that', 'his', 'new', 'album', ',', '``', 'inner', 'voices', ',', \"''\", 'had', 'just', 'been', 'released', ',', 'that', 'his', 'family', 'was', 'in', 'the', 'front', 'row', ',', 'and', 'that', 'it', 'was', 'his', 'mother', \"'s\", 'birthday', ',', 'so', 'he', 'was', 'going', 'to', 'play', 'her', 'favorite', 'tune', 'from', 'the', 'record', 'he', 'launched', 'into', 'saint-saens', \"'s\", '``', 'the', 'swan', \"''\", 'from', '``', 'carnival', 'of', 'the', 'animals', ',', \"''\", 'a', 'favorite', 'encore', 'piece', 'for', 'cellists', ',', 'with', 'lovely', ',', 'glossy', 'tone', 'and', 'no', 'bite', 'mr.', 'stoltzman', 'introduced', 'his', 'colleagues', ':', 'bill', 'douglas', ',', 'pianist/bassoonist/composer', 'and', 'an', 'old', 'buddy', 'from', 'yale', ',', 'and', 'jazz', 'bassist', 'eddie', 'gomez', 'an', 'improvisational', 'section', 'was', 'built', 'around', 'pieces', 'by', 'mr.', 'douglas', ',', 'beginning', 'with', '``', 'golden', 'rain', ',', \"''\", 'a', 'lilting', ',', 'laid-back', 'lead', 'in', 'to', 'the', 'uptempo', '``', 'sky', ',', \"''\", 'which', 'gave', 'mr.', 'stoltzman', 'the', 'opportunity', 'to', 'wail', 'in', 'a', 'high', 'register', 'and', 'show', 'off', 'his', 'fleet', 'fingers', 'an', 'improvisational', 'section', 'was', 'built', 'around', 'pieces', 'by', 'mr.', 'douglas', ',', 'beginning', 'with', '``', 'golden', 'rain', ',', \"''\", 'a', 'lilting', ',', 'laid-back', 'lead', 'in', 'to', 'the', 'uptempo', '``', 'sky', ',', \"''\", 'which', 'gave', 'mr.', 'stoltzman', 'the', 'opportunity', 'to', 'wail', 'in', 'a', 'high', 'register', 'and', 'show', 'off', 'his', 'fleet', 'fingers', 'bach', \"'s\", '``', 'air', \"''\", 'followed']\n",
    "        word_vocab.extend([w.lower() for w in r['Arg1']['Word']])\n",
    "        word_vocab.extend([w.lower() for w in r['Arg2']['Word']])\n",
    "        \n",
    "        arg1_train_pos.append(r['Arg1']['POS'])\n",
    "        arg2_train_pos.append(r['Arg2']['POS'])    \n",
    "        # 存放 Arg1,Arg2的所有POS(词性)\n",
    "        # pos_vocab: ['vbn', 'in', 'prp$', 'nn', 'jj', 'nn', 'nn', ',', 'dt', 'jj', 'nn', 'vbd', 'in', 'prp$', 'jj', 'nn', ',', '``', 'jj', 'nns', ',', \"''\", 'vbd', 'rb', 'vbn', 'vbn', ',', 'in', 'prp$', 'nn', 'vbd', 'in', 'dt', 'jj', 'nn', ',', 'cc', 'in', 'prp', 'vbd', 'prp$', 'nn', 'pos', 'nn', ',', 'rb', 'prp', 'vbd', 'vbg', 'to', 'vb', 'prp$', 'jj', 'nn', 'in', 'dt', 'nn', 'prp', 'vbd', 'in', 'nnp', 'pos', '``', 'dt', 'nn', \"''\", 'in', '``', 'nnp', 'in', 'dt', 'nns', ',', \"''\", 'dt', 'jj', 'nn', 'nn', 'in', 'nns', ',', 'in', 'jj', ',', 'jj', 'nn', 'cc', 'dt', 'nn', 'nnp', 'nnp', 'vbd', 'prp$', 'nns', ':', 'nnp', 'nnp', ',', 'nn', 'cc', 'dt', 'jj', 'nn', 'in', 'nnp', ',', 'cc', 'nn', 'nn', 'nnp', 'nnp', 'dt', 'jj', 'nn', 'vbd', 'vbn', 'in', 'nns', 'in', 'nnp', 'nnp', ',', 'vbg', 'in', '``', 'nnp', 'nn', ',', \"''\", 'dt', 'jj', ',', 'jj', 'nn', 'in', 'to', 'dt', 'jj', '``', 'nnp', ',', \"''\", 'wdt', 'vbd', 'nnp', 'nnp', 'dt', 'nn', 'to', 'vb', 'in', 'dt', 'jj', 'nn', 'cc', 'vb', 'rp', 'prp$', 'nn', 'nns', 'dt', 'jj', 'nn', 'vbd', 'vbn', 'in', 'nns', 'in', 'nnp', 'nnp', ',', 'vbg', 'in', '``', 'nnp', 'nn', ',', \"''\", 'dt', 'jj', ',', 'jj', 'nn', 'in', 'to', 'dt', 'jj', '``', 'nnp', ',', \"''\", 'wdt', 'vbd', 'nnp', 'nnp', 'dt', 'nn', 'to', 'vb', 'in', 'dt', 'jj', 'nn', 'cc', 'vb', 'rp', 'prp$', 'nn', 'nns', 'nnp', 'pos', '``', 'nnp', \"''\", 'vbd']\n",
    "        pos_vocab.extend([w.lower() for w in r['Arg1'][\"POS\"]])\n",
    "        pos_vocab.extend([w.lower() for w in r['Arg2'][\"POS\"]])\n",
    "        \n",
    "        count1 += 1\n",
    "    \n",
    "    elif a != 'True' and count2 < count:  # 不是时序且count2数不到count\n",
    "        labels_train.append(\"Not\" + sense)\n",
    "        arg1_train.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "        arg2_train.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "        word_vocab.extend([w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "        word_vocab.extend([w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "       \n",
    "        arg1_train_pos.append(r[\"Arg1\"][\"POS\"])\n",
    "        arg2_train_pos.append(r[\"Arg2\"][\"POS\"])\n",
    "        pos_vocab.extend([w.lower() for w in r[\"Arg1\"][\"POS\"]])\n",
    "        pos_vocab.extend([w.lower() for w in r[\"Arg2\"][\"POS\"]])\n",
    "        count2 += 1\n",
    "        \n",
    "# 正样本 和 负样本 数据量相同！\n",
    "print(\"is and not\\n\")\n",
    "print(count1, count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev 和 test 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:22.723912Z",
     "start_time": "2020-10-10T12:01:22.710142Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_not_balance_data(_dic1):\n",
    "    global word_vocab, pos_vocab\n",
    "    arg1 = []\n",
    "    arg2 = []\n",
    "    arg1_pos = []\n",
    "    arg2_pos = []\n",
    "    labels = []\n",
    "    for r in _dic1:\n",
    "        a = 'False'\n",
    "        for s in r['Sense']:\n",
    "            if sense in s:\n",
    "                a = 'True'\n",
    "        \n",
    "        if a == \"True\":\n",
    "            labels.append(sense)\n",
    "            arg1.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "            arg2.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "            word_vocab.extend([w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "            word_vocab.extend([w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "            arg1_pos.append(r[\"Arg1\"][\"POS\"])\n",
    "            arg2_pos.append(r[\"Arg2\"][\"POS\"])\n",
    "            pos_vocab.extend([w.lower() for w in r[\"Arg1\"][\"POS\"]])\n",
    "            pos_vocab.extend([w.lower() for w in r[\"Arg2\"][\"POS\"]])\n",
    "            \n",
    "        elif a != \"True\":\n",
    "            labels.append(\"Not\" + sense)\n",
    "            arg1.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "            arg2.append([\"<UNK/>\" if w.lower() not in model.vocab.keys() else w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "            word_vocab.extend([w.lower() for w in r[\"Arg1\"][\"Word\"]])\n",
    "            word_vocab.extend([w.lower() for w in r[\"Arg2\"][\"Word\"]])\n",
    "            arg1_pos.append(r[\"Arg1\"][\"POS\"])\n",
    "            arg2_pos.append(r[\"Arg2\"][\"POS\"])\n",
    "            pos_vocab.extend([w.lower() for w in r[\"Arg1\"][\"POS\"]])\n",
    "            pos_vocab.extend([w.lower() for w in r[\"Arg2\"][\"POS\"]])\n",
    "            \n",
    "    return arg1, arg2, arg1_pos, arg2_pos, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:23.746565Z",
     "start_time": "2020-10-10T12:01:23.669408Z"
    }
   },
   "outputs": [],
   "source": [
    "arg1_dev, arg2_dev, arg1_dev_pos, arg2_dev_pos, labels_dev = get_not_balance_data(dicList_dev)\n",
    "arg1_test, arg2_test, arg1_test_pos, arg2_test_pos, labels_test = get_not_balance_data(dicList_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_vocab 和 pos_vocab处理\n",
    "\n",
    "- 开始位置添加 `<PAD/>` 和 `<UNK/>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:01:24.990098Z",
     "start_time": "2020-10-10T12:01:24.964117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vacab.len, pos_vocab.len, arg1_test.len, arg1_train.len\n",
      " \n",
      "142509\n",
      "142509\n",
      "1520\n",
      "15062 46\n",
      "['swathed', 'jansz', 'write-downs', '9.51', '1.88', 'significance', 'drop', 'elephant', 'insisted', 'sprung']\n",
      "['wrb', 'vbz', '<PAD/>', 'rp', 'pdt', 'rbr', 'cd', 'wp$', 'jj', 'nn', 'nns', 'rb', 'vbg', 'vb', 'nnps', 'jjs', 'nnp', 'uh', 'fw', 'ex', 'vbp', 'wp', ':', 'pos', 'vbd', 'rbs', '$', 'vbn', 'jjr', 'prp$', ',', 'in', 'prp', '.', 'ls', '<UNK/>', 'dt', 'cc', '-lrb-', 'to', 'md', '#', \"''\", '-rrb-', 'wdt', '``']\n"
     ]
    }
   ],
   "source": [
    "print(\"word_vacab.len, pos_vocab.len, arg1_test.len, arg1_train.len\\n \")\n",
    "print(len(word_vocab))\n",
    "print(len(pos_vocab))\n",
    "print(len(arg1_train))\n",
    "\n",
    "# 在word_vocab \\ pos_vocab 开始位置插入<UNK/> <PAD/>\n",
    "word_vocab.insert(0, '<PAD/>')\n",
    "pos_vocab.insert(0, '<PAD/>')\n",
    "word_vocab.insert(0, '<UNK/>')\n",
    "pos_vocab.insert(0, '<UNK/>')\n",
    "\n",
    "# 去重\n",
    "word_vocab = list(set(word_vocab))\n",
    "pos_vocab = list(set(pos_vocab))\n",
    "\n",
    "print(len(word_vocab), len(pos_vocab))\n",
    "print(word_vocab[:10])\n",
    "print(pos_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:20:02.774895Z",
     "start_time": "2020-10-10T12:20:02.617516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of word_embed:  15062\n",
      "len of pos_embed:  46\n"
     ]
    }
   ],
   "source": [
    "word_embed = []       # 输入的所有单词的词向量(300维)\n",
    "pos_embed = []        # 输入的所有单词词性的词向量(300维)\n",
    "\n",
    "for word in word_vocab:\n",
    "    try:\n",
    "        word_embed.append(model[word.lower()])\n",
    "    except:\n",
    "        # 从一个均匀分布[-1,1)中随机采样300个数，并tolist()将数组转列表（一维列表）\n",
    "        word_embed.append(np.random.uniform(-1, 1, 300).tolist())\n",
    "    \n",
    "print(\"len of word_embed: \", len(word_embed))\n",
    "# word_embed.insert(0, [0] * 300)   # insert 后，会比word_vocab多一个成员\n",
    "\n",
    "for pos in pos_vocab:\n",
    "    pos_embed.append(np.random.uniform(-1, 1, 300).tolist())\n",
    "    \n",
    "print(\"len of pos_embed: \", len(pos_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储预处理数据\n",
    "\n",
    "- dump()方法\n",
    "\n",
    " - `pickle.dump(obj, file, [,protocol])`：序列化对象，将对象obj保存到文件file中去。\n",
    " \n",
    " - 参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。 其中，1是老式的二进制协议；2是新二进制协议）。\n",
    " \n",
    " - file表示保存到的类文件对象，file必须有write()接口，file可以是一个以'w'打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。\n",
    "\n",
    "\n",
    "\n",
    "- load()方法\n",
    "\n",
    " - `pickle.load(file)`\n",
    "\n",
    " - 注释：反序列化对象，将文件中的数据解析为一个python对象。file中有read()接口和readline()接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:25:11.006870Z",
     "start_time": "2020-10-10T12:25:10.828565Z"
    }
   },
   "outputs": [],
   "source": [
    "url_preprocess = base_path + '/preprocess/'\n",
    "\n",
    "# train\n",
    "with open(url_preprocess + 'train_temporal_word.pkl', 'wb') as f:\n",
    "    pickle.dump([arg1_train, arg2_train, labels_train], f)\n",
    "\n",
    "# dev\n",
    "with open(url_preprocess + 'dev_temporal_word.pkl', 'wb') as f:\n",
    "    pickle.dump([arg1_dev, arg2_dev, labels_dev], f)\n",
    "\n",
    "# test\n",
    "with open(url_preprocess + 'test_temporal_word.pkl', 'wb') as f:\n",
    "    pickle.dump([arg1_test, arg2_test, labels_test], f)\n",
    "\n",
    "with open(url_preprocess + 'test_temporal_pos.pkl', 'wb') as f:\n",
    "    pickle.dump([arg1_test_pos, arg2_test_pos, labels_test], f)\n",
    "\n",
    "# pos_vocab_embed 和 word_vocab_embed\n",
    "with open(url_preprocess + 'pos_vocab_embd_word_vocab_embd_temporal.pkl', 'wb') as f:\n",
    "    pickle.dump([pos_vocab, pos_embed, word_vocab, word_embed], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:10:42.795604Z",
     "start_time": "2020-10-10T12:10:42.793175Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(url_preprocess + 'test_temporal_pos.pkl', 'rb') as f:\n",
    "#     t = pickle.load(f)    # 取数据\n",
    "#     print(t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_cnn.py\n",
    "\n",
    "- 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.505647Z",
     "start_time": "2020-10-10T12:01:04.507Z"
    }
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.518560Z",
     "start_time": "2020-10-10T12:01:05.354Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length_arg1, sequence_length_arg2, \n",
    "                       word_embedding_size, word_vocab_size, num_classes,\n",
    "                       filter_sizes, num_filters, l2_reg_lambda = 0.0):\n",
    "        '''\n",
    "        :param sequence_length_arg1: 序列1长度\n",
    "        :param word_embedding_size:  词向量维度\n",
    "        :param word_vocab_size:      词汇表大小\n",
    "        :param sequence_length_arg2: 序列2长度\n",
    "        :param num_classes:          输出的类别数\n",
    "        :param filter_sizes:         卷积核尺寸\n",
    "        :param num_filters:          卷积核数目\n",
    "        :param l2_reg_lambda:        l2正则化超参数\n",
    "        '''\n",
    "        # Placeholders for input, output and dropout\n",
    "        # None: 表示batch_size是任意值\n",
    "        self.input_x_arg1_word = tf.placeholder(tf.int32, [None, sequence_length_arg1], name='input_x_arg1_word')\n",
    "        self.input_x_arg2_word = tf.placeholder(tf.int32, [None, sequence_length_arg2], name='input_x_arg2_word')\n",
    "        self.input_y = tf.placeholder(tf.int64, [None, num_classes], name='input_y')\n",
    "        # tf是保留结点比例\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        # Keeping track of L2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)                 # 使用l2正则化的loss\n",
    "        embedding_size = word_embedding_size\n",
    "        vocab_size = word_vocab_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            self._W_emb = _variable_on_cpu(name='embedding', shape=[vocab_size, embedding_size], \n",
    "                                           initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n",
    "            # embedding_lookup return tensor\n",
    "            # tf.nn.embedding_lookup（params, ids）: 选取一个张量里面索引对应的元素; 选取一个张量里面索引对应的元素\n",
    "            # [batch_size, seq_len, embedding_size]\n",
    "            self.embedded_chars_arg1 = tf.nn.embedding_lookup(self._W_emb, self.input_x_arg1_word) # get embedding by id\n",
    "            self.embedded_chars_arg2 = tf.nn.embedding_lookup(self._W_emb, self.input_x_arg2_word)\n",
    "            \n",
    "            # 't' is a tensor of shape [2]：shape(expand_dims(t, 0)) ==> [1, 2]; shape(expand_dims(t, -1)) ==> [2, 1] \n",
    "            # [batch_size, seq_len, embedding_size, 1]\n",
    "            self.embedded_chars_expanded_arg1 = tf.expand_dims(self.embedded_chars_arg1, -1)\n",
    "            self.embedded_chars_expanded_arg2 = tf.expand_dims(self.embedded_chars_arg2, -1)\n",
    "            \n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_output_arg1 = []\n",
    "            pooled_output_arg2 = []\n",
    "            \n",
    "            # filter的种类\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Convolution layer\n",
    "                    # 卷积核大小为(filter_size, embedding_dim)\n",
    "                    # 1表示输入数据通道数，num_filters即输出数据通道数\n",
    "                    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                    \n",
    "                    # 截断的产生正态分布的随机数，即随机数与均值的差值若大于两倍的标准差，则重新生成。\n",
    "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W') \n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "                    \n",
    "                    conv_arg1 = tf.nn.conv2d(\n",
    "                        # input: [batch, in_height, in_weight, in_channel]\n",
    "                        # [batch, seq_len, embedding_size, 1] \n",
    "                        self.embedded_chars_expanded_arg1, \n",
    "                        # filter: [filter_height, filter_weight, in_channel, out_channels]\n",
    "                        # [filter_size, embedding_size, 1, num_filters]\n",
    "                        W,                       \n",
    "                        strides = [1, 1, 1, 1],  # strides: [1, strides, strides, 1], 第一位和最后一位固定为1\n",
    "                        padding = 'VALID',       # padding：值为“SAME” 和 “VALID”,卷积的形式，\"SAME\"是考虑边界，不足的时候用0去填充周围，\"VALID\"则不考虑\n",
    "                        name = 'conv_arg1'\n",
    "                    )\n",
    "                    # [b,s,e,1] [fz, e, 1, num_filters] = [b, s, fz, num_filters] ????\n",
    "                    \n",
    "                    conv_arg2 = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded_arg2,\n",
    "                        W,\n",
    "                        strides = [1, 1, 1, 1],\n",
    "                        padding = 'VALID',\n",
    "                        name = 'conv_arg2'\n",
    "                    )\n",
    "                    \n",
    "                    # Apply nonlinearity\n",
    "                    # activation function [b,s,(1,2,3),1024]\n",
    "                    # (1,2,3)表示filter_size分别为1,2,3情况，1024为num_filters\n",
    "                    h_arg1 = tf.nn.relu(tf.nn.bias_add(conv_arg1, b), name='relu_arg1')\n",
    "                    h_arg2 = tf.nn.relu(tf.nn.bias_add(conv_arg2, b), name='relu_arg2')\n",
    "            \n",
    "                    # Maxpooling over the outputs\n",
    "                    pooled_arg1 = tf.nn.max_pool(\n",
    "                        h_arg1,\n",
    "                        ksize = [1, sequence_length_arg1 - filter_size + 1, 1, 1], # 不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "                        strides = [1, 1, 1, 1],\n",
    "                        padding = 'VALID',\n",
    "                        name = 'pool_arg1'\n",
    "                    )\n",
    "                    \n",
    "                    pooled_arg2 = tf.nn.max_pool(\n",
    "                        h_arg2,\n",
    "                        ksize=[1, sequence_length_arg2 - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool_arg2\")\n",
    "                    \n",
    "                    pooled_output_arg1.append(pooled_arg1)\n",
    "                    pooled_output_arg2.append(pooled_arg2)\n",
    "                    \n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = num_filters * len(filter_sizes)\n",
    "            # 3个filter，由于每个filter输出数据通道num_filters，\n",
    "            self.h_pool_arg1 = tf.concat(pooled_output_arg1, 3)   # 连接最大池化后\n",
    "            self.h_pool_arg2 = tf.concat(pooled_output_arg2, 3)   \n",
    "            \n",
    "            # 经过flatten可以得到 num_filters*3维向量\n",
    "            self.h_pool_flat_arg1 = tf.reshape(self.h_pool_arg1, [-1, num_filters_total])\n",
    "            self.h_pool_flat_arg2 = tf.reshape(self.h_pool_arg2, [-1, num_filters_total])\n",
    "            self.h_pool_flat = tf.concat([self.h_pool_flat_arg1, self.h_pool_flat_arg2], axis=1, name='h') \n",
    "            # [batch, num_filters_total*2]\n",
    "            \n",
    "            # Add dropout\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)  # 防止过拟合\n",
    "                \n",
    "            # Final (unnormalized) scores and predictions\n",
    "            # Full connected\n",
    "            with tf.name_scope(\"output\"):\n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[2 * num_filters_total, num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "                )\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n",
    "                \n",
    "                # L2正则化\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                \n",
    "                # tf.matmul(x, w) + b\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name='scores')\n",
    "                self.predictions = tf.argmax(self.scores, 1, name='predictions') \n",
    "\n",
    "            # CaluateMean cross-entropy loss \n",
    "            with tf.name_scope('loss'):\n",
    "                # 第一步是先对网络最后一层的输出做一个softmax\n",
    "                # 第二步是softmax的输出向量[Y1，Y2,Y3...]和样本的实际标签做一个交叉熵\n",
    "                self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y, name='losses')\n",
    "                self.loss = tf.reduce_mean(self.losses) + l2_reg_lambda * l2_loss\n",
    "                \n",
    "            # Accuracy\n",
    "            with tf.name_scope('accuracy'):\n",
    "                # 总体准确率\n",
    "                correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "                \n",
    "                all_positive_indexes = tf.where(self.predictions > 0)           # 正例的索引\n",
    "                all_positive_indexes = tf.reshape(all_positive_indexes, [-1])\n",
    "                \n",
    "                positive_pred = tf.nn.embedding_lookup(self.predictions, all_positive_indexes)\n",
    "                positive_label = tf.nn.embedding_lookup(tf.argmax(self.input_y, 1), all_positive_indexes)\n",
    "                \n",
    "                # 比较 预测的正例 和 label\n",
    "                correct_positive_predictions = tf.equal(positive_pred, positive_label)\n",
    "                \n",
    "                # Precision值，精确率\n",
    "                # TP / (TP + FP)\n",
    "                self.precision = tf.reduce_mean(tf.cast(correct_positive_predictions, dtype=tf.float32), name='precision')\n",
    "                \n",
    "                # Recall值，召回率\n",
    "                # TP / (TP + FN) = TP / P\n",
    "                # TP: 正确预测正例\n",
    "                tp = tf.reduce_sum(tf.cast(correct_positive_predictions, dtype=tf.float32))\n",
    "                \n",
    "                # P = TP + FN\n",
    "                self.label_index = tf.cast(tf.argmax(self.input_y, 1), dtype=tf.int32)\n",
    "                tpfn = tf.reduce_sum(tf.where(self.label_index > 0, \n",
    "                                              tf.ones_like(self.label_index, dtype=tf.float32),   # 索引>0, 标志为1\n",
    "                                              tf.zeros_like(self.label_index, dtype=tf.float32))) # 否则，标志为0\n",
    "                \n",
    "                # 函数返回以浮点数进行计算的 x/y.\n",
    "                self.recall = tf.truediv(tp, tpfn, name='recall')\n",
    "                \n",
    "    # 使用预训练词向量\n",
    "    def assign_embedding(self, session, pretrained_word_embedding):\n",
    "        session.run(tf.assign(self._W_emb, pretrained_word_embedding))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py\n",
    "\n",
    "- 工具函数\n",
    "\n",
    " - data_helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.530036Z",
     "start_time": "2020-10-10T12:01:06.709Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "import itertools\n",
    "from collections import Counter\n",
    "from codecs import open\n",
    "import gensim\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "    \n",
    "def load_word_pkl(filename):\n",
    "    with open(filename, 'rb') as file_pkl:\n",
    "        x_text_arg1, x_text_arg2, labels = pickle.load(file_pkl)\n",
    "        \n",
    "    # one-hot：Not temporal--[1, 0], temporal--[0, 1]\n",
    "    labels = [[1, 0] if \"Not\" in label else [0, 1] for label in labels]\n",
    "    \n",
    "    return x_text_arg1, x_text_arg2, labels\n",
    "\n",
    "\n",
    "# 处理输入数据\n",
    "# 1. 填充长度不满足max_document_length的句子\n",
    "# 2. return the index of the word in the vocab\n",
    "def build_input_data(sentences, vocab, max_document_length, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    return the index of the word in the vocab\n",
    "    \"\"\"\n",
    "    padded_sentences = [x[:max_document_length - 1] + [padding_word] * max(max_document_length - len(x), 1) for x in sentences]\n",
    "    \n",
    "    x_arg = np.array([[vocab.index(word) for word in sentence] for sentence in padded_sentences])\n",
    "    \n",
    "    return x_arg \n",
    "\n",
    "# 生成batch数据\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    \n",
    "    data_size = len(data)\n",
    "    \n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffle_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffle_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffle_data[start_index : end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "\n",
    "- 这里需要读取命令行参数，写成 .py 文件了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.541913Z",
     "start_time": "2020-10-10T12:01:07.828Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "# import data_helpers\n",
    "# from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.553332Z",
     "start_time": "2020-10-10T12:01:08.469Z"
    }
   },
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'dev_sample_percentage' is defined twice. First from /home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: Percentage of the training data to use for validation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fea988d06c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Data loading params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev_sample_percentage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Percentage of the training data to use for validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positive_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/rt-polaritydata/rt-polarity.pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the positive data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/rt-polaritydata/rt-polarity.neg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the negative data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF1/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF1/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_float\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF1/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m    101\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    102\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       module_name)\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF1/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF1/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'dev_sample_percentage' is defined twice. First from /home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: Percentage of the training data to use for validation"
     ]
    }
   ],
   "source": [
    "result_path = base_path + 'experiment/temporal20201009/result1.txt'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"      # 指定gpu版本\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 64, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1,2,3\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 1024, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.2, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "tf.flags.DEFINE_boolean(\"use_pretrain\",True,\"use pretrained embedding\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"max_document_length\", 80, \"Max document length\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()        # 后来版本不用了\n",
    "FLAGS.flag_values_dict()    \n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.564438Z",
     "start_time": "2020-10-10T12:01:09.883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "# load pos file\n",
    "# x_text_train_arg1_pos, x_text_train_arg2_pos, y_train = data_helpers.load_pos_pkl('./PDTB/one_vs_others_standard/Cause_vs_others/pdtb_with_repetiton_relation_base_pkl/train_cause_pos.pkl')\n",
    "# x_text_test_arg1_pos, x_text_test_arg2_pos, y_test = data_helpers.load_pos_pkl('./PDTB/one_vs_others_standard/Cause_vs_others/pdtb_with_repetiton_relation_base_pkl/test_cause_pos.pkl')\n",
    "\n",
    "# load word file\n",
    "x_text_train_arg1_word, x_text_train_arg2_word, y_train = load_word_pkl('./preprocess/train_temporal_word.pkl')\n",
    "x_text_test_arg1_word, x_text_test_arg2_word, y_test = load_word_pkl('./preprocess/test_temporal_word.pkl')\n",
    "\n",
    "with open('./preprocess/pos_vocab_embd_word_vocab_embd_temporal.pkl', 'rb') as file_pkl:\n",
    "    _, _, word_vocab, word_embd = pickle.load(file_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.575354Z",
     "start_time": "2020-10-10T12:01:10.341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<UNK/>', 'viewers', 'will', 'be', 'given', '<UNK/>', '<UNK/>', 'number', '<UNK/>', 'call'], ['an', 'assassin', 'in', 'colombia', 'killed', '<UNK/>', 'federal', 'judge', 'on', '<UNK/>', '<UNK/>', 'street'], ['yesterday', '<UNK/>', 'selling', 'began', 'after', '<UNK/>', 'japanese', 'news', 'agency', 'reported', 'that', 'japanese', 'banks', '<UNK/>', 'which', 'balked', 'at', 'the', 'first', 'bid', '<UNK/>', 'were', 'ready', '<UNK/>', 'reject', '<UNK/>', 'revised', 'version', 'at', 'around', '$', '<UNK/>', '<UNK/>', 'share', '<UNK/>', 'or', '$', '<UNK/>', 'billion'], ['the', 'machinists', 'also', 'asked', 'for', 'an', 'investigation', 'by', 'the', 'securities', '<UNK/>', 'exchange', 'commission', 'into', 'possible', '<UNK/>', 'violations', 'in', 'the', 'original', 'bid', 'for', 'ual', 'by', '<UNK/>', 'davis', '<UNK/>', 'as', 'well', 'as', 'in', 'the', 'response', 'by', 'ual'], ['the', 'previous', 'period', '<UNK/>', 'results', 'included', '<UNK/>', '$', '<UNK/>', 'million', 'pretax', 'charge', 'related', '<UNK/>', 'unrecoverable', 'contract', 'costs', '<UNK/>', '<UNK/>', '$', '<UNK/>', 'million', 'pretax', 'gain', 'on', 'real', 'estate', 'sales']]\n",
      "[[0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(x_text_test_arg1_word[:5])\n",
    "print(y_train[:5])\n",
    "\n",
    "word_embedding_dim = len(word_embd[0])\n",
    "\n",
    "print(word_embedding_dim)   # 查看词向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.586408Z",
     "start_time": "2020-10-10T12:01:10.884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8916   429 14714  6863   937  9042   517  4076  3073  4076  5973  2863\n",
      "   1889 14714 13624  3005  4076  4076 13973  6347  4076  4076  6769  6889\n",
      "   9243  5582  4076  1889 14714  5843 14070   429  3073 13734  7732  4076\n",
      "   4076  1889   597 14070 14714 10769  4076  1879  4076  4386 12352 14070\n",
      "  10888  4076  6197  5213 14090  1779  5720  3073 11882  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976]\n",
      " [ 4076  4076   721 14714  6104  4076  6763   688  4076  4076  4076  1151\n",
      "   1919  7389  5720  9185  4076  4076   876 10822 10437  5024  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976  8976\n",
      "   8976  8976  8976  8976  8976  8976  8976  8976]]\n"
     ]
    }
   ],
   "source": [
    "# 每个句子都不一样长，需要把他们pad成一样的长度，并得到索引\n",
    "x_train_arg1_word = build_input_data(x_text_train_arg1_word, word_vocab, 80)\n",
    "x_train_arg2_word = build_input_data(x_text_train_arg2_word, word_vocab, 80)\n",
    "x_test_arg1_word = build_input_data(x_text_test_arg1_word, word_vocab, 80)\n",
    "x_test_arg2_word = build_input_data(x_text_test_arg2_word, word_vocab, 80)\n",
    "\n",
    "print(x_train_arg1_word[:2])   # 句子的每个单词在单词表的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.603046Z",
     "start_time": "2020-10-10T12:01:11.508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15062, 300)\n",
      "data load successfully!\n"
     ]
    }
   ],
   "source": [
    "# pretrained_pos_embedding = np.array(pos_embd)\n",
    "pretrained_word_embedding = np.array(word_embd)\n",
    "print(pretrained_word_embedding.shape)\n",
    "\n",
    "print('data load successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:18:56.618777Z",
     "start_time": "2020-10-10T12:01:12.075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 1520/1046\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "# dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "# x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "# y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "# print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "\n",
    "np.random.seed(10)\n",
    "# permutation不直接在原来的数组上进行操作，而是返回一个新的打乱顺序的数组，并不打乱原来的数组\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "# np.nrange作用是一个序列，可被当做向量使用,支持步长为小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T12:59:31.311196Z",
     "start_time": "2020-10-10T12:59:31.218448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_length_arg1 :80\n",
      "sequence_length_arg2:80\n",
      "num_classes:2\n",
      "word_vocab_size:15062\n",
      "word_embedding_size:300\n",
      "filter_sizes:[1, 2, 3]\n",
      "\n",
      "INFO:tensorflow:Summary name embedding_1:0/grad/hist is illegal; using embedding_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding_1:0/grad/sparsity is illegal; using embedding_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-1/W:0/grad/hist is illegal; using embedding/conv-maxpool-1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-1/W:0/grad/sparsity is illegal; using embedding/conv-maxpool-1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-1/b:0/grad/hist is illegal; using embedding/conv-maxpool-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-1/b:0/grad/sparsity is illegal; using embedding/conv-maxpool-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-2/W:0/grad/hist is illegal; using embedding/conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-2/W:0/grad/sparsity is illegal; using embedding/conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-2/b:0/grad/hist is illegal; using embedding/conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-2/b:0/grad/sparsity is illegal; using embedding/conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-3/W:0/grad/hist is illegal; using embedding/conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-3/W:0/grad/sparsity is illegal; using embedding/conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-3/b:0/grad/hist is illegal; using embedding/conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/conv-maxpool-3/b:0/grad/sparsity is illegal; using embedding/conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name embedding/output/b:0/grad/hist is illegal; using embedding/output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/output/b:0/grad/sparsity is illegal; using embedding/output/b_0/grad/sparsity instead.\n",
      "Writing to /data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/runs/explicit_1602339881\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjdou/anaconda3/envs/TF1/lib/python3.6/site-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal 2020-10-10T22:24:43.055404: step 1, loss 4.18811, acc 0.515625, precision 0.5, recall 0.387097\n",
      "temporal 2020-10-10T22:24:43.324424: step 2, loss 6.19421, acc 0.390625, precision 0.466667, recall 0.583333\n",
      "temporal 2020-10-10T22:24:43.564212: step 3, loss 4.91019, acc 0.5, precision 0.510204, recall 0.757576\n",
      "temporal 2020-10-10T22:24:43.804747: step 4, loss 5.29958, acc 0.4375, precision 0.4, recall 0.117647\n",
      "temporal 2020-10-10T22:24:44.041643: step 5, loss 4.04662, acc 0.5, precision 0.411765, recall 0.241379\n",
      "temporal 2020-10-10T22:24:44.298926: step 6, loss 3.47414, acc 0.609375, precision 0.566038, recall 0.9375\n",
      "temporal 2020-10-10T22:24:44.534565: step 7, loss 4.86689, acc 0.546875, precision 0.552632, recall 0.636364\n",
      "temporal 2020-10-10T22:24:44.769879: step 8, loss 3.98059, acc 0.4375, precision 0.5, recall 0.361111\n",
      "temporal 2020-10-10T22:24:45.026726: step 9, loss 2.85719, acc 0.609375, precision 0.733333, recall 0.564103\n",
      "temporal 2020-10-10T22:24:45.260509: step 10, loss 3.82631, acc 0.46875, precision 0.512821, recall 0.571429\n",
      "temporal 2020-10-10T22:24:45.512606: step 11, loss 3.78515, acc 0.546875, precision 0.488372, recall 0.75\n",
      "temporal 2020-10-10T22:24:45.746011: step 12, loss 4.83966, acc 0.421875, precision 0.446809, recall 0.65625\n",
      "temporal 2020-10-10T22:24:46.006852: step 13, loss 3.51919, acc 0.53125, precision 0.484848, recall 0.551724\n",
      "temporal 2020-10-10T22:24:46.239334: step 14, loss 4.90556, acc 0.46875, precision 0.666667, recall 0.3\n",
      "temporal 2020-10-10T22:24:46.474147: step 15, loss 4.07142, acc 0.484375, precision 0.473684, recall 0.28125\n",
      "temporal 2020-10-10T22:24:46.738252: step 16, loss 5.46815, acc 0.46875, precision 0.428571, recall 0.517241\n",
      "temporal 2020-10-10T22:24:46.986195: step 17, loss 3.09036, acc 0.5625, precision 0.545455, recall 0.580645\n",
      "temporal 2020-10-10T22:24:47.219404: step 18, loss 4.66202, acc 0.390625, precision 0.394737, recall 0.483871\n",
      "temporal 2020-10-10T22:24:47.480033: step 19, loss 4.97529, acc 0.453125, precision 0.447368, recall 0.548387\n",
      "temporal 2020-10-10T22:24:47.719923: step 20, loss 4.64251, acc 0.46875, precision 0.382353, recall 0.5\n",
      "temporal 2020-10-10T22:24:47.973300: step 21, loss 4.50232, acc 0.46875, precision 0.5, recall 0.441176\n",
      "temporal 2020-10-10T22:24:48.207245: step 22, loss 4.30747, acc 0.453125, precision 0.4, recall 0.1875\n",
      "temporal 2020-10-10T22:24:48.440516: step 23, loss 3.74584, acc 0.578125, precision 0.4, recall 0.347826\n",
      "temporal 2020-10-10T22:24:48.642859: step 24, loss 4.02625, acc 0.541667, precision 0.545455, recall 0.5\n",
      "temporal 2020-10-10T22:24:48.874207: step 25, loss 2.52181, acc 0.671875, precision 0.647059, recall 0.709677\n",
      "temporal 2020-10-10T22:24:49.126756: step 26, loss 3.22505, acc 0.53125, precision 0.542857, recall 0.575758\n",
      "temporal 2020-10-10T22:24:49.364569: step 27, loss 3.24312, acc 0.515625, precision 0.5, recall 0.548387\n",
      "temporal 2020-10-10T22:24:49.611657: step 28, loss 2.7151, acc 0.5, precision 0.487179, recall 0.612903\n",
      "temporal 2020-10-10T22:24:49.872012: step 29, loss 3.38042, acc 0.5, precision 0.483871, recall 0.483871\n",
      "temporal 2020-10-10T22:24:50.112558: step 30, loss 2.83639, acc 0.578125, precision 0.424242, recall 0.636364\n",
      "temporal 2020-10-10T22:24:50.350852: step 31, loss 4.70467, acc 0.5625, precision 0.578947, recall 0.354839\n",
      "temporal 2020-10-10T22:24:50.626968: step 32, loss 3.71271, acc 0.59375, precision 0.588235, recall 0.344828\n",
      "temporal 2020-10-10T22:24:50.882469: step 33, loss 1.61318, acc 0.734375, precision 0.714286, recall 0.434783\n",
      "temporal 2020-10-10T22:24:51.139532: step 34, loss 3.97874, acc 0.515625, precision 0.65, recall 0.351351\n",
      "temporal 2020-10-10T22:24:51.373601: step 35, loss 3.16295, acc 0.609375, precision 0.540541, recall 0.714286\n",
      "temporal 2020-10-10T22:24:51.610655: step 36, loss 4.38766, acc 0.5, precision 0.471698, recall 0.862069\n",
      "temporal 2020-10-10T22:24:51.867728: step 37, loss 3.76954, acc 0.578125, precision 0.586957, recall 0.771429\n",
      "temporal 2020-10-10T22:24:52.128810: step 38, loss 2.04782, acc 0.59375, precision 0.566667, recall 0.566667\n",
      "temporal 2020-10-10T22:24:52.388770: step 39, loss 3.7642, acc 0.578125, precision 0.645161, recall 0.555556\n",
      "temporal 2020-10-10T22:24:52.624698: step 40, loss 3.13165, acc 0.625, precision 0.884615, recall 0.522727\n",
      "temporal 2020-10-10T22:24:52.861160: step 41, loss 2.01297, acc 0.640625, precision 0.823529, recall 0.622222\n",
      "temporal 2020-10-10T22:24:53.104242: step 42, loss 3.46369, acc 0.515625, precision 0.511628, recall 0.6875\n",
      "temporal 2020-10-10T22:24:53.338467: step 43, loss 3.4579, acc 0.453125, precision 0.564103, recall 0.55\n",
      "temporal 2020-10-10T22:24:53.576613: step 44, loss 3.57739, acc 0.59375, precision 0.581395, recall 0.757576\n",
      "temporal 2020-10-10T22:24:53.813800: step 45, loss 3.50681, acc 0.609375, precision 0.510638, recall 0.923077\n",
      "temporal 2020-10-10T22:24:54.072763: step 46, loss 3.29195, acc 0.53125, precision 0.533333, recall 0.5\n",
      "temporal 2020-10-10T22:24:54.307901: step 47, loss 2.50623, acc 0.640625, precision 0.611111, recall 0.407407\n",
      "temporal 2020-10-10T22:24:54.510390: step 48, loss 2.77662, acc 0.645833, precision 0.818182, recall 0.375\n",
      "temporal 2020-10-10T22:24:54.768321: step 49, loss 2.59176, acc 0.609375, precision 0.666667, recall 0.529412\n",
      "temporal 2020-10-10T22:24:55.022018: step 50, loss 2.54534, acc 0.625, precision 0.785714, recall 0.55\n",
      "temporal 2020-10-10T22:24:55.289840: step 51, loss 2.60595, acc 0.671875, precision 0.594595, recall 0.785714\n",
      "temporal 2020-10-10T22:24:55.533428: step 52, loss 2.86007, acc 0.578125, precision 0.534884, recall 0.766667\n",
      "temporal 2020-10-10T22:24:55.778377: step 53, loss 3.5669, acc 0.546875, precision 0.512195, recall 0.7\n",
      "temporal 2020-10-10T22:24:56.042083: step 54, loss 2.27296, acc 0.734375, precision 0.615385, recall 0.923077\n",
      "temporal 2020-10-10T22:24:56.313969: step 55, loss 2.656, acc 0.625, precision 0.653846, recall 0.53125\n",
      "temporal 2020-10-10T22:24:56.577514: step 56, loss 3.27303, acc 0.59375, precision 0.7, recall 0.411765\n",
      "temporal 2020-10-10T22:24:56.822106: step 57, loss 3.85385, acc 0.578125, precision 0.684211, recall 0.382353\n",
      "temporal 2020-10-10T22:24:57.064244: step 58, loss 2.45443, acc 0.65625, precision 0.645161, recall 0.645161\n",
      "temporal 2020-10-10T22:24:57.318250: step 59, loss 1.8934, acc 0.625, precision 0.645161, recall 0.606061\n",
      "temporal 2020-10-10T22:24:57.600702: step 60, loss 2.21016, acc 0.734375, precision 0.681818, recall 0.909091\n",
      "temporal 2020-10-10T22:24:57.866624: step 61, loss 3.39107, acc 0.625, precision 0.534884, recall 0.851852\n",
      "temporal 2020-10-10T22:24:58.111724: step 62, loss 2.26175, acc 0.59375, precision 0.464286, recall 0.541667\n",
      "temporal 2020-10-10T22:24:58.359048: step 63, loss 1.94588, acc 0.671875, precision 0.689655, recall 0.625\n",
      "temporal 2020-10-10T22:24:58.603863: step 64, loss 2.34928, acc 0.671875, precision 0.782609, recall 0.529412\n",
      "temporal 2020-10-10T22:24:58.853441: step 65, loss 3.85733, acc 0.546875, precision 0.705882, recall 0.333333\n",
      "temporal 2020-10-10T22:24:59.099806: step 66, loss 3.15272, acc 0.546875, precision 0.48, recall 0.428571\n",
      "temporal 2020-10-10T22:24:59.365392: step 67, loss 3.33152, acc 0.546875, precision 0.525, recall 0.677419\n",
      "temporal 2020-10-10T22:24:59.642263: step 68, loss 3.49439, acc 0.640625, precision 0.604167, recall 0.878788\n",
      "temporal 2020-10-10T22:24:59.913670: step 69, loss 3.00765, acc 0.609375, precision 0.638889, recall 0.657143\n",
      "temporal 2020-10-10T22:25:00.173774: step 70, loss 1.53849, acc 0.734375, precision 0.78125, recall 0.714286\n",
      "temporal 2020-10-10T22:25:00.424543: step 71, loss 2.0525, acc 0.71875, precision 0.717949, recall 0.8\n",
      "temporal 2020-10-10T22:25:00.636338: step 72, loss 2.24657, acc 0.5, precision 0.521739, recall 0.48\n",
      "temporal 2020-10-10T22:25:00.881006: step 73, loss 2.82114, acc 0.578125, precision 0.75, recall 0.461538\n",
      "temporal 2020-10-10T22:25:01.152007: step 74, loss 3.20559, acc 0.609375, precision 0.652174, recall 0.46875\n",
      "temporal 2020-10-10T22:25:01.397598: step 75, loss 2.93633, acc 0.65625, precision 0.585366, recall 0.827586\n",
      "temporal 2020-10-10T22:25:01.645633: step 76, loss 2.79804, acc 0.578125, precision 0.5, recall 0.62963\n",
      "temporal 2020-10-10T22:25:01.894521: step 77, loss 2.00438, acc 0.59375, precision 0.575758, recall 0.612903\n",
      "temporal 2020-10-10T22:25:02.138627: step 78, loss 2.12328, acc 0.671875, precision 0.703704, recall 0.59375\n",
      "temporal 2020-10-10T22:25:02.384703: step 79, loss 1.96657, acc 0.71875, precision 0.678571, recall 0.678571\n",
      "temporal 2020-10-10T22:25:02.628995: step 80, loss 2.48055, acc 0.640625, precision 0.6875, recall 0.628571\n",
      "temporal 2020-10-10T22:25:02.917451: step 81, loss 2.13309, acc 0.609375, precision 0.818182, recall 0.461538\n",
      "temporal 2020-10-10T22:25:03.164075: step 82, loss 2.73469, acc 0.59375, precision 0.638889, recall 0.638889\n",
      "temporal 2020-10-10T22:25:03.427328: step 83, loss 2.2033, acc 0.59375, precision 0.5, recall 0.769231\n",
      "temporal 2020-10-10T22:25:03.675208: step 84, loss 1.90195, acc 0.6875, precision 0.685714, recall 0.727273\n",
      "temporal 2020-10-10T22:25:03.919978: step 85, loss 1.4647, acc 0.765625, precision 0.717949, recall 0.875\n",
      "temporal 2020-10-10T22:25:04.163157: step 86, loss 2.20288, acc 0.59375, precision 0.583333, recall 0.466667\n",
      "temporal 2020-10-10T22:25:04.402825: step 87, loss 2.58307, acc 0.6875, precision 0.75, recall 0.5625\n",
      "temporal 2020-10-10T22:25:04.639698: step 88, loss 1.77375, acc 0.65625, precision 0.695652, recall 0.516129\n",
      "temporal 2020-10-10T22:25:04.874965: step 89, loss 1.96899, acc 0.703125, precision 0.594595, recall 0.846154\n",
      "temporal 2020-10-10T22:25:05.136313: step 90, loss 3.02813, acc 0.65625, precision 0.6, recall 0.724138\n",
      "temporal 2020-10-10T22:25:05.370160: step 91, loss 3.24955, acc 0.625, precision 0.6875, recall 0.611111\n",
      "temporal 2020-10-10T22:25:05.623191: step 92, loss 2.59064, acc 0.65625, precision 0.733333, recall 0.611111\n",
      "temporal 2020-10-10T22:25:05.856107: step 93, loss 2.75685, acc 0.671875, precision 0.606061, recall 0.714286\n",
      "temporal 2020-10-10T22:25:06.087810: step 94, loss 1.3287, acc 0.78125, precision 0.896552, recall 0.702703\n",
      "temporal 2020-10-10T22:25:06.325164: step 95, loss 2.23018, acc 0.59375, precision 0.633333, recall 0.558824\n",
      "temporal 2020-10-10T22:25:06.548795: step 96, loss 1.83012, acc 0.6875, precision 0.612903, recall 0.863636\n",
      "temporal 2020-10-10T22:25:06.779366: step 97, loss 1.82829, acc 0.6875, precision 0.611111, recall 0.785714\n",
      "temporal 2020-10-10T22:25:07.011383: step 98, loss 1.41005, acc 0.71875, precision 0.714286, recall 0.666667\n",
      "temporal 2020-10-10T22:25:07.249799: step 99, loss 1.54788, acc 0.6875, precision 0.708333, recall 0.566667\n",
      "temporal 2020-10-10T22:25:07.485017: step 100, loss 1.63384, acc 0.75, precision 0.615385, recall 0.421053\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:25:08.739650: step 100, loss 0.433042, acc 0.848948, precision 0.198347, recall 0.282353, F1 0.23301\n",
      "Saved model checkpoint to /data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/runs/explicit_1602339881/checkpoints/explicit-model-100\n",
      "\n",
      "\n",
      "temporal 2020-10-10T22:25:09.252953: step 101, loss 1.5651, acc 0.765625, precision 0.809524, recall 0.607143\n",
      "temporal 2020-10-10T22:25:09.470398: step 102, loss 2.34464, acc 0.640625, precision 0.633333, recall 0.612903\n",
      "temporal 2020-10-10T22:25:09.701288: step 103, loss 0.883907, acc 0.8125, precision 0.783784, recall 0.878788\n",
      "temporal 2020-10-10T22:25:09.933197: step 104, loss 1.89509, acc 0.71875, precision 0.705882, recall 0.75\n",
      "temporal 2020-10-10T22:25:10.166681: step 105, loss 2.06246, acc 0.671875, precision 0.710526, recall 0.72973\n",
      "temporal 2020-10-10T22:25:10.404341: step 106, loss 2.22922, acc 0.609375, precision 0.619048, recall 0.742857\n",
      "temporal 2020-10-10T22:25:10.638472: step 107, loss 1.20335, acc 0.765625, precision 0.8, recall 0.777778\n",
      "temporal 2020-10-10T22:25:10.883905: step 108, loss 1.15092, acc 0.796875, precision 0.8, recall 0.864865\n",
      "temporal 2020-10-10T22:25:11.149057: step 109, loss 2.26984, acc 0.65625, precision 0.666667, recall 0.580645\n",
      "temporal 2020-10-10T22:25:11.391755: step 110, loss 2.43174, acc 0.546875, precision 0.652174, recall 0.416667\n",
      "temporal 2020-10-10T22:25:11.629951: step 111, loss 2.04928, acc 0.671875, precision 0.818182, recall 0.514286\n",
      "temporal 2020-10-10T22:25:11.863253: step 112, loss 2.17748, acc 0.578125, precision 0.533333, recall 0.551724\n",
      "temporal 2020-10-10T22:25:12.095112: step 113, loss 2.00094, acc 0.734375, precision 0.711111, recall 0.888889\n",
      "temporal 2020-10-10T22:25:12.332385: step 114, loss 2.65939, acc 0.65625, precision 0.6, recall 0.8\n",
      "temporal 2020-10-10T22:25:12.565392: step 115, loss 2.00901, acc 0.71875, precision 0.630435, recall 0.966667\n",
      "temporal 2020-10-10T22:25:12.802656: step 116, loss 1.65979, acc 0.71875, precision 0.65625, recall 0.75\n",
      "temporal 2020-10-10T22:25:13.036849: step 117, loss 2.58102, acc 0.640625, precision 0.681818, recall 0.483871\n",
      "temporal 2020-10-10T22:25:13.290404: step 118, loss 1.68707, acc 0.734375, precision 0.875, recall 0.6\n",
      "temporal 2020-10-10T22:25:13.533378: step 119, loss 3.19192, acc 0.609375, precision 0.777778, recall 0.4\n",
      "temporal 2020-10-10T22:25:13.740763: step 120, loss 1.44831, acc 0.729167, precision 0.777778, recall 0.75\n",
      "temporal 2020-10-10T22:25:13.972908: step 121, loss 3.38241, acc 0.671875, precision 0.526316, recall 0.869565\n",
      "temporal 2020-10-10T22:25:14.209704: step 122, loss 1.58083, acc 0.59375, precision 0.525, recall 0.75\n",
      "temporal 2020-10-10T22:25:14.459188: step 123, loss 1.97667, acc 0.703125, precision 0.638889, recall 0.793103\n",
      "temporal 2020-10-10T22:25:14.695691: step 124, loss 1.6516, acc 0.671875, precision 0.869565, recall 0.526316\n",
      "temporal 2020-10-10T22:25:14.928615: step 125, loss 1.72259, acc 0.734375, precision 0.9, recall 0.545455\n",
      "temporal 2020-10-10T22:25:15.163530: step 126, loss 3.55184, acc 0.5625, precision 0.6, recall 0.454545\n",
      "temporal 2020-10-10T22:25:15.397622: step 127, loss 1.79217, acc 0.75, precision 0.775, recall 0.815789\n",
      "temporal 2020-10-10T22:25:15.617492: step 128, loss 1.55036, acc 0.765625, precision 0.75, recall 0.891892\n",
      "temporal 2020-10-10T22:25:15.843481: step 129, loss 3.10151, acc 0.59375, precision 0.521739, recall 0.857143\n",
      "temporal 2020-10-10T22:25:16.077374: step 130, loss 1.892, acc 0.734375, precision 0.695652, recall 0.914286\n",
      "temporal 2020-10-10T22:25:16.293803: step 131, loss 1.81642, acc 0.671875, precision 0.676471, recall 0.69697\n",
      "temporal 2020-10-10T22:25:16.534585: step 132, loss 1.97464, acc 0.6875, precision 0.769231, recall 0.588235\n",
      "temporal 2020-10-10T22:25:16.775947: step 133, loss 2.39508, acc 0.671875, precision 0.75, recall 0.413793\n",
      "temporal 2020-10-10T22:25:17.039408: step 134, loss 2.22124, acc 0.65625, precision 0.666667, recall 0.37037\n",
      "temporal 2020-10-10T22:25:17.278892: step 135, loss 1.51476, acc 0.703125, precision 0.875, recall 0.651163\n",
      "temporal 2020-10-10T22:25:17.532282: step 136, loss 1.34003, acc 0.703125, precision 0.658537, recall 0.84375\n",
      "temporal 2020-10-10T22:25:17.774705: step 137, loss 1.98839, acc 0.734375, precision 0.666667, recall 0.969697\n",
      "temporal 2020-10-10T22:25:17.997241: step 138, loss 2.25033, acc 0.703125, precision 0.638298, recall 0.9375\n",
      "temporal 2020-10-10T22:25:18.229954: step 139, loss 2.37549, acc 0.640625, precision 0.526316, recall 0.8\n",
      "temporal 2020-10-10T22:25:18.462898: step 140, loss 1.97785, acc 0.6875, precision 0.6875, recall 0.6875\n",
      "temporal 2020-10-10T22:25:18.694741: step 141, loss 2.69834, acc 0.65625, precision 0.8, recall 0.387097\n",
      "temporal 2020-10-10T22:25:18.928042: step 142, loss 2.41001, acc 0.6875, precision 1, recall 0.393939\n",
      "temporal 2020-10-10T22:25:19.162005: step 143, loss 1.75866, acc 0.671875, precision 0.73913, recall 0.53125\n",
      "temporal 2020-10-10T22:25:19.348416: step 144, loss 1.59834, acc 0.6875, precision 0.652174, recall 0.681818\n",
      "temporal 2020-10-10T22:25:19.586095: step 145, loss 1.76792, acc 0.71875, precision 0.641026, recall 0.862069\n",
      "temporal 2020-10-10T22:25:19.843824: step 146, loss 1.17335, acc 0.734375, precision 0.716981, recall 0.95\n",
      "temporal 2020-10-10T22:25:20.075759: step 147, loss 1.99525, acc 0.796875, precision 0.75, recall 0.942857\n",
      "temporal 2020-10-10T22:25:20.312367: step 148, loss 1.542, acc 0.765625, precision 0.694444, recall 0.862069\n",
      "temporal 2020-10-10T22:25:20.549648: step 149, loss 1.35084, acc 0.6875, precision 0.705882, recall 0.705882\n",
      "temporal 2020-10-10T22:25:20.791448: step 150, loss 1.77774, acc 0.71875, precision 0.8, recall 0.606061\n",
      "temporal 2020-10-10T22:25:21.026670: step 151, loss 1.79936, acc 0.71875, precision 0.818182, recall 0.5625\n",
      "temporal 2020-10-10T22:25:21.263475: step 152, loss 1.72168, acc 0.703125, precision 0.7, recall 0.677419\n",
      "temporal 2020-10-10T22:25:21.501320: step 153, loss 1.20148, acc 0.8125, precision 0.806452, recall 0.806452\n",
      "temporal 2020-10-10T22:25:21.735921: step 154, loss 1.46076, acc 0.734375, precision 0.757576, recall 0.735294\n",
      "temporal 2020-10-10T22:25:21.969995: step 155, loss 1.09588, acc 0.796875, precision 0.75, recall 0.870968\n",
      "temporal 2020-10-10T22:25:22.211046: step 156, loss 1.16816, acc 0.734375, precision 0.72, recall 0.642857\n",
      "temporal 2020-10-10T22:25:22.449875: step 157, loss 2.00606, acc 0.65625, precision 0.512821, recall 0.869565\n",
      "temporal 2020-10-10T22:25:22.690414: step 158, loss 1.47368, acc 0.75, precision 0.756757, recall 0.8\n",
      "temporal 2020-10-10T22:25:22.935861: step 159, loss 1.29784, acc 0.78125, precision 0.904762, recall 0.612903\n",
      "temporal 2020-10-10T22:25:23.175014: step 160, loss 1.17451, acc 0.78125, precision 0.857143, recall 0.705882\n",
      "temporal 2020-10-10T22:25:23.420299: step 161, loss 0.766393, acc 0.875, precision 0.913043, recall 0.777778\n",
      "temporal 2020-10-10T22:25:23.659302: step 162, loss 1.29772, acc 0.703125, precision 0.75, recall 0.685714\n",
      "temporal 2020-10-10T22:25:23.896397: step 163, loss 1.83253, acc 0.640625, precision 0.740741, recall 0.555556\n",
      "temporal 2020-10-10T22:25:24.134895: step 164, loss 1.83713, acc 0.6875, precision 0.611111, recall 0.785714\n",
      "temporal 2020-10-10T22:25:24.370475: step 165, loss 1.87018, acc 0.65625, precision 0.577778, recall 0.896552\n",
      "temporal 2020-10-10T22:25:24.603834: step 166, loss 1.34712, acc 0.78125, precision 0.72973, recall 0.870968\n",
      "temporal 2020-10-10T22:25:24.840541: step 167, loss 0.862265, acc 0.796875, precision 0.794872, recall 0.861111\n",
      "temporal 2020-10-10T22:25:25.044036: step 168, loss 2.0757, acc 0.8125, precision 0.88, recall 0.785714\n",
      "temporal 2020-10-10T22:25:25.281371: step 169, loss 1.46345, acc 0.71875, precision 0.785714, recall 0.647059\n",
      "temporal 2020-10-10T22:25:25.516308: step 170, loss 1.34453, acc 0.78125, precision 0.823529, recall 0.56\n",
      "temporal 2020-10-10T22:25:25.748949: step 171, loss 1.56916, acc 0.71875, precision 0.75, recall 0.705882\n",
      "temporal 2020-10-10T22:25:25.973537: step 172, loss 1.38872, acc 0.765625, precision 0.787879, recall 0.764706\n",
      "temporal 2020-10-10T22:25:26.212517: step 173, loss 1.26472, acc 0.78125, precision 0.6875, recall 0.846154\n",
      "temporal 2020-10-10T22:25:26.457249: step 174, loss 1.63513, acc 0.796875, precision 0.8, recall 0.864865\n",
      "temporal 2020-10-10T22:25:26.727342: step 175, loss 1.58623, acc 0.734375, precision 0.625, recall 0.8\n",
      "temporal 2020-10-10T22:25:26.976591: step 176, loss 1.2426, acc 0.71875, precision 0.708333, recall 0.607143\n",
      "temporal 2020-10-10T22:25:27.224521: step 177, loss 0.896625, acc 0.765625, precision 0.8, recall 0.727273\n",
      "temporal 2020-10-10T22:25:27.474375: step 178, loss 0.825113, acc 0.828125, precision 0.837838, recall 0.861111\n",
      "temporal 2020-10-10T22:25:27.720354: step 179, loss 0.851888, acc 0.84375, precision 0.916667, recall 0.733333\n",
      "temporal 2020-10-10T22:25:27.968042: step 180, loss 2.28493, acc 0.671875, precision 0.75, recall 0.648649\n",
      "temporal 2020-10-10T22:25:28.212710: step 181, loss 0.903787, acc 0.765625, precision 0.72973, recall 0.84375\n",
      "temporal 2020-10-10T22:25:28.457495: step 182, loss 1.53449, acc 0.765625, precision 0.75, recall 0.774194\n",
      "temporal 2020-10-10T22:25:28.725608: step 183, loss 1.91673, acc 0.640625, precision 0.53125, recall 0.68\n",
      "temporal 2020-10-10T22:25:28.968575: step 184, loss 0.887953, acc 0.828125, precision 0.882353, recall 0.810811\n",
      "temporal 2020-10-10T22:25:29.212623: step 185, loss 1.2248, acc 0.765625, precision 0.733333, recall 0.758621\n",
      "temporal 2020-10-10T22:25:29.460987: step 186, loss 0.84395, acc 0.828125, precision 0.868421, recall 0.846154\n",
      "temporal 2020-10-10T22:25:29.730884: step 187, loss 1.55598, acc 0.71875, precision 0.612903, recall 0.76\n",
      "temporal 2020-10-10T22:25:29.976277: step 188, loss 0.238073, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:25:30.222888: step 189, loss 1.25338, acc 0.765625, precision 0.806452, recall 0.735294\n",
      "temporal 2020-10-10T22:25:30.488775: step 190, loss 1.51094, acc 0.765625, precision 0.862069, recall 0.694444\n",
      "temporal 2020-10-10T22:25:30.739469: step 191, loss 0.926565, acc 0.84375, precision 0.806452, recall 0.862069\n",
      "temporal 2020-10-10T22:25:30.967095: step 192, loss 0.579021, acc 0.833333, precision 0.96, recall 0.774194\n",
      "temporal 2020-10-10T22:25:31.243104: step 193, loss 0.997807, acc 0.8125, precision 0.888889, recall 0.8\n",
      "temporal 2020-10-10T22:25:31.495886: step 194, loss 1.13453, acc 0.796875, precision 0.735294, recall 0.862069\n",
      "temporal 2020-10-10T22:25:31.768095: step 195, loss 1.11862, acc 0.8125, precision 0.756757, recall 0.903226\n",
      "temporal 2020-10-10T22:25:32.016525: step 196, loss 1.08634, acc 0.75, precision 0.617647, recall 0.875\n",
      "temporal 2020-10-10T22:25:32.263334: step 197, loss 0.649604, acc 0.796875, precision 0.878788, recall 0.763158\n",
      "temporal 2020-10-10T22:25:32.508193: step 198, loss 0.939154, acc 0.8125, precision 0.785714, recall 0.785714\n",
      "temporal 2020-10-10T22:25:32.777359: step 199, loss 1.6791, acc 0.703125, precision 0.857143, recall 0.529412\n",
      "temporal 2020-10-10T22:25:33.022511: step 200, loss 2.43831, acc 0.59375, precision 0.75, recall 0.416667\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:25:34.141260: step 200, loss 1.85762, acc 0.501912, precision 0.121528, recall 0.823529, F1 0.2118\n",
      "\n",
      "temporal 2020-10-10T22:25:34.353269: step 201, loss 1.424, acc 0.78125, precision 0.736842, recall 0.875\n",
      "temporal 2020-10-10T22:25:34.576336: step 202, loss 1.13056, acc 0.765625, precision 0.72973, recall 0.84375\n",
      "temporal 2020-10-10T22:25:34.800132: step 203, loss 1.50239, acc 0.78125, precision 0.75, recall 0.947368\n",
      "temporal 2020-10-10T22:25:35.044137: step 204, loss 0.93405, acc 0.859375, precision 0.86, recall 0.955556\n",
      "temporal 2020-10-10T22:25:35.311584: step 205, loss 1.72847, acc 0.734375, precision 0.6, recall 0.96\n",
      "temporal 2020-10-10T22:25:35.558163: step 206, loss 0.988203, acc 0.828125, precision 0.833333, recall 0.806452\n",
      "temporal 2020-10-10T22:25:35.800304: step 207, loss 0.961399, acc 0.84375, precision 0.846154, recall 0.785714\n",
      "temporal 2020-10-10T22:25:36.047563: step 208, loss 1.84601, acc 0.78125, precision 0.909091, recall 0.625\n",
      "temporal 2020-10-10T22:25:36.315800: step 209, loss 1.72401, acc 0.765625, precision 0.923077, recall 0.648649\n",
      "temporal 2020-10-10T22:25:36.584704: step 210, loss 1.96248, acc 0.625, precision 0.761905, recall 0.457143\n",
      "temporal 2020-10-10T22:25:36.831498: step 211, loss 0.646419, acc 0.828125, precision 0.64, recall 0.888889\n",
      "temporal 2020-10-10T22:25:37.079887: step 212, loss 1.24763, acc 0.75, precision 0.628571, recall 0.88\n",
      "temporal 2020-10-10T22:25:37.351009: step 213, loss 1.52537, acc 0.796875, precision 0.761905, recall 0.914286\n",
      "temporal 2020-10-10T22:25:37.598882: step 214, loss 1.68317, acc 0.734375, precision 0.717949, recall 0.823529\n",
      "temporal 2020-10-10T22:25:37.843913: step 215, loss 1.21345, acc 0.765625, precision 0.758621, recall 0.733333\n",
      "temporal 2020-10-10T22:25:38.055012: step 216, loss 1.60077, acc 0.729167, precision 0.75, recall 0.652174\n",
      "temporal 2020-10-10T22:25:38.326547: step 217, loss 1.26563, acc 0.75, precision 0.904762, recall 0.575758\n",
      "temporal 2020-10-10T22:25:38.605488: step 218, loss 1.21473, acc 0.734375, precision 0.741935, recall 0.71875\n",
      "temporal 2020-10-10T22:25:38.849778: step 219, loss 1.41576, acc 0.71875, precision 0.682927, recall 0.848485\n",
      "temporal 2020-10-10T22:25:39.097452: step 220, loss 0.553435, acc 0.875, precision 0.916667, recall 0.868421\n",
      "temporal 2020-10-10T22:25:39.340510: step 221, loss 1.01261, acc 0.796875, precision 0.828571, recall 0.805556\n",
      "temporal 2020-10-10T22:25:39.609041: step 222, loss 1.27183, acc 0.796875, precision 0.733333, recall 0.970588\n",
      "temporal 2020-10-10T22:25:39.879311: step 223, loss 0.8983, acc 0.765625, precision 0.731707, recall 0.882353\n",
      "temporal 2020-10-10T22:25:40.128025: step 224, loss 0.981604, acc 0.796875, precision 0.703704, recall 0.791667\n",
      "temporal 2020-10-10T22:25:40.373543: step 225, loss 0.523061, acc 0.921875, precision 1, recall 0.84375\n",
      "temporal 2020-10-10T22:25:40.616907: step 226, loss 1.31499, acc 0.8125, precision 0.896552, recall 0.742857\n",
      "temporal 2020-10-10T22:25:40.864869: step 227, loss 2.10508, acc 0.734375, precision 0.8, recall 0.551724\n",
      "temporal 2020-10-10T22:25:41.135485: step 228, loss 1.19557, acc 0.84375, precision 0.863636, recall 0.730769\n",
      "temporal 2020-10-10T22:25:41.380673: step 229, loss 0.6759, acc 0.8125, precision 0.764706, recall 0.866667\n",
      "temporal 2020-10-10T22:25:41.627315: step 230, loss 1.01047, acc 0.796875, precision 0.735294, recall 0.862069\n",
      "temporal 2020-10-10T22:25:41.893009: step 231, loss 0.866814, acc 0.8125, precision 0.785714, recall 0.916667\n",
      "temporal 2020-10-10T22:25:42.160901: step 232, loss 1.27769, acc 0.828125, precision 0.827586, recall 0.8\n",
      "temporal 2020-10-10T22:25:42.426024: step 233, loss 1.37083, acc 0.765625, precision 0.794118, recall 0.771429\n",
      "temporal 2020-10-10T22:25:42.671932: step 234, loss 1.05053, acc 0.78125, precision 0.742857, recall 0.83871\n",
      "temporal 2020-10-10T22:25:42.939958: step 235, loss 0.618579, acc 0.875, precision 0.827586, recall 0.888889\n",
      "temporal 2020-10-10T22:25:43.209678: step 236, loss 1.1546, acc 0.796875, precision 0.8125, recall 0.787879\n",
      "temporal 2020-10-10T22:25:43.463668: step 237, loss 1.04022, acc 0.8125, precision 0.84375, recall 0.794118\n",
      "temporal 2020-10-10T22:25:43.708038: step 238, loss 1.04679, acc 0.84375, precision 0.857143, recall 0.857143\n",
      "temporal 2020-10-10T22:25:43.953286: step 239, loss 1.43483, acc 0.765625, precision 0.793103, recall 0.71875\n",
      "temporal 2020-10-10T22:25:44.166720: step 240, loss 0.751443, acc 0.854167, precision 0.894737, recall 0.772727\n",
      "temporal 2020-10-10T22:25:44.413320: step 241, loss 0.614839, acc 0.859375, precision 0.878788, recall 0.852941\n",
      "temporal 2020-10-10T22:25:44.684636: step 242, loss 0.655826, acc 0.84375, precision 0.888889, recall 0.842105\n",
      "temporal 2020-10-10T22:25:44.931908: step 243, loss 0.902006, acc 0.765625, precision 0.75, recall 0.774194\n",
      "temporal 2020-10-10T22:25:45.201653: step 244, loss 0.695247, acc 0.859375, precision 0.842105, recall 0.914286\n",
      "temporal 2020-10-10T22:25:45.463686: step 245, loss 0.535758, acc 0.828125, precision 0.766667, recall 0.851852\n",
      "temporal 2020-10-10T22:25:45.710561: step 246, loss 0.882651, acc 0.78125, precision 0.658537, recall 1\n",
      "temporal 2020-10-10T22:25:45.956838: step 247, loss 0.409567, acc 0.890625, precision 0.911765, recall 0.885714\n",
      "temporal 2020-10-10T22:25:46.213770: step 248, loss 1.12816, acc 0.828125, precision 0.8, recall 0.769231\n",
      "temporal 2020-10-10T22:25:46.460475: step 249, loss 0.439638, acc 0.890625, precision 1, recall 0.815789\n",
      "temporal 2020-10-10T22:25:46.704401: step 250, loss 1.15391, acc 0.75, precision 0.904762, recall 0.575758\n",
      "temporal 2020-10-10T22:25:46.951690: step 251, loss 0.918008, acc 0.828125, precision 0.903226, recall 0.777778\n",
      "temporal 2020-10-10T22:25:47.202221: step 252, loss 0.479903, acc 0.875, precision 0.884615, recall 0.821429\n",
      "temporal 2020-10-10T22:25:47.445906: step 253, loss 0.778602, acc 0.84375, precision 0.861111, recall 0.861111\n",
      "temporal 2020-10-10T22:25:47.720999: step 254, loss 2.28874, acc 0.734375, precision 0.609756, recall 0.961538\n",
      "temporal 2020-10-10T22:25:47.966031: step 255, loss 1.25039, acc 0.75, precision 0.692308, recall 0.870968\n",
      "temporal 2020-10-10T22:25:48.212219: step 256, loss 0.656234, acc 0.84375, precision 0.794118, recall 0.9\n",
      "temporal 2020-10-10T22:25:48.480995: step 257, loss 0.725079, acc 0.875, precision 0.785714, recall 0.916667\n",
      "temporal 2020-10-10T22:25:48.750862: step 258, loss 1.11111, acc 0.765625, precision 0.904762, recall 0.59375\n",
      "temporal 2020-10-10T22:25:49.019487: step 259, loss 1.64447, acc 0.6875, precision 0.9375, recall 0.441176\n",
      "temporal 2020-10-10T22:25:49.290289: step 260, loss 1.67301, acc 0.796875, precision 0.894737, recall 0.607143\n",
      "temporal 2020-10-10T22:25:49.555446: step 261, loss 0.931405, acc 0.75, precision 0.804878, recall 0.804878\n",
      "temporal 2020-10-10T22:25:49.819077: step 262, loss 1.15321, acc 0.8125, precision 0.75, recall 0.9375\n",
      "temporal 2020-10-10T22:25:50.063864: step 263, loss 1.10614, acc 0.75, precision 0.723404, recall 0.918919\n",
      "temporal 2020-10-10T22:25:50.303204: step 264, loss 1.42911, acc 0.791667, precision 0.689655, recall 0.952381\n",
      "temporal 2020-10-10T22:25:50.551912: step 265, loss 0.374254, acc 0.875, precision 0.8125, recall 0.928571\n",
      "temporal 2020-10-10T22:25:50.800591: step 266, loss 0.664542, acc 0.890625, precision 0.925926, recall 0.833333\n",
      "temporal 2020-10-10T22:25:51.067494: step 267, loss 0.405746, acc 0.859375, precision 0.958333, recall 0.741935\n",
      "temporal 2020-10-10T22:25:51.315828: step 268, loss 0.76717, acc 0.8125, precision 0.958333, recall 0.676471\n",
      "temporal 2020-10-10T22:25:51.585006: step 269, loss 1.72236, acc 0.671875, precision 0.7, recall 0.482759\n",
      "temporal 2020-10-10T22:25:51.831415: step 270, loss 0.622076, acc 0.890625, precision 0.903226, recall 0.875\n",
      "temporal 2020-10-10T22:25:52.078680: step 271, loss 1.03445, acc 0.828125, precision 0.75, recall 0.967742\n",
      "temporal 2020-10-10T22:25:52.323151: step 272, loss 1.28465, acc 0.8125, precision 0.809524, recall 0.894737\n",
      "temporal 2020-10-10T22:25:52.568373: step 273, loss 0.50946, acc 0.875, precision 0.857143, recall 0.947368\n",
      "temporal 2020-10-10T22:25:52.812318: step 274, loss 1.09536, acc 0.84375, precision 0.857143, recall 0.9\n",
      "temporal 2020-10-10T22:25:53.079417: step 275, loss 0.850468, acc 0.796875, precision 0.758621, recall 0.785714\n",
      "temporal 2020-10-10T22:25:53.323866: step 276, loss 0.9185, acc 0.796875, precision 0.742857, recall 0.866667\n",
      "temporal 2020-10-10T22:25:53.568720: step 277, loss 1.02485, acc 0.84375, precision 0.954545, recall 0.7\n",
      "temporal 2020-10-10T22:25:53.839288: step 278, loss 1.11131, acc 0.796875, precision 0.857143, recall 0.727273\n",
      "temporal 2020-10-10T22:25:54.107240: step 279, loss 0.863742, acc 0.8125, precision 0.83871, recall 0.787879\n",
      "temporal 2020-10-10T22:25:54.354241: step 280, loss 0.815652, acc 0.84375, precision 0.774194, recall 0.888889\n",
      "temporal 2020-10-10T22:25:54.596588: step 281, loss 0.700287, acc 0.859375, precision 0.818182, recall 0.782609\n",
      "temporal 2020-10-10T22:25:54.841304: step 282, loss 0.98094, acc 0.875, precision 0.875, recall 0.875\n",
      "temporal 2020-10-10T22:25:55.082810: step 283, loss 0.48767, acc 0.875, precision 0.916667, recall 0.868421\n",
      "temporal 2020-10-10T22:25:55.325961: step 284, loss 1.16885, acc 0.84375, precision 0.805556, recall 0.90625\n",
      "temporal 2020-10-10T22:25:55.594219: step 285, loss 0.576087, acc 0.90625, precision 0.882353, recall 0.9375\n",
      "temporal 2020-10-10T22:25:55.840822: step 286, loss 1.82518, acc 0.78125, precision 0.727273, recall 0.827586\n",
      "temporal 2020-10-10T22:25:56.101116: step 287, loss 0.827739, acc 0.765625, precision 0.806452, recall 0.735294\n",
      "temporal 2020-10-10T22:25:56.314395: step 288, loss 1.61331, acc 0.75, precision 0.833333, recall 0.714286\n",
      "temporal 2020-10-10T22:25:56.574119: step 289, loss 0.87827, acc 0.796875, precision 0.863636, recall 0.655172\n",
      "temporal 2020-10-10T22:25:56.821663: step 290, loss 1.29487, acc 0.765625, precision 0.742857, recall 0.8125\n",
      "temporal 2020-10-10T22:25:57.066183: step 291, loss 0.539514, acc 0.921875, precision 0.888889, recall 0.969697\n",
      "temporal 2020-10-10T22:25:57.310323: step 292, loss 0.458013, acc 0.890625, precision 0.853659, recall 0.972222\n",
      "temporal 2020-10-10T22:25:57.560396: step 293, loss 0.61863, acc 0.890625, precision 0.870968, recall 0.9\n",
      "temporal 2020-10-10T22:25:57.813651: step 294, loss 0.74637, acc 0.84375, precision 0.8, recall 0.857143\n",
      "temporal 2020-10-10T22:25:58.059244: step 295, loss 0.829027, acc 0.859375, precision 0.903226, recall 0.823529\n",
      "temporal 2020-10-10T22:25:58.303934: step 296, loss 1.04929, acc 0.84375, precision 0.787879, recall 0.896552\n",
      "temporal 2020-10-10T22:25:58.570080: step 297, loss 0.791726, acc 0.8125, precision 0.933333, recall 0.736842\n",
      "temporal 2020-10-10T22:25:58.817139: step 298, loss 1.20449, acc 0.75, precision 0.857143, recall 0.666667\n",
      "temporal 2020-10-10T22:25:59.061762: step 299, loss 0.669897, acc 0.859375, precision 0.794872, recall 0.96875\n",
      "temporal 2020-10-10T22:25:59.309248: step 300, loss 0.269988, acc 0.9375, precision 0.941176, recall 0.941176\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:26:00.433967: step 300, loss 2.20988, acc 0.493308, precision 0.120954, recall 0.835294, F1 0.21131\n",
      "\n",
      "temporal 2020-10-10T22:26:00.683395: step 301, loss 0.44441, acc 0.90625, precision 0.878788, recall 0.935484\n",
      "temporal 2020-10-10T22:26:00.927289: step 302, loss 0.754228, acc 0.796875, precision 0.758621, recall 0.785714\n",
      "temporal 2020-10-10T22:26:01.195621: step 303, loss 0.831205, acc 0.828125, precision 0.882353, recall 0.810811\n",
      "temporal 2020-10-10T22:26:01.466122: step 304, loss 0.96181, acc 0.84375, precision 0.823529, recall 0.875\n",
      "temporal 2020-10-10T22:26:01.735324: step 305, loss 0.7476, acc 0.859375, precision 0.861111, recall 0.885714\n",
      "temporal 2020-10-10T22:26:01.983555: step 306, loss 0.250275, acc 0.90625, precision 0.878049, recall 0.972973\n",
      "temporal 2020-10-10T22:26:02.254679: step 307, loss 0.881972, acc 0.8125, precision 0.851852, recall 0.741935\n",
      "temporal 2020-10-10T22:26:02.500535: step 308, loss 0.729289, acc 0.828125, precision 0.866667, recall 0.787879\n",
      "temporal 2020-10-10T22:26:02.751425: step 309, loss 0.456648, acc 0.921875, precision 0.967742, recall 0.882353\n",
      "temporal 2020-10-10T22:26:02.995591: step 310, loss 0.74013, acc 0.875, precision 0.857143, recall 0.909091\n",
      "temporal 2020-10-10T22:26:03.253756: step 311, loss 0.642854, acc 0.84375, precision 0.724138, recall 0.913043\n",
      "temporal 2020-10-10T22:26:03.468319: step 312, loss 0.712062, acc 0.916667, precision 0.789474, recall 1\n",
      "temporal 2020-10-10T22:26:03.714657: step 313, loss 0.709217, acc 0.875, precision 0.848485, recall 0.903226\n",
      "temporal 2020-10-10T22:26:03.979191: step 314, loss 0.752378, acc 0.828125, precision 0.828571, recall 0.852941\n",
      "temporal 2020-10-10T22:26:04.225589: step 315, loss 1.09656, acc 0.8125, precision 0.925926, recall 0.714286\n",
      "temporal 2020-10-10T22:26:04.500230: step 316, loss 1.28172, acc 0.8125, precision 1, recall 0.6\n",
      "temporal 2020-10-10T22:26:04.749205: step 317, loss 0.709575, acc 0.828125, precision 0.851852, recall 0.766667\n",
      "temporal 2020-10-10T22:26:05.013793: step 318, loss 0.496239, acc 0.859375, precision 0.766667, recall 0.92\n",
      "temporal 2020-10-10T22:26:05.283898: step 319, loss 0.77532, acc 0.8125, precision 0.72973, recall 0.931035\n",
      "temporal 2020-10-10T22:26:05.557019: step 320, loss 0.234729, acc 0.890625, precision 0.884615, recall 0.851852\n",
      "temporal 2020-10-10T22:26:05.802737: step 321, loss 0.572727, acc 0.890625, precision 0.8125, recall 0.962963\n",
      "temporal 2020-10-10T22:26:06.071131: step 322, loss 0.688908, acc 0.84375, precision 0.777778, recall 0.933333\n",
      "temporal 2020-10-10T22:26:06.360951: step 323, loss 0.537603, acc 0.828125, precision 0.857143, recall 0.833333\n",
      "temporal 2020-10-10T22:26:06.631488: step 324, loss 0.627693, acc 0.90625, precision 0.944444, recall 0.894737\n",
      "temporal 2020-10-10T22:26:06.873349: step 325, loss 1.62612, acc 0.75, precision 0.833333, recall 0.625\n",
      "temporal 2020-10-10T22:26:07.133436: step 326, loss 1.11175, acc 0.84375, precision 0.807692, recall 0.807692\n",
      "temporal 2020-10-10T22:26:07.406373: step 327, loss 0.519642, acc 0.859375, precision 0.878788, recall 0.852941\n",
      "temporal 2020-10-10T22:26:07.654954: step 328, loss 0.397262, acc 0.9375, precision 0.942857, recall 0.942857\n",
      "temporal 2020-10-10T22:26:07.902160: step 329, loss 0.626198, acc 0.84375, precision 0.857143, recall 0.8\n",
      "temporal 2020-10-10T22:26:08.149215: step 330, loss 0.44539, acc 0.890625, precision 0.861111, recall 0.939394\n",
      "temporal 2020-10-10T22:26:08.402915: step 331, loss 0.503406, acc 0.875, precision 0.852941, recall 0.90625\n",
      "temporal 2020-10-10T22:26:08.669315: step 332, loss 0.722183, acc 0.828125, precision 0.826087, recall 0.926829\n",
      "temporal 2020-10-10T22:26:08.942039: step 333, loss 0.669115, acc 0.875, precision 0.78125, recall 0.961538\n",
      "temporal 2020-10-10T22:26:09.189032: step 334, loss 0.950704, acc 0.84375, precision 0.875, recall 0.823529\n",
      "temporal 2020-10-10T22:26:09.481985: step 335, loss 0.379095, acc 0.890625, precision 0.941176, recall 0.864865\n",
      "temporal 2020-10-10T22:26:09.695820: step 336, loss 0.591963, acc 0.854167, precision 0.956522, recall 0.785714\n",
      "temporal 2020-10-10T22:26:09.940255: step 337, loss 0.595441, acc 0.890625, precision 0.933333, recall 0.848485\n",
      "temporal 2020-10-10T22:26:10.186820: step 338, loss 0.800557, acc 0.84375, precision 0.8, recall 0.903226\n",
      "temporal 2020-10-10T22:26:10.436099: step 339, loss 0.507511, acc 0.90625, precision 0.875, recall 0.972222\n",
      "temporal 2020-10-10T22:26:10.704378: step 340, loss 0.900033, acc 0.765625, precision 0.764706, recall 0.787879\n",
      "temporal 2020-10-10T22:26:10.949736: step 341, loss 0.392495, acc 0.90625, precision 0.878788, recall 0.935484\n",
      "temporal 2020-10-10T22:26:11.192976: step 342, loss 0.706308, acc 0.828125, precision 0.846154, recall 0.868421\n",
      "temporal 2020-10-10T22:26:11.438179: step 343, loss 0.654553, acc 0.890625, precision 0.9, recall 0.870968\n",
      "temporal 2020-10-10T22:26:11.708530: step 344, loss 0.358886, acc 0.890625, precision 0.941176, recall 0.864865\n",
      "temporal 2020-10-10T22:26:11.979342: step 345, loss 0.828629, acc 0.78125, precision 0.8125, recall 0.764706\n",
      "temporal 2020-10-10T22:26:12.248241: step 346, loss 0.665987, acc 0.828125, precision 0.925926, recall 0.735294\n",
      "temporal 2020-10-10T22:26:12.495646: step 347, loss 0.4667, acc 0.859375, precision 0.852941, recall 0.878788\n",
      "temporal 2020-10-10T22:26:12.744121: step 348, loss 0.634007, acc 0.890625, precision 0.857143, recall 0.9375\n",
      "temporal 2020-10-10T22:26:13.010787: step 349, loss 0.562956, acc 0.875, precision 0.888889, recall 0.827586\n",
      "temporal 2020-10-10T22:26:13.281867: step 350, loss 0.652011, acc 0.875, precision 0.9, recall 0.84375\n",
      "temporal 2020-10-10T22:26:13.527973: step 351, loss 0.789776, acc 0.859375, precision 0.804878, recall 0.970588\n",
      "temporal 2020-10-10T22:26:13.785999: step 352, loss 0.337256, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:26:14.033924: step 353, loss 0.485224, acc 0.875, precision 0.875, recall 0.807692\n",
      "temporal 2020-10-10T22:26:14.282412: step 354, loss 0.626087, acc 0.875, precision 0.9, recall 0.84375\n",
      "temporal 2020-10-10T22:26:14.527167: step 355, loss 0.37129, acc 0.875, precision 0.9, recall 0.84375\n",
      "temporal 2020-10-10T22:26:14.774799: step 356, loss 0.466338, acc 0.9375, precision 0.9, recall 0.964286\n",
      "temporal 2020-10-10T22:26:15.042917: step 357, loss 1.29357, acc 0.8125, precision 0.892857, recall 0.735294\n",
      "temporal 2020-10-10T22:26:15.293992: step 358, loss 0.834203, acc 0.875, precision 0.875, recall 0.807692\n",
      "temporal 2020-10-10T22:26:15.575870: step 359, loss 0.409242, acc 0.90625, precision 0.928571, recall 0.866667\n",
      "temporal 2020-10-10T22:26:15.788314: step 360, loss 0.46715, acc 0.916667, precision 0.875, recall 0.954545\n",
      "temporal 2020-10-10T22:26:16.032988: step 361, loss 0.508959, acc 0.859375, precision 0.904762, recall 0.883721\n",
      "temporal 2020-10-10T22:26:16.281658: step 362, loss 0.54808, acc 0.84375, precision 0.875, recall 0.823529\n",
      "temporal 2020-10-10T22:26:16.524517: step 363, loss 0.470301, acc 0.90625, precision 0.861111, recall 0.96875\n",
      "temporal 2020-10-10T22:26:16.792451: step 364, loss 0.302474, acc 0.90625, precision 0.882353, recall 0.9375\n",
      "temporal 2020-10-10T22:26:17.040166: step 365, loss 0.865134, acc 0.859375, precision 0.833333, recall 0.909091\n",
      "temporal 2020-10-10T22:26:17.283219: step 366, loss 0.313579, acc 0.90625, precision 0.928571, recall 0.866667\n",
      "temporal 2020-10-10T22:26:17.552217: step 367, loss 0.365249, acc 0.890625, precision 0.818182, recall 0.964286\n",
      "temporal 2020-10-10T22:26:17.839958: step 368, loss 0.4629, acc 0.890625, precision 0.9375, recall 0.857143\n",
      "temporal 2020-10-10T22:26:18.084887: step 369, loss 0.375587, acc 0.890625, precision 0.827586, recall 0.923077\n",
      "temporal 2020-10-10T22:26:18.331426: step 370, loss 0.951748, acc 0.8125, precision 0.892857, recall 0.735294\n",
      "temporal 2020-10-10T22:26:18.626387: step 371, loss 0.670555, acc 0.796875, precision 0.870968, recall 0.75\n",
      "temporal 2020-10-10T22:26:18.873796: step 372, loss 0.396021, acc 0.828125, precision 0.764706, recall 0.896552\n",
      "temporal 2020-10-10T22:26:19.129617: step 373, loss 0.331965, acc 0.875, precision 0.8, recall 0.965517\n",
      "temporal 2020-10-10T22:26:19.377407: step 374, loss 0.569485, acc 0.890625, precision 0.852941, recall 0.935484\n",
      "temporal 2020-10-10T22:26:19.646477: step 375, loss 0.431985, acc 0.9375, precision 0.90625, recall 0.966667\n",
      "temporal 2020-10-10T22:26:19.894673: step 376, loss 0.442834, acc 0.90625, precision 0.866667, recall 0.928571\n",
      "temporal 2020-10-10T22:26:20.140162: step 377, loss 1.02281, acc 0.828125, precision 0.827586, recall 0.8\n",
      "temporal 2020-10-10T22:26:20.386370: step 378, loss 0.867275, acc 0.84375, precision 0.911765, recall 0.815789\n",
      "temporal 2020-10-10T22:26:20.635995: step 379, loss 1.02753, acc 0.828125, precision 0.857143, recall 0.774194\n",
      "temporal 2020-10-10T22:26:20.880572: step 380, loss 0.876054, acc 0.8125, precision 0.928571, recall 0.722222\n",
      "temporal 2020-10-10T22:26:21.126872: step 381, loss 0.559199, acc 0.84375, precision 0.769231, recall 0.833333\n",
      "temporal 2020-10-10T22:26:21.370578: step 382, loss 0.583276, acc 0.859375, precision 0.794872, recall 0.96875\n",
      "temporal 2020-10-10T22:26:21.618674: step 383, loss 0.5194, acc 0.875, precision 0.852941, recall 0.90625\n",
      "temporal 2020-10-10T22:26:21.855710: step 384, loss 0.109044, acc 0.979167, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:26:22.102759: step 385, loss 0.391649, acc 0.921875, precision 0.870968, recall 0.964286\n",
      "temporal 2020-10-10T22:26:22.344564: step 386, loss 0.743805, acc 0.78125, precision 0.961538, recall 0.657895\n",
      "temporal 2020-10-10T22:26:22.593768: step 387, loss 0.366026, acc 0.921875, precision 0.888889, recall 0.923077\n",
      "temporal 2020-10-10T22:26:22.842241: step 388, loss 0.330347, acc 0.84375, precision 0.866667, recall 0.8125\n",
      "temporal 2020-10-10T22:26:23.087817: step 389, loss 0.503518, acc 0.875, precision 0.774194, recall 0.96\n",
      "temporal 2020-10-10T22:26:23.356919: step 390, loss 0.155137, acc 0.953125, precision 0.95, recall 0.974359\n",
      "temporal 2020-10-10T22:26:23.603924: step 391, loss 0.27072, acc 0.953125, precision 0.966667, recall 0.935484\n",
      "temporal 2020-10-10T22:26:23.871175: step 392, loss 0.591848, acc 0.859375, precision 0.852941, recall 0.878788\n",
      "temporal 2020-10-10T22:26:24.138344: step 393, loss 0.752419, acc 0.890625, precision 0.914286, recall 0.888889\n",
      "temporal 2020-10-10T22:26:24.384110: step 394, loss 0.49334, acc 0.859375, precision 0.90625, recall 0.828571\n",
      "temporal 2020-10-10T22:26:24.628257: step 395, loss 0.225676, acc 0.9375, precision 1, recall 0.862069\n",
      "temporal 2020-10-10T22:26:24.874512: step 396, loss 0.538235, acc 0.90625, precision 0.90625, recall 0.90625\n",
      "temporal 2020-10-10T22:26:25.122389: step 397, loss 0.802338, acc 0.875, precision 0.914286, recall 0.864865\n",
      "temporal 2020-10-10T22:26:25.366452: step 398, loss 1.03983, acc 0.84375, precision 0.896552, recall 0.787879\n",
      "temporal 2020-10-10T22:26:25.615266: step 399, loss 0.445099, acc 0.859375, precision 0.823529, recall 0.903226\n",
      "temporal 2020-10-10T22:26:25.858186: step 400, loss 0.677513, acc 0.859375, precision 0.85, recall 0.918919\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:26:26.965461: step 400, loss 2.33208, acc 0.512428, precision 0.123894, recall 0.823529, F1 0.215385\n",
      "\n",
      "temporal 2020-10-10T22:26:27.180807: step 401, loss 0.916715, acc 0.8125, precision 0.766667, recall 0.821429\n",
      "temporal 2020-10-10T22:26:27.448082: step 402, loss 0.282876, acc 0.90625, precision 0.861111, recall 0.96875\n",
      "temporal 2020-10-10T22:26:27.715366: step 403, loss 0.351158, acc 0.90625, precision 0.857143, recall 0.923077\n",
      "temporal 2020-10-10T22:26:27.963740: step 404, loss 0.299899, acc 0.921875, precision 0.892857, recall 0.925926\n",
      "temporal 2020-10-10T22:26:28.243998: step 405, loss 0.441246, acc 0.859375, precision 0.962963, recall 0.764706\n",
      "temporal 2020-10-10T22:26:28.511742: step 406, loss 0.571012, acc 0.875, precision 0.958333, recall 0.766667\n",
      "temporal 2020-10-10T22:26:28.755910: step 407, loss 0.598145, acc 0.859375, precision 0.914286, recall 0.842105\n",
      "temporal 2020-10-10T22:26:28.985751: step 408, loss 0.721495, acc 0.833333, precision 0.8, recall 0.869565\n",
      "temporal 2020-10-10T22:26:29.234862: step 409, loss 0.34851, acc 0.90625, precision 0.888889, recall 0.941176\n",
      "temporal 2020-10-10T22:26:29.476284: step 410, loss 0.59685, acc 0.84375, precision 0.769231, recall 0.967742\n",
      "temporal 2020-10-10T22:26:29.717661: step 411, loss 0.315051, acc 0.921875, precision 0.964286, recall 0.870968\n",
      "temporal 2020-10-10T22:26:29.985819: step 412, loss 0.662451, acc 0.859375, precision 0.9375, recall 0.810811\n",
      "temporal 2020-10-10T22:26:30.257699: step 413, loss 0.56118, acc 0.890625, precision 0.909091, recall 0.882353\n",
      "temporal 2020-10-10T22:26:30.520456: step 414, loss 0.280407, acc 0.921875, precision 0.9, recall 0.931035\n",
      "temporal 2020-10-10T22:26:30.768785: step 415, loss 0.273786, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:26:31.028848: step 416, loss 0.315775, acc 0.90625, precision 0.880952, recall 0.973684\n",
      "temporal 2020-10-10T22:26:31.275036: step 417, loss 0.659002, acc 0.890625, precision 0.903226, recall 0.875\n",
      "temporal 2020-10-10T22:26:31.524372: step 418, loss 0.335589, acc 0.921875, precision 0.925926, recall 0.892857\n",
      "temporal 2020-10-10T22:26:31.770211: step 419, loss 0.368888, acc 0.890625, precision 0.966667, recall 0.828571\n",
      "temporal 2020-10-10T22:26:32.039732: step 420, loss 0.674564, acc 0.859375, precision 0.833333, recall 0.909091\n",
      "temporal 2020-10-10T22:26:32.283569: step 421, loss 0.54473, acc 0.84375, precision 0.794872, recall 0.939394\n",
      "temporal 2020-10-10T22:26:32.551575: step 422, loss 0.238661, acc 0.9375, precision 0.935484, recall 0.935484\n",
      "temporal 2020-10-10T22:26:32.822662: step 423, loss 0.266919, acc 0.90625, precision 0.868421, recall 0.970588\n",
      "temporal 2020-10-10T22:26:33.068425: step 424, loss 0.475977, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:26:33.313758: step 425, loss 0.240904, acc 0.9375, precision 0.941176, recall 0.941176\n",
      "temporal 2020-10-10T22:26:33.580205: step 426, loss 0.319391, acc 0.890625, precision 0.931035, recall 0.84375\n",
      "temporal 2020-10-10T22:26:33.827833: step 427, loss 0.132785, acc 0.953125, precision 0.96, recall 0.923077\n",
      "temporal 2020-10-10T22:26:34.096520: step 428, loss 0.461103, acc 0.875, precision 0.965517, recall 0.8\n",
      "temporal 2020-10-10T22:26:34.338516: step 429, loss 0.636617, acc 0.921875, precision 0.958333, recall 0.851852\n",
      "temporal 2020-10-10T22:26:34.580452: step 430, loss 0.809764, acc 0.875, precision 0.848485, recall 0.903226\n",
      "temporal 2020-10-10T22:26:34.823299: step 431, loss 1.18876, acc 0.828125, precision 0.771429, recall 0.9\n",
      "temporal 2020-10-10T22:26:35.037749: step 432, loss 0.329966, acc 0.958333, precision 0.952381, recall 0.952381\n",
      "temporal 2020-10-10T22:26:35.302237: step 433, loss 0.4907, acc 0.890625, precision 0.861111, recall 0.939394\n",
      "temporal 2020-10-10T22:26:35.547383: step 434, loss 0.370644, acc 0.90625, precision 0.892857, recall 0.892857\n",
      "temporal 2020-10-10T22:26:35.789727: step 435, loss 0.420465, acc 0.921875, precision 0.965517, recall 0.875\n",
      "temporal 2020-10-10T22:26:36.031735: step 436, loss 0.500883, acc 0.875, precision 0.76, recall 0.904762\n",
      "temporal 2020-10-10T22:26:36.297614: step 437, loss 0.098742, acc 0.96875, precision 0.97619, recall 0.97619\n",
      "temporal 2020-10-10T22:26:36.543257: step 438, loss 0.415804, acc 0.921875, precision 0.931035, recall 0.9\n",
      "temporal 2020-10-10T22:26:36.790165: step 439, loss 0.772058, acc 0.890625, precision 0.88, recall 0.846154\n",
      "temporal 2020-10-10T22:26:37.034956: step 440, loss 0.398499, acc 0.9375, precision 1, recall 0.885714\n",
      "temporal 2020-10-10T22:26:37.283751: step 441, loss 0.598474, acc 0.890625, precision 0.964286, recall 0.818182\n",
      "temporal 2020-10-10T22:26:37.584534: step 442, loss 0.900057, acc 0.828125, precision 0.888889, recall 0.75\n",
      "temporal 2020-10-10T22:26:37.831237: step 443, loss 1.16439, acc 0.8125, precision 0.736842, recall 0.933333\n",
      "temporal 2020-10-10T22:26:38.075229: step 444, loss 0.722986, acc 0.828125, precision 0.871795, recall 0.85\n",
      "temporal 2020-10-10T22:26:38.324307: step 445, loss 0.634122, acc 0.84375, precision 0.761905, recall 1\n",
      "temporal 2020-10-10T22:26:38.593350: step 446, loss 0.693529, acc 0.890625, precision 0.864865, recall 0.941176\n",
      "temporal 2020-10-10T22:26:38.858341: step 447, loss 0.33982, acc 0.890625, precision 0.827586, recall 0.923077\n",
      "temporal 2020-10-10T22:26:39.122008: step 448, loss 0.16069, acc 0.953125, precision 1, recall 0.90625\n",
      "temporal 2020-10-10T22:26:39.409792: step 449, loss 0.383553, acc 0.90625, precision 0.944444, recall 0.772727\n",
      "temporal 2020-10-10T22:26:39.656694: step 450, loss 0.511073, acc 0.875, precision 0.967742, recall 0.810811\n",
      "temporal 2020-10-10T22:26:39.899811: step 451, loss 0.257091, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:26:40.179550: step 452, loss 0.36458, acc 0.921875, precision 1, recall 0.857143\n",
      "temporal 2020-10-10T22:26:40.430502: step 453, loss 0.312896, acc 0.875, precision 0.965517, recall 0.8\n",
      "temporal 2020-10-10T22:26:40.674632: step 454, loss 0.213513, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:26:40.921093: step 455, loss 0.656555, acc 0.890625, precision 0.833333, recall 0.967742\n",
      "temporal 2020-10-10T22:26:41.156956: step 456, loss 0.475445, acc 0.833333, precision 0.857143, recall 0.857143\n",
      "temporal 2020-10-10T22:26:41.401944: step 457, loss 0.130322, acc 0.953125, precision 0.942857, recall 0.970588\n",
      "temporal 2020-10-10T22:26:41.657436: step 458, loss 0.674544, acc 0.890625, precision 0.902439, recall 0.925\n",
      "temporal 2020-10-10T22:26:41.902254: step 459, loss 0.291806, acc 0.90625, precision 0.888889, recall 0.941176\n",
      "temporal 2020-10-10T22:26:42.172621: step 460, loss 0.437079, acc 0.890625, precision 0.774194, recall 1\n",
      "temporal 2020-10-10T22:26:42.418101: step 461, loss 0.293359, acc 0.9375, precision 0.942857, recall 0.942857\n",
      "temporal 2020-10-10T22:26:42.666462: step 462, loss 0.549715, acc 0.875, precision 0.909091, recall 0.857143\n",
      "temporal 2020-10-10T22:26:42.936450: step 463, loss 0.412403, acc 0.921875, precision 0.965517, recall 0.875\n",
      "temporal 2020-10-10T22:26:43.179334: step 464, loss 0.313542, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:26:43.426776: step 465, loss 0.434353, acc 0.90625, precision 0.961538, recall 0.833333\n",
      "temporal 2020-10-10T22:26:43.667464: step 466, loss 0.772843, acc 0.828125, precision 0.964286, recall 0.72973\n",
      "temporal 2020-10-10T22:26:43.915246: step 467, loss 0.211576, acc 0.921875, precision 1, recall 0.852941\n",
      "temporal 2020-10-10T22:26:44.225637: step 468, loss 0.390559, acc 0.890625, precision 0.941176, recall 0.864865\n",
      "temporal 2020-10-10T22:26:44.496000: step 469, loss 0.576603, acc 0.90625, precision 0.837838, recall 1\n",
      "temporal 2020-10-10T22:26:44.768296: step 470, loss 1.02567, acc 0.78125, precision 0.74359, recall 0.878788\n",
      "temporal 2020-10-10T22:26:45.016760: step 471, loss 0.696552, acc 0.84375, precision 0.805556, recall 0.90625\n",
      "temporal 2020-10-10T22:26:45.260746: step 472, loss 0.645192, acc 0.875, precision 0.764706, recall 1\n",
      "temporal 2020-10-10T22:26:45.501623: step 473, loss 0.346897, acc 0.921875, precision 0.857143, recall 1\n",
      "temporal 2020-10-10T22:26:45.748527: step 474, loss 0.544871, acc 0.875, precision 0.933333, recall 0.823529\n",
      "temporal 2020-10-10T22:26:46.011778: step 475, loss 0.20268, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:26:46.255886: step 476, loss 0.0697746, acc 0.96875, precision 1, recall 0.923077\n",
      "temporal 2020-10-10T22:26:46.503514: step 477, loss 0.897885, acc 0.875, precision 0.954545, recall 0.75\n",
      "temporal 2020-10-10T22:26:46.806697: step 478, loss 1.00414, acc 0.859375, precision 1, recall 0.742857\n",
      "temporal 2020-10-10T22:26:47.052327: step 479, loss 0.412922, acc 0.921875, precision 0.939394, recall 0.911765\n",
      "temporal 2020-10-10T22:26:47.265771: step 480, loss 0.732485, acc 0.854167, precision 0.842105, recall 0.8\n",
      "temporal 2020-10-10T22:26:47.552348: step 481, loss 0.288378, acc 0.9375, precision 0.862069, recall 1\n",
      "temporal 2020-10-10T22:26:47.815147: step 482, loss 0.539367, acc 0.890625, precision 0.805556, recall 1\n",
      "temporal 2020-10-10T22:26:48.057240: step 483, loss 0.852646, acc 0.84375, precision 0.763158, recall 0.966667\n",
      "temporal 2020-10-10T22:26:48.323646: step 484, loss 0.492853, acc 0.875, precision 0.815789, recall 0.96875\n",
      "temporal 2020-10-10T22:26:48.568605: step 485, loss 0.420139, acc 0.890625, precision 0.914286, recall 0.888889\n",
      "temporal 2020-10-10T22:26:48.819730: step 486, loss 0.101695, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:26:49.082905: step 487, loss 0.0986838, acc 0.9375, precision 0.958333, recall 0.884615\n",
      "temporal 2020-10-10T22:26:49.353105: step 488, loss 0.0410975, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:26:49.615117: step 489, loss 0.274106, acc 0.9375, precision 1, recall 0.862069\n",
      "temporal 2020-10-10T22:26:49.881550: step 490, loss 0.591092, acc 0.875, precision 0.92, recall 0.793103\n",
      "temporal 2020-10-10T22:26:50.148407: step 491, loss 0.386569, acc 0.953125, precision 0.966667, recall 0.935484\n",
      "temporal 2020-10-10T22:26:50.393493: step 492, loss 0.324218, acc 0.890625, precision 1, recall 0.810811\n",
      "temporal 2020-10-10T22:26:50.658644: step 493, loss 0.232447, acc 0.921875, precision 0.92, recall 0.884615\n",
      "temporal 2020-10-10T22:26:50.902838: step 494, loss 0.609326, acc 0.90625, precision 0.888889, recall 0.97561\n",
      "temporal 2020-10-10T22:26:51.145218: step 495, loss 0.667353, acc 0.890625, precision 0.833333, recall 1\n",
      "temporal 2020-10-10T22:26:51.390772: step 496, loss 0.462909, acc 0.921875, precision 0.868421, recall 1\n",
      "temporal 2020-10-10T22:26:51.636872: step 497, loss 0.636202, acc 0.828125, precision 0.790698, recall 0.944444\n",
      "temporal 2020-10-10T22:26:51.883152: step 498, loss 0.39441, acc 0.921875, precision 0.909091, recall 0.9375\n",
      "temporal 2020-10-10T22:26:52.147873: step 499, loss 0.66431, acc 0.890625, precision 0.914286, recall 0.888889\n",
      "temporal 2020-10-10T22:26:52.395179: step 500, loss 0.567499, acc 0.890625, precision 0.904762, recall 0.791667\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:26:53.568727: step 500, loss 0.640042, acc 0.856597, precision 0.201835, recall 0.258824, F1 0.226804\n",
      "\n",
      "temporal 2020-10-10T22:26:53.786609: step 501, loss 0.689694, acc 0.890625, precision 1, recall 0.825\n",
      "temporal 2020-10-10T22:26:54.013836: step 502, loss 0.627816, acc 0.84375, precision 1, recall 0.714286\n",
      "temporal 2020-10-10T22:26:54.270598: step 503, loss 0.417771, acc 0.875, precision 0.931035, recall 0.818182\n",
      "temporal 2020-10-10T22:26:54.480479: step 504, loss 0.941526, acc 0.895833, precision 0.862069, recall 0.961538\n",
      "temporal 2020-10-10T22:26:54.748678: step 505, loss 0.645541, acc 0.890625, precision 0.83871, recall 0.928571\n",
      "temporal 2020-10-10T22:26:55.014192: step 506, loss 0.355653, acc 0.9375, precision 0.897436, recall 1\n",
      "temporal 2020-10-10T22:26:55.279890: step 507, loss 0.502275, acc 0.90625, precision 0.853659, recall 1\n",
      "temporal 2020-10-10T22:26:55.526813: step 508, loss 0.16481, acc 0.921875, precision 0.885714, recall 0.96875\n",
      "temporal 2020-10-10T22:26:55.771255: step 509, loss 0.204752, acc 0.953125, precision 1, recall 0.916667\n",
      "temporal 2020-10-10T22:26:56.013091: step 510, loss 0.459071, acc 0.9375, precision 0.964286, recall 0.9\n",
      "temporal 2020-10-10T22:26:56.254947: step 511, loss 0.820954, acc 0.828125, precision 0.888889, recall 0.820513\n",
      "temporal 2020-10-10T22:26:56.512931: step 512, loss 0.285861, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:26:56.754946: step 513, loss 0.0779932, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:26:57.022976: step 514, loss 0.771852, acc 0.90625, precision 1, recall 0.828571\n",
      "temporal 2020-10-10T22:26:57.268405: step 515, loss 0.390948, acc 0.875, precision 0.837838, recall 0.939394\n",
      "temporal 2020-10-10T22:26:57.533249: step 516, loss 0.199822, acc 0.9375, precision 0.916667, recall 0.970588\n",
      "temporal 2020-10-10T22:26:57.774465: step 517, loss 0.394728, acc 0.921875, precision 0.875, recall 1\n",
      "temporal 2020-10-10T22:26:58.015184: step 518, loss 0.273948, acc 0.90625, precision 0.891892, recall 0.942857\n",
      "temporal 2020-10-10T22:26:58.315763: step 519, loss 0.52987, acc 0.875, precision 0.83871, recall 0.896552\n",
      "temporal 2020-10-10T22:26:58.560593: step 520, loss 0.671472, acc 0.84375, precision 0.823529, recall 0.875\n",
      "temporal 2020-10-10T22:26:58.801834: step 521, loss 0.316742, acc 0.9375, precision 0.882353, recall 1\n",
      "temporal 2020-10-10T22:26:59.044171: step 522, loss 0.32543, acc 0.9375, precision 0.933333, recall 0.933333\n",
      "temporal 2020-10-10T22:26:59.309641: step 523, loss 0.318055, acc 0.875, precision 0.961538, recall 0.78125\n",
      "temporal 2020-10-10T22:26:59.553405: step 524, loss 0.580303, acc 0.875, precision 1, recall 0.741935\n",
      "temporal 2020-10-10T22:26:59.833727: step 525, loss 0.315607, acc 0.9375, precision 0.9, recall 0.964286\n",
      "temporal 2020-10-10T22:27:00.095110: step 526, loss 0.204294, acc 0.9375, precision 1, recall 0.866667\n",
      "temporal 2020-10-10T22:27:00.338088: step 527, loss 0.570361, acc 0.84375, precision 0.75, recall 0.964286\n",
      "temporal 2020-10-10T22:27:00.548276: step 528, loss 0.214372, acc 0.9375, precision 0.88, recall 1\n",
      "temporal 2020-10-10T22:27:00.793178: step 529, loss 0.234423, acc 0.921875, precision 1, recall 0.848485\n",
      "temporal 2020-10-10T22:27:01.037909: step 530, loss 0.227922, acc 0.90625, precision 0.870968, recall 0.931035\n",
      "temporal 2020-10-10T22:27:01.326614: step 531, loss 0.020915, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:27:01.570718: step 532, loss 0.353794, acc 0.890625, precision 0.911765, recall 0.885714\n",
      "temporal 2020-10-10T22:27:01.832859: step 533, loss 0.0600796, acc 0.953125, precision 0.971429, recall 0.944444\n",
      "temporal 2020-10-10T22:27:02.073665: step 534, loss 0.214738, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:27:02.316043: step 535, loss 0.171667, acc 0.953125, precision 0.96875, recall 0.939394\n",
      "temporal 2020-10-10T22:27:02.577706: step 536, loss 0.659515, acc 0.921875, precision 0.970588, recall 0.891892\n",
      "temporal 2020-10-10T22:27:02.821115: step 537, loss 0.258625, acc 0.953125, precision 0.909091, recall 1\n",
      "temporal 2020-10-10T22:27:03.063194: step 538, loss 0.259719, acc 0.9375, precision 0.928571, recall 0.928571\n",
      "temporal 2020-10-10T22:27:03.330357: step 539, loss 0.352014, acc 0.921875, precision 0.862069, recall 0.961538\n",
      "temporal 2020-10-10T22:27:03.575436: step 540, loss 0.417205, acc 0.921875, precision 0.964286, recall 0.870968\n",
      "temporal 2020-10-10T22:27:03.820772: step 541, loss 0.0863116, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:27:04.061805: step 542, loss 0.189488, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:27:04.306261: step 543, loss 0.196265, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:27:04.549011: step 544, loss 0.648927, acc 0.921875, precision 1, recall 0.84375\n",
      "temporal 2020-10-10T22:27:04.816098: step 545, loss 0.227336, acc 0.9375, precision 0.933333, recall 0.933333\n",
      "temporal 2020-10-10T22:27:05.083455: step 546, loss 0.237707, acc 0.921875, precision 0.892857, recall 0.925926\n",
      "temporal 2020-10-10T22:27:05.349158: step 547, loss 0.3247, acc 0.921875, precision 0.885714, recall 0.96875\n",
      "temporal 2020-10-10T22:27:05.596134: step 548, loss 0.47641, acc 0.859375, precision 0.833333, recall 0.862069\n",
      "temporal 2020-10-10T22:27:05.842695: step 549, loss 0.324923, acc 0.90625, precision 0.891892, recall 0.942857\n",
      "temporal 2020-10-10T22:27:06.105478: step 550, loss 0.689682, acc 0.859375, precision 0.815789, recall 0.939394\n",
      "temporal 2020-10-10T22:27:06.348881: step 551, loss 0.145504, acc 0.953125, precision 0.914286, recall 1\n",
      "temporal 2020-10-10T22:27:06.583773: step 552, loss 0.405076, acc 0.916667, precision 1, recall 0.870968\n",
      "temporal 2020-10-10T22:27:06.827208: step 553, loss 0.302152, acc 0.890625, precision 0.851852, recall 0.884615\n",
      "temporal 2020-10-10T22:27:07.069852: step 554, loss 0.108542, acc 0.9375, precision 0.941176, recall 0.941176\n",
      "temporal 2020-10-10T22:27:07.337106: step 555, loss 0.340657, acc 0.90625, precision 0.851852, recall 0.92\n",
      "temporal 2020-10-10T22:27:07.581796: step 556, loss 0.442858, acc 0.90625, precision 0.965517, recall 0.848485\n",
      "temporal 2020-10-10T22:27:07.823567: step 557, loss 0.292529, acc 0.90625, precision 0.928571, recall 0.866667\n",
      "temporal 2020-10-10T22:27:08.066354: step 558, loss 0.29985, acc 0.921875, precision 0.9, recall 0.931035\n",
      "temporal 2020-10-10T22:27:08.307320: step 559, loss 0.429935, acc 0.875, precision 0.823529, recall 0.933333\n",
      "temporal 2020-10-10T22:27:08.550796: step 560, loss 0.296419, acc 0.953125, precision 0.947368, recall 0.972973\n",
      "temporal 2020-10-10T22:27:08.794390: step 561, loss 0.351347, acc 0.890625, precision 0.918919, recall 0.894737\n",
      "temporal 2020-10-10T22:27:09.037298: step 562, loss 0.608855, acc 0.921875, precision 0.935484, recall 0.90625\n",
      "temporal 2020-10-10T22:27:09.299741: step 563, loss 0.253712, acc 0.9375, precision 0.964286, recall 0.9\n",
      "temporal 2020-10-10T22:27:09.543521: step 564, loss 0.167393, acc 0.9375, precision 1, recall 0.888889\n",
      "temporal 2020-10-10T22:27:09.787955: step 565, loss 0.594551, acc 0.90625, precision 0.894737, recall 0.944444\n",
      "temporal 2020-10-10T22:27:10.032762: step 566, loss 0.128316, acc 0.953125, precision 0.944444, recall 0.971429\n",
      "temporal 2020-10-10T22:27:10.274035: step 567, loss 0.451564, acc 0.90625, precision 0.875, recall 0.972222\n",
      "temporal 2020-10-10T22:27:10.602386: step 568, loss 0.492103, acc 0.890625, precision 0.888889, recall 0.857143\n",
      "temporal 2020-10-10T22:27:10.871712: step 569, loss 0.253938, acc 0.921875, precision 0.84375, recall 1\n",
      "temporal 2020-10-10T22:27:11.142831: step 570, loss 0.283384, acc 0.953125, precision 0.896552, recall 1\n",
      "temporal 2020-10-10T22:27:11.437587: step 571, loss 0.0598507, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:27:11.688152: step 572, loss 0.648444, acc 0.90625, precision 0.956522, recall 0.814815\n",
      "temporal 2020-10-10T22:27:11.931500: step 573, loss 0.449407, acc 0.9375, precision 0.975, recall 0.928571\n",
      "temporal 2020-10-10T22:27:12.175083: step 574, loss 0.264981, acc 0.921875, precision 0.967742, recall 0.882353\n",
      "temporal 2020-10-10T22:27:12.416057: step 575, loss 0.432125, acc 0.875, precision 0.931035, recall 0.818182\n",
      "temporal 2020-10-10T22:27:12.624936: step 576, loss 0.490598, acc 0.875, precision 0.846154, recall 0.916667\n",
      "temporal 2020-10-10T22:27:12.867002: step 577, loss 0.149498, acc 0.953125, precision 1, recall 0.9\n",
      "temporal 2020-10-10T22:27:13.136093: step 578, loss 0.44128, acc 0.875, precision 0.8, recall 0.965517\n",
      "temporal 2020-10-10T22:27:13.383085: step 579, loss 0.329491, acc 0.9375, precision 0.885714, recall 1\n",
      "temporal 2020-10-10T22:27:13.623461: step 580, loss 0.394698, acc 0.90625, precision 0.888889, recall 0.888889\n",
      "temporal 2020-10-10T22:27:13.857423: step 581, loss 0.305511, acc 0.921875, precision 0.892857, recall 0.925926\n",
      "temporal 2020-10-10T22:27:14.124563: step 582, loss 0.157, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:27:14.369416: step 583, loss 0.226741, acc 0.953125, precision 1, recall 0.9\n",
      "temporal 2020-10-10T22:27:14.609779: step 584, loss 0.577709, acc 0.859375, precision 0.904762, recall 0.730769\n",
      "temporal 2020-10-10T22:27:14.867625: step 585, loss 0.156278, acc 0.953125, precision 1, recall 0.903226\n",
      "temporal 2020-10-10T22:27:15.110730: step 586, loss 0.270676, acc 0.9375, precision 0.933333, recall 0.933333\n",
      "temporal 2020-10-10T22:27:15.346754: step 587, loss 0.666453, acc 0.921875, precision 0.885714, recall 0.96875\n",
      "temporal 2020-10-10T22:27:15.579756: step 588, loss 0.373645, acc 0.921875, precision 0.944444, recall 0.918919\n",
      "temporal 2020-10-10T22:27:15.810296: step 589, loss 0.646373, acc 0.859375, precision 0.823529, recall 0.903226\n",
      "temporal 2020-10-10T22:27:16.041423: step 590, loss 0.113394, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:27:16.297255: step 591, loss 0.365102, acc 0.953125, precision 1, recall 0.921053\n",
      "temporal 2020-10-10T22:27:16.527529: step 592, loss 0.0935171, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:27:16.794519: step 593, loss 0.230209, acc 0.953125, precision 0.935484, recall 0.966667\n",
      "temporal 2020-10-10T22:27:17.044710: step 594, loss 0.11844, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:27:17.289458: step 595, loss 0.127318, acc 0.9375, precision 0.942857, recall 0.942857\n",
      "temporal 2020-10-10T22:27:17.551739: step 596, loss 0.0413463, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:27:17.786999: step 597, loss 0.129216, acc 0.96875, precision 0.95, recall 1\n",
      "temporal 2020-10-10T22:27:18.027009: step 598, loss 0.236494, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:27:18.295558: step 599, loss 0.409211, acc 0.890625, precision 0.904762, recall 0.926829\n",
      "temporal 2020-10-10T22:27:18.529006: step 600, loss 0.363886, acc 0.958333, precision 1, recall 0.923077\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:27:19.684041: step 600, loss 2.41221, acc 0.565966, precision 0.133201, recall 0.788235, F1 0.227891\n",
      "\n",
      "temporal 2020-10-10T22:27:19.896556: step 601, loss 0.0547009, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:27:20.133996: step 602, loss 0.125809, acc 0.953125, precision 0.971429, recall 0.944444\n",
      "temporal 2020-10-10T22:27:20.385202: step 603, loss 0.25557, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:27:20.616404: step 604, loss 0.00709843, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:20.852904: step 605, loss 0.463596, acc 0.90625, precision 0.875, recall 0.933333\n",
      "temporal 2020-10-10T22:27:21.107506: step 606, loss 0.543909, acc 0.875, precision 0.833333, recall 0.9375\n",
      "temporal 2020-10-10T22:27:21.340461: step 607, loss 0.177641, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:27:21.574781: step 608, loss 0.690399, acc 0.921875, precision 0.885714, recall 0.96875\n",
      "temporal 2020-10-10T22:27:21.812848: step 609, loss 0.261046, acc 0.953125, precision 1, recall 0.888889\n",
      "temporal 2020-10-10T22:27:22.069541: step 610, loss 0.138102, acc 0.921875, precision 1, recall 0.857143\n",
      "temporal 2020-10-10T22:27:22.294184: step 611, loss 0.577151, acc 0.90625, precision 0.966667, recall 0.852941\n",
      "temporal 2020-10-10T22:27:22.525145: step 612, loss 0.37349, acc 0.96875, precision 1, recall 0.928571\n",
      "temporal 2020-10-10T22:27:22.750406: step 613, loss 0.191334, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:27:22.987270: step 614, loss 0.260118, acc 0.9375, precision 0.9, recall 0.964286\n",
      "temporal 2020-10-10T22:27:23.219754: step 615, loss 0.621712, acc 0.890625, precision 0.810811, recall 1\n",
      "temporal 2020-10-10T22:27:23.450588: step 616, loss 0.21188, acc 0.9375, precision 0.882353, recall 1\n",
      "temporal 2020-10-10T22:27:23.685198: step 617, loss 0.12052, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:27:23.929348: step 618, loss 0.301007, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:27:24.193096: step 619, loss 0.0753456, acc 0.96875, precision 0.923077, recall 1\n",
      "temporal 2020-10-10T22:27:24.463680: step 620, loss 0.123181, acc 0.9375, precision 0.96875, recall 0.911765\n",
      "temporal 2020-10-10T22:27:24.732272: step 621, loss 0.202098, acc 0.9375, precision 1, recall 0.885714\n",
      "temporal 2020-10-10T22:27:24.976049: step 622, loss 0.254542, acc 0.9375, precision 1, recall 0.870968\n",
      "temporal 2020-10-10T22:27:25.219229: step 623, loss 0.274451, acc 0.953125, precision 1, recall 0.923077\n",
      "temporal 2020-10-10T22:27:25.446438: step 624, loss 0.17713, acc 0.916667, precision 0.857143, recall 0.947368\n",
      "temporal 2020-10-10T22:27:25.690810: step 625, loss 0.502107, acc 0.921875, precision 0.857143, recall 0.96\n",
      "temporal 2020-10-10T22:27:25.959397: step 626, loss 0.216051, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:27:26.204813: step 627, loss 0.233457, acc 0.9375, precision 0.885714, recall 1\n",
      "temporal 2020-10-10T22:27:26.473961: step 628, loss 0.0905525, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:27:26.716997: step 629, loss 0.139353, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:27:26.978413: step 630, loss 0.284994, acc 0.890625, precision 0.857143, recall 0.972973\n",
      "temporal 2020-10-10T22:27:27.256468: step 631, loss 0.434164, acc 0.953125, precision 0.928571, recall 0.962963\n",
      "temporal 2020-10-10T22:27:27.513823: step 632, loss 0.195795, acc 0.921875, precision 0.913043, recall 0.875\n",
      "temporal 2020-10-10T22:27:27.819736: step 633, loss 0.396436, acc 0.90625, precision 1, recall 0.777778\n",
      "temporal 2020-10-10T22:27:28.064020: step 634, loss 0.414455, acc 0.9375, precision 1, recall 0.875\n",
      "temporal 2020-10-10T22:27:28.305761: step 635, loss 0.306137, acc 0.953125, precision 1, recall 0.909091\n",
      "temporal 2020-10-10T22:27:28.547486: step 636, loss 0.330231, acc 0.90625, precision 0.941176, recall 0.888889\n",
      "temporal 2020-10-10T22:27:28.791088: step 637, loss 0.167784, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:27:29.034782: step 638, loss 0.0653113, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:27:29.294931: step 639, loss 0.0951622, acc 0.921875, precision 0.925926, recall 0.892857\n",
      "temporal 2020-10-10T22:27:29.560786: step 640, loss 0.616154, acc 0.875, precision 0.8, recall 1\n",
      "temporal 2020-10-10T22:27:29.824484: step 641, loss 0.217635, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:27:30.139954: step 642, loss 0.207033, acc 0.890625, precision 0.891892, recall 0.916667\n",
      "temporal 2020-10-10T22:27:30.385383: step 643, loss 0.370792, acc 0.90625, precision 0.878788, recall 0.935484\n",
      "temporal 2020-10-10T22:27:30.629649: step 644, loss 0.187155, acc 0.953125, precision 0.947368, recall 0.972973\n",
      "temporal 2020-10-10T22:27:30.873237: step 645, loss 0.337403, acc 0.9375, precision 0.948718, recall 0.948718\n",
      "temporal 2020-10-10T22:27:31.133693: step 646, loss 0.426832, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:27:31.395283: step 647, loss 0.209421, acc 0.96875, precision 1, recall 0.931035\n",
      "temporal 2020-10-10T22:27:31.605582: step 648, loss 0.606649, acc 0.875, precision 0.961538, recall 0.833333\n",
      "temporal 2020-10-10T22:27:31.871769: step 649, loss 0.92796, acc 0.875, precision 0.96, recall 0.774194\n",
      "temporal 2020-10-10T22:27:32.115438: step 650, loss 0.134638, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:27:32.382138: step 651, loss 0.128982, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:27:32.629077: step 652, loss 0.304957, acc 0.9375, precision 0.935484, recall 0.935484\n",
      "temporal 2020-10-10T22:27:32.871917: step 653, loss 0.436948, acc 0.9375, precision 0.903226, recall 0.965517\n",
      "temporal 2020-10-10T22:27:33.114192: step 654, loss 0.317863, acc 0.90625, precision 0.828571, recall 1\n",
      "temporal 2020-10-10T22:27:33.356876: step 655, loss 0.0872722, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:27:33.604216: step 656, loss 0.172955, acc 0.96875, precision 0.95, recall 1\n",
      "temporal 2020-10-10T22:27:33.862623: step 657, loss 0.240905, acc 0.953125, precision 0.935484, recall 0.966667\n",
      "temporal 2020-10-10T22:27:34.107455: step 658, loss 0.290611, acc 0.953125, precision 0.916667, recall 1\n",
      "temporal 2020-10-10T22:27:34.345151: step 659, loss 0.0237832, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:27:34.612427: step 660, loss 0.123452, acc 0.9375, precision 0.965517, recall 0.903226\n",
      "temporal 2020-10-10T22:27:34.878292: step 661, loss 0.265126, acc 0.921875, precision 1, recall 0.84375\n",
      "temporal 2020-10-10T22:27:35.133032: step 662, loss 0.542309, acc 0.90625, precision 1, recall 0.793103\n",
      "temporal 2020-10-10T22:27:35.398610: step 663, loss 0.514226, acc 0.90625, precision 1, recall 0.785714\n",
      "temporal 2020-10-10T22:27:35.640755: step 664, loss 0.220479, acc 0.9375, precision 0.970588, recall 0.916667\n",
      "temporal 2020-10-10T22:27:35.883732: step 665, loss 0.159371, acc 0.921875, precision 0.885714, recall 0.96875\n",
      "temporal 2020-10-10T22:27:36.127280: step 666, loss 0.285234, acc 0.921875, precision 0.894737, recall 0.971429\n",
      "temporal 2020-10-10T22:27:36.390338: step 667, loss 0.365763, acc 0.859375, precision 0.763158, recall 1\n",
      "temporal 2020-10-10T22:27:36.684148: step 668, loss 0.290374, acc 0.890625, precision 0.864865, recall 0.941176\n",
      "temporal 2020-10-10T22:27:36.927223: step 669, loss 0.46273, acc 0.875, precision 0.789474, recall 1\n",
      "temporal 2020-10-10T22:27:37.188531: step 670, loss 0.046752, acc 0.96875, precision 0.948718, recall 1\n",
      "temporal 2020-10-10T22:27:37.433984: step 671, loss 0.772999, acc 0.875, precision 0.941176, recall 0.842105\n",
      "temporal 2020-10-10T22:27:37.657857: step 672, loss 0.370151, acc 0.916667, precision 0.947368, recall 0.857143\n",
      "temporal 2020-10-10T22:27:37.899085: step 673, loss 0.452069, acc 0.90625, precision 1, recall 0.823529\n",
      "temporal 2020-10-10T22:27:38.162809: step 674, loss 0.549051, acc 0.921875, precision 0.962963, recall 0.866667\n",
      "temporal 2020-10-10T22:27:38.428726: step 675, loss 0.883379, acc 0.84375, precision 0.952381, recall 0.689655\n",
      "temporal 2020-10-10T22:27:38.670461: step 676, loss 0.0174236, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:38.914571: step 677, loss 0.685143, acc 0.890625, precision 0.896552, recall 0.866667\n",
      "temporal 2020-10-10T22:27:39.159961: step 678, loss 0.280871, acc 0.921875, precision 0.918919, recall 0.944444\n",
      "temporal 2020-10-10T22:27:39.428108: step 679, loss 0.0210804, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:27:39.670974: step 680, loss 0.880768, acc 0.890625, precision 0.820513, recall 1\n",
      "temporal 2020-10-10T22:27:39.913808: step 681, loss 0.338623, acc 0.921875, precision 0.857143, recall 1\n",
      "temporal 2020-10-10T22:27:40.174593: step 682, loss 0.173631, acc 0.953125, precision 0.931035, recall 0.964286\n",
      "temporal 2020-10-10T22:27:40.419110: step 683, loss 0.570134, acc 0.953125, precision 0.90625, recall 1\n",
      "temporal 2020-10-10T22:27:40.663057: step 684, loss 0.0147512, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:40.929633: step 685, loss 0.282698, acc 0.921875, precision 0.931035, recall 0.9\n",
      "temporal 2020-10-10T22:27:41.170965: step 686, loss 0.409547, acc 0.9375, precision 1, recall 0.870968\n",
      "temporal 2020-10-10T22:27:41.431682: step 687, loss 0.73439, acc 0.890625, precision 1, recall 0.774194\n",
      "temporal 2020-10-10T22:27:41.694391: step 688, loss 0.298291, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:27:41.939415: step 689, loss 0.358822, acc 0.921875, precision 0.939394, recall 0.911765\n",
      "temporal 2020-10-10T22:27:42.182882: step 690, loss 0.342692, acc 0.9375, precision 0.921053, recall 0.972222\n",
      "temporal 2020-10-10T22:27:42.422762: step 691, loss 0.273692, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:27:42.665664: step 692, loss 0.168166, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:27:42.934608: step 693, loss 0.0975817, acc 0.953125, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:27:43.177228: step 694, loss 0.00481326, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:43.438815: step 695, loss 0.0572787, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:27:43.674692: step 696, loss 0.12927, acc 0.9375, precision 0.95, recall 0.904762\n",
      "temporal 2020-10-10T22:27:43.921392: step 697, loss 0.173703, acc 0.9375, precision 0.909091, recall 0.967742\n",
      "temporal 2020-10-10T22:27:44.161984: step 698, loss 0.293782, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:27:44.401084: step 699, loss 0.000781225, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:44.662698: step 700, loss 0.0728838, acc 0.96875, precision 0.933333, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:27:45.812211: step 700, loss 2.07812, acc 0.616635, precision 0.135945, recall 0.694118, F1 0.22736\n",
      "\n",
      "temporal 2020-10-10T22:27:46.060319: step 701, loss 0.528175, acc 0.953125, precision 0.909091, recall 1\n",
      "temporal 2020-10-10T22:27:46.338409: step 702, loss 0.270081, acc 0.921875, precision 0.974359, recall 0.904762\n",
      "temporal 2020-10-10T22:27:46.579008: step 703, loss 0.00951682, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:46.837876: step 704, loss 0.545697, acc 0.953125, precision 1, recall 0.884615\n",
      "temporal 2020-10-10T22:27:47.092688: step 705, loss 0.150957, acc 0.953125, precision 1, recall 0.921053\n",
      "temporal 2020-10-10T22:27:47.334803: step 706, loss 0.0197485, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:47.576247: step 707, loss 0.101726, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:27:47.843031: step 708, loss 0.183323, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:27:48.088624: step 709, loss 0.703382, acc 0.921875, precision 0.880952, recall 1\n",
      "temporal 2020-10-10T22:27:48.359485: step 710, loss 0.0155366, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:48.603458: step 711, loss 0.0917281, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:27:48.866320: step 712, loss 0.219139, acc 0.9375, precision 0.88, recall 0.956522\n",
      "temporal 2020-10-10T22:27:49.135311: step 713, loss 0.337624, acc 0.9375, precision 1, recall 0.888889\n",
      "temporal 2020-10-10T22:27:49.398426: step 714, loss 0.347674, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:27:49.660680: step 715, loss 0.0393404, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:27:49.903763: step 716, loss 0.211141, acc 0.921875, precision 0.969697, recall 0.888889\n",
      "temporal 2020-10-10T22:27:50.143838: step 717, loss 0.0944413, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:27:50.382820: step 718, loss 0.229114, acc 0.96875, precision 0.962963, recall 0.962963\n",
      "temporal 2020-10-10T22:27:50.635600: step 719, loss 0.0922539, acc 0.953125, precision 0.945946, recall 0.972222\n",
      "temporal 2020-10-10T22:27:50.846947: step 720, loss 0.241086, acc 0.916667, precision 0.931035, recall 0.931035\n",
      "temporal 2020-10-10T22:27:51.099637: step 721, loss 0.125656, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:27:51.354254: step 722, loss 0.340922, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:27:51.598722: step 723, loss 0.00143384, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:51.839328: step 724, loss 0.131698, acc 0.9375, precision 0.911765, recall 0.96875\n",
      "temporal 2020-10-10T22:27:52.078443: step 725, loss 0.178822, acc 0.953125, precision 0.944444, recall 0.971429\n",
      "temporal 2020-10-10T22:27:52.323495: step 726, loss 0.141388, acc 0.953125, precision 0.925926, recall 0.961538\n",
      "temporal 2020-10-10T22:27:52.588386: step 727, loss 0.0131064, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:52.835077: step 728, loss 0.126524, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:27:53.078569: step 729, loss 0.0476526, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:27:53.317463: step 730, loss 0.14718, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:27:53.566051: step 731, loss 0.270178, acc 0.890625, precision 1, recall 0.794118\n",
      "temporal 2020-10-10T22:27:53.807692: step 732, loss 0.243213, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:27:54.047965: step 733, loss 0.48437, acc 0.921875, precision 0.925926, recall 0.892857\n",
      "temporal 2020-10-10T22:27:54.292447: step 734, loss 0.0346217, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:27:54.551474: step 735, loss 0.114291, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:27:54.792579: step 736, loss 0.13863, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:27:55.054169: step 737, loss 0.15698, acc 0.953125, precision 0.92, recall 0.958333\n",
      "temporal 2020-10-10T22:27:55.297599: step 738, loss 0.255122, acc 0.9375, precision 0.90625, recall 0.966667\n",
      "temporal 2020-10-10T22:27:55.559403: step 739, loss 0.257282, acc 0.9375, precision 0.967742, recall 0.909091\n",
      "temporal 2020-10-10T22:27:55.800073: step 740, loss 0.613535, acc 0.90625, precision 0.868421, recall 0.970588\n",
      "temporal 2020-10-10T22:27:56.063655: step 741, loss 0.00818392, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:56.329625: step 742, loss 0.608878, acc 0.90625, precision 0.970588, recall 0.868421\n",
      "temporal 2020-10-10T22:27:56.572605: step 743, loss 0.0827248, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:27:56.801891: step 744, loss 0.0385222, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:27:57.066374: step 745, loss 0.0376056, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:27:57.309992: step 746, loss 0.0242121, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:27:57.546874: step 747, loss 0.0884879, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:27:57.793668: step 748, loss 0.0333728, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:27:58.036423: step 749, loss 0.1159, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:27:58.304594: step 750, loss 0.0479186, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:27:58.548264: step 751, loss 0.200176, acc 0.921875, precision 0.852941, recall 1\n",
      "temporal 2020-10-10T22:27:58.788236: step 752, loss 0.117314, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:27:59.056988: step 753, loss 0.0742743, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:27:59.324829: step 754, loss 0.144213, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:27:59.571313: step 755, loss 0.175843, acc 0.953125, precision 0.95, recall 0.974359\n",
      "temporal 2020-10-10T22:27:59.835466: step 756, loss 0.295579, acc 0.921875, precision 0.935484, recall 0.90625\n",
      "temporal 2020-10-10T22:28:00.097453: step 757, loss 0.240733, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:28:00.363955: step 758, loss 0.374515, acc 0.90625, precision 0.969697, recall 0.864865\n",
      "temporal 2020-10-10T22:28:00.606813: step 759, loss 0.440525, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:28:00.847261: step 760, loss 0.0550202, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:28:01.113826: step 761, loss 0.0240484, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:28:01.381987: step 762, loss 0.18653, acc 0.921875, precision 0.891892, recall 0.970588\n",
      "temporal 2020-10-10T22:28:01.644637: step 763, loss 0.108602, acc 0.953125, precision 0.92, recall 0.958333\n",
      "temporal 2020-10-10T22:28:01.882900: step 764, loss 0.3309, acc 0.90625, precision 0.848485, recall 0.965517\n",
      "temporal 2020-10-10T22:28:02.124793: step 765, loss 0.0439174, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:28:02.368218: step 766, loss 0.0156709, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:02.626702: step 767, loss 0.152886, acc 0.9375, precision 0.939394, recall 0.939394\n",
      "temporal 2020-10-10T22:28:02.861346: step 768, loss 0.318881, acc 0.9375, precision 0.96, recall 0.923077\n",
      "temporal 2020-10-10T22:28:03.119897: step 769, loss 0.146068, acc 0.953125, precision 0.92, recall 0.958333\n",
      "temporal 2020-10-10T22:28:03.392562: step 770, loss 0.137631, acc 0.921875, precision 0.944444, recall 0.918919\n",
      "temporal 2020-10-10T22:28:03.661862: step 771, loss 0.482563, acc 0.9375, precision 1, recall 0.862069\n",
      "temporal 2020-10-10T22:28:03.905701: step 772, loss 0.271287, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:28:04.173316: step 773, loss 0.0139209, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:04.436879: step 774, loss 0.0963247, acc 0.953125, precision 0.972222, recall 0.945946\n",
      "temporal 2020-10-10T22:28:04.684314: step 775, loss 0.0619923, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:28:04.925568: step 776, loss 0.334417, acc 0.953125, precision 0.967742, recall 0.9375\n",
      "temporal 2020-10-10T22:28:05.164449: step 777, loss 0.0742046, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:28:05.408204: step 778, loss 0.115197, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:28:05.651291: step 779, loss 0.00134888, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:05.892613: step 780, loss 0.104305, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:28:06.150992: step 781, loss 0.150986, acc 0.96875, precision 1, recall 0.92\n",
      "temporal 2020-10-10T22:28:06.395016: step 782, loss 0.0646218, acc 0.96875, precision 1, recall 0.916667\n",
      "temporal 2020-10-10T22:28:06.634438: step 783, loss 0.240732, acc 0.953125, precision 1, recall 0.918919\n",
      "temporal 2020-10-10T22:28:06.898536: step 784, loss 0.130051, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:28:07.142094: step 785, loss 0.146342, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:28:07.382445: step 786, loss 0.407817, acc 0.921875, precision 0.966667, recall 0.878788\n",
      "temporal 2020-10-10T22:28:07.654035: step 787, loss 0.116595, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:28:07.897571: step 788, loss 0.129204, acc 0.953125, precision 0.935484, recall 0.966667\n",
      "temporal 2020-10-10T22:28:08.139713: step 789, loss 0.00954742, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:08.389823: step 790, loss 0.154805, acc 0.953125, precision 0.916667, recall 1\n",
      "temporal 2020-10-10T22:28:08.644833: step 791, loss 0.421286, acc 0.9375, precision 0.9, recall 1\n",
      "temporal 2020-10-10T22:28:08.851959: step 792, loss 0.208637, acc 0.9375, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:28:09.113527: step 793, loss 0.246194, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:28:09.357107: step 794, loss 0.0882828, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:28:09.601846: step 795, loss 0.184248, acc 0.953125, precision 1, recall 0.916667\n",
      "temporal 2020-10-10T22:28:09.844397: step 796, loss 0.56949, acc 0.90625, precision 0.958333, recall 0.821429\n",
      "temporal 2020-10-10T22:28:10.183264: step 797, loss 0.253524, acc 0.9375, precision 0.909091, recall 0.967742\n",
      "temporal 2020-10-10T22:28:10.425821: step 798, loss 0.044961, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:28:10.665733: step 799, loss 0.45442, acc 0.921875, precision 1, recall 0.84375\n",
      "temporal 2020-10-10T22:28:10.905769: step 800, loss 0.273613, acc 0.953125, precision 1, recall 0.909091\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:28:12.035169: step 800, loss 1.5805, acc 0.67304, precision 0.142061, recall 0.6, F1 0.22973\n",
      "\n",
      "temporal 2020-10-10T22:28:12.248226: step 801, loss 0.0791583, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:28:12.478881: step 802, loss 0.0202737, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:12.721576: step 803, loss 0.136992, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:28:12.991815: step 804, loss 0.126372, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:28:13.254128: step 805, loss 0.238207, acc 0.9375, precision 0.941176, recall 0.941176\n",
      "temporal 2020-10-10T22:28:13.522845: step 806, loss 0.0854273, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:28:13.782441: step 807, loss 0.13182, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:28:14.021852: step 808, loss 0.00482703, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:14.286682: step 809, loss 0.193946, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:28:14.553772: step 810, loss 0.0872026, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:28:14.816213: step 811, loss 0.151114, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:28:15.059966: step 812, loss 0.0814312, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:28:15.351821: step 813, loss 0.074503, acc 0.953125, precision 0.92, recall 0.958333\n",
      "temporal 2020-10-10T22:28:15.595396: step 814, loss 0.0280363, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:15.839448: step 815, loss 0.0862372, acc 0.953125, precision 0.942857, recall 0.970588\n",
      "temporal 2020-10-10T22:28:16.073255: step 816, loss 0.192862, acc 0.9375, precision 0.962963, recall 0.928571\n",
      "temporal 2020-10-10T22:28:16.317456: step 817, loss 0.0363464, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:28:16.563927: step 818, loss 0.1848, acc 0.953125, precision 1, recall 0.916667\n",
      "temporal 2020-10-10T22:28:16.811483: step 819, loss 0.183943, acc 0.953125, precision 0.962963, recall 0.928571\n",
      "temporal 2020-10-10T22:28:17.075571: step 820, loss 0.161365, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:28:17.316243: step 821, loss 0.0186853, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:17.559814: step 822, loss 0.231275, acc 0.953125, precision 0.971429, recall 0.944444\n",
      "temporal 2020-10-10T22:28:17.799860: step 823, loss 0.0599942, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:28:18.062660: step 824, loss 0.404006, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:28:18.306032: step 825, loss 0.429593, acc 0.953125, precision 0.896552, recall 1\n",
      "temporal 2020-10-10T22:28:18.547446: step 826, loss 0.300667, acc 0.953125, precision 0.9375, recall 0.967742\n",
      "temporal 2020-10-10T22:28:18.787461: step 827, loss 0.136285, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:28:19.052966: step 828, loss 0.114977, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:28:19.294498: step 829, loss 0.114151, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:19.557809: step 830, loss 0.0621339, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:28:19.800335: step 831, loss 0.347533, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:28:20.064933: step 832, loss 0.19568, acc 0.953125, precision 0.970588, recall 0.942857\n",
      "temporal 2020-10-10T22:28:20.328971: step 833, loss 0.358871, acc 0.9375, precision 1, recall 0.866667\n",
      "temporal 2020-10-10T22:28:20.568799: step 834, loss 0.214305, acc 0.921875, precision 0.9375, recall 0.909091\n",
      "temporal 2020-10-10T22:28:20.826608: step 835, loss 0.000892912, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:21.072629: step 836, loss 0.458876, acc 0.953125, precision 0.88, recall 1\n",
      "temporal 2020-10-10T22:28:21.315419: step 837, loss 0.0849666, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:28:21.558484: step 838, loss 0.0530342, acc 0.96875, precision 0.951219, recall 1\n",
      "temporal 2020-10-10T22:28:21.802744: step 839, loss 0.0228766, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:28:22.012646: step 840, loss 0.0074664, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:22.252024: step 841, loss 0.172613, acc 0.953125, precision 0.964286, recall 0.931035\n",
      "temporal 2020-10-10T22:28:22.517710: step 842, loss 0.111292, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:28:22.760618: step 843, loss 0.138172, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:28:23.007542: step 844, loss 0.0669007, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:28:23.248185: step 845, loss 0.0208871, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:28:23.491540: step 846, loss 0.00765106, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:23.735862: step 847, loss 0.0258601, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:28:23.983664: step 848, loss 0.204756, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:28:24.220407: step 849, loss 0.066024, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:28:24.483347: step 850, loss 0.0364776, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:24.742373: step 851, loss 0.159412, acc 0.953125, precision 0.884615, recall 1\n",
      "temporal 2020-10-10T22:28:24.983621: step 852, loss 0.0894238, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:28:25.224547: step 853, loss 0.468037, acc 0.9375, precision 0.945946, recall 0.945946\n",
      "temporal 2020-10-10T22:28:25.462512: step 854, loss 0.2343, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:28:25.704321: step 855, loss 0.00892889, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:25.946630: step 856, loss 0.164358, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:28:26.187778: step 857, loss 0.152818, acc 0.953125, precision 1, recall 0.884615\n",
      "temporal 2020-10-10T22:28:26.431333: step 858, loss 0.0674644, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:28:26.685155: step 859, loss 0.0184417, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:26.928453: step 860, loss 0.216592, acc 0.9375, precision 0.888889, recall 0.96\n",
      "temporal 2020-10-10T22:28:27.169707: step 861, loss 0.0861032, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:28:27.413148: step 862, loss 0.250872, acc 0.9375, precision 0.911765, recall 0.96875\n",
      "temporal 2020-10-10T22:28:27.678082: step 863, loss 0.0500594, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:28:27.925813: step 864, loss 0.209711, acc 0.9375, precision 0.884615, recall 1\n",
      "temporal 2020-10-10T22:28:28.199653: step 865, loss 0.113653, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:28:28.441226: step 866, loss 0.0096056, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:28.702949: step 867, loss 0.0360211, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:28:28.944242: step 868, loss 0.101865, acc 0.9375, precision 0.9375, recall 0.9375\n",
      "temporal 2020-10-10T22:28:29.187030: step 869, loss 0.372606, acc 0.953125, precision 0.964286, recall 0.931035\n",
      "temporal 2020-10-10T22:28:29.431302: step 870, loss 0.280282, acc 0.9375, precision 0.966667, recall 0.90625\n",
      "temporal 2020-10-10T22:28:29.700266: step 871, loss 0.213781, acc 0.953125, precision 0.923077, recall 0.96\n",
      "temporal 2020-10-10T22:28:29.967162: step 872, loss 0.232376, acc 0.9375, precision 0.935484, recall 0.935484\n",
      "temporal 2020-10-10T22:28:30.207565: step 873, loss 0.811099, acc 0.859375, precision 0.9375, recall 0.810811\n",
      "temporal 2020-10-10T22:28:30.453834: step 874, loss 0.0259935, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:28:30.692168: step 875, loss 0.130353, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:28:30.933204: step 876, loss 0.161944, acc 0.9375, precision 0.933333, recall 0.933333\n",
      "temporal 2020-10-10T22:28:31.175661: step 877, loss 0.0444206, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:28:31.415501: step 878, loss 0.182226, acc 0.953125, precision 0.923077, recall 1\n",
      "temporal 2020-10-10T22:28:31.656120: step 879, loss 0.0525955, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:28:31.902495: step 880, loss 0.193925, acc 0.953125, precision 0.9375, recall 0.967742\n",
      "temporal 2020-10-10T22:28:32.167129: step 881, loss 0.169243, acc 0.953125, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:28:32.433914: step 882, loss 0.141897, acc 0.96875, precision 0.975, recall 0.975\n",
      "temporal 2020-10-10T22:28:32.678666: step 883, loss 0.0839886, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:28:32.920649: step 884, loss 0.442849, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:28:33.163288: step 885, loss 0.259377, acc 0.953125, precision 0.966667, recall 0.935484\n",
      "temporal 2020-10-10T22:28:33.405565: step 886, loss 0.132498, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:28:33.660854: step 887, loss 0.112428, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:28:33.929175: step 888, loss 0.118764, acc 0.979167, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:28:34.171470: step 889, loss 0.0703774, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:28:34.411489: step 890, loss 0.0484717, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:28:34.653032: step 891, loss 0.304691, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:28:34.913337: step 892, loss 0.434138, acc 0.921875, precision 0.9, recall 0.931035\n",
      "temporal 2020-10-10T22:28:35.155548: step 893, loss 0.22399, acc 0.953125, precision 0.914286, recall 1\n",
      "temporal 2020-10-10T22:28:35.418120: step 894, loss 0.275108, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:28:35.683734: step 895, loss 0.0200836, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:28:35.927327: step 896, loss 0.386782, acc 0.9375, precision 0.967742, recall 0.909091\n",
      "temporal 2020-10-10T22:28:36.213422: step 897, loss 0.297225, acc 0.9375, precision 1, recall 0.878788\n",
      "temporal 2020-10-10T22:28:36.455939: step 898, loss 0.0637425, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:28:36.693966: step 899, loss 0.225249, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:28:36.950451: step 900, loss 0.0370521, acc 0.984375, precision 0.969697, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:28:38.079322: step 900, loss 2.50907, acc 0.587954, precision 0.133475, recall 0.741176, F1 0.226212\n",
      "\n",
      "temporal 2020-10-10T22:28:38.329618: step 901, loss 0.192001, acc 0.96875, precision 1, recall 0.92\n",
      "temporal 2020-10-10T22:28:38.555434: step 902, loss 0.00543706, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:38.808255: step 903, loss 0.127107, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:39.073046: step 904, loss 0.475942, acc 0.90625, precision 0.828571, recall 1\n",
      "temporal 2020-10-10T22:28:39.339483: step 905, loss 0.137248, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:28:39.584086: step 906, loss 0.122821, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:28:39.846675: step 907, loss 0.201453, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:28:40.119813: step 908, loss 0.224872, acc 0.953125, precision 0.961538, recall 0.925926\n",
      "temporal 2020-10-10T22:28:40.373479: step 909, loss 0.0525093, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:28:40.614796: step 910, loss 0.27527, acc 0.90625, precision 1, recall 0.828571\n",
      "temporal 2020-10-10T22:28:40.857142: step 911, loss 0.00459095, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:41.068534: step 912, loss 0.0602919, acc 0.958333, precision 0.962963, recall 0.962963\n",
      "temporal 2020-10-10T22:28:41.328510: step 913, loss 0.131369, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:28:41.569450: step 914, loss 0.00475369, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:41.812326: step 915, loss 0.311663, acc 0.953125, precision 0.961538, recall 0.925926\n",
      "temporal 2020-10-10T22:28:42.052013: step 916, loss 0.100239, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:28:42.294623: step 917, loss 0.054624, acc 0.96875, precision 0.974359, recall 0.974359\n",
      "temporal 2020-10-10T22:28:42.537756: step 918, loss 0.0760472, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:42.804010: step 919, loss 0.380932, acc 0.9375, precision 0.965517, recall 0.903226\n",
      "temporal 2020-10-10T22:28:43.048669: step 920, loss 0.0612361, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:28:43.296105: step 921, loss 0.0448046, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:28:43.537840: step 922, loss 0.161368, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:28:43.779005: step 923, loss 0.0604909, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:28:44.044167: step 924, loss 0.0139529, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:44.286614: step 925, loss 0.0559223, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:28:44.529697: step 926, loss 0.0918889, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:28:44.773214: step 927, loss 0.566791, acc 0.921875, precision 0.935484, recall 0.90625\n",
      "temporal 2020-10-10T22:28:45.014219: step 928, loss 0.189689, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:28:45.282693: step 929, loss 0.228795, acc 0.9375, precision 0.964286, recall 0.9\n",
      "temporal 2020-10-10T22:28:45.525126: step 930, loss 0.11031, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:28:45.765653: step 931, loss 0.190587, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:28:46.005356: step 932, loss 0.224783, acc 0.953125, precision 0.966667, recall 0.935484\n",
      "temporal 2020-10-10T22:28:46.245684: step 933, loss 0.29357, acc 0.921875, precision 0.918919, recall 0.944444\n",
      "temporal 2020-10-10T22:28:46.486454: step 934, loss 0.242253, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:28:46.749684: step 935, loss 0.808721, acc 0.875, precision 0.807692, recall 0.875\n",
      "temporal 2020-10-10T22:28:46.957536: step 936, loss 0.29243, acc 0.9375, precision 0.916667, recall 0.956522\n",
      "temporal 2020-10-10T22:28:47.223906: step 937, loss 0.0371713, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:28:47.466212: step 938, loss 0.0837685, acc 0.984375, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:28:47.706599: step 939, loss 0.0871094, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:28:47.974757: step 940, loss 0.119549, acc 0.953125, precision 0.931035, recall 0.964286\n",
      "temporal 2020-10-10T22:28:48.239473: step 941, loss 0.00174402, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:48.485520: step 942, loss 0.160606, acc 0.9375, precision 0.931035, recall 0.931035\n",
      "temporal 2020-10-10T22:28:48.752619: step 943, loss 0.2544, acc 0.953125, precision 1, recall 0.90625\n",
      "temporal 2020-10-10T22:28:49.023427: step 944, loss 0.0384838, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:28:49.289847: step 945, loss 0.122568, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:28:49.531655: step 946, loss 0.0685335, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:28:49.800601: step 947, loss 0.00159131, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:50.041525: step 948, loss 0.00888141, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:50.284663: step 949, loss 0.00358355, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:50.526663: step 950, loss 0.0826316, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:28:50.820493: step 951, loss 0.0774985, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:28:51.064652: step 952, loss 0.0585448, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:28:51.313866: step 953, loss 0.0676703, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:28:51.576448: step 954, loss 0.0495099, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:28:51.820122: step 955, loss 0.108765, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:28:52.059237: step 956, loss 0.241283, acc 0.9375, precision 0.892857, recall 0.961538\n",
      "temporal 2020-10-10T22:28:52.297807: step 957, loss 0.00461031, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:52.548981: step 958, loss 0.0450987, acc 0.96875, precision 0.951219, recall 1\n",
      "temporal 2020-10-10T22:28:52.813540: step 959, loss 0.0140747, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:28:53.021007: step 960, loss 0.309484, acc 0.916667, precision 0.913043, recall 0.913043\n",
      "temporal 2020-10-10T22:28:53.261563: step 961, loss 0.00337377, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:53.502616: step 962, loss 0.00182709, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:53.769932: step 963, loss 0.0548809, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:28:54.010213: step 964, loss 0.0937646, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:28:54.263710: step 965, loss 0.0112417, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:54.501824: step 966, loss 0.0152603, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:28:54.745458: step 967, loss 0.0139094, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:54.988815: step 968, loss 0.0104081, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:55.228886: step 969, loss 0.0644591, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:28:55.496445: step 970, loss 0.00281184, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:55.761866: step 971, loss 0.0235453, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:28:56.003328: step 972, loss 0.175001, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:28:56.242611: step 973, loss 0.000788266, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:56.508262: step 974, loss 0.0778461, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:28:56.750639: step 975, loss 0.00124563, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:57.032899: step 976, loss 0.368233, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:28:57.277378: step 977, loss 0.0468118, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:28:57.515680: step 978, loss 0.137781, acc 0.9375, precision 0.90625, recall 0.966667\n",
      "temporal 2020-10-10T22:28:57.775229: step 979, loss 0.195509, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:28:58.016391: step 980, loss 0.0382428, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:28:58.259391: step 981, loss 0.143485, acc 0.921875, precision 0.964286, recall 0.870968\n",
      "temporal 2020-10-10T22:28:58.521504: step 982, loss 0.340432, acc 0.953125, precision 0.970588, recall 0.942857\n",
      "temporal 2020-10-10T22:28:58.787569: step 983, loss 0.000577507, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:59.016380: step 984, loss 0.014123, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:28:59.257791: step 985, loss 0.176559, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:28:59.501478: step 986, loss 0.223131, acc 0.9375, precision 0.967742, recall 0.909091\n",
      "temporal 2020-10-10T22:28:59.768704: step 987, loss 0.391034, acc 0.921875, precision 0.96, recall 0.857143\n",
      "temporal 2020-10-10T22:29:00.010935: step 988, loss 0.0301318, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:29:00.253656: step 989, loss 0.0319665, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:29:00.496374: step 990, loss 0.0498704, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:29:00.737983: step 991, loss 0.0477307, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:29:00.980300: step 992, loss 0.0414044, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:29:01.219828: step 993, loss 0.0136383, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:01.460867: step 994, loss 0.161959, acc 0.9375, precision 0.870968, recall 1\n",
      "temporal 2020-10-10T22:29:01.746799: step 995, loss 0.259554, acc 0.96875, precision 0.973684, recall 0.973684\n",
      "temporal 2020-10-10T22:29:01.988443: step 996, loss 0.00390887, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:02.247935: step 997, loss 0.060416, acc 0.953125, precision 0.931035, recall 0.964286\n",
      "temporal 2020-10-10T22:29:02.487291: step 998, loss 0.133315, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:29:02.725853: step 999, loss 0.00893696, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:02.969233: step 1000, loss 0.162676, acc 0.953125, precision 1, recall 0.918919\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:29:04.127653: step 1000, loss 1.62795, acc 0.693117, precision 0.135802, recall 0.517647, F1 0.215159\n",
      "\n",
      "temporal 2020-10-10T22:29:04.349098: step 1001, loss 0.0786773, acc 0.96875, precision 1, recall 0.923077\n",
      "temporal 2020-10-10T22:29:04.608274: step 1002, loss 0.0344883, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:29:04.853821: step 1003, loss 0.331884, acc 0.9375, precision 0.948718, recall 0.948718\n",
      "temporal 2020-10-10T22:29:05.103617: step 1004, loss 0.281263, acc 0.9375, precision 0.947368, recall 0.947368\n",
      "temporal 2020-10-10T22:29:05.344872: step 1005, loss 0.0984933, acc 0.9375, precision 0.897436, recall 1\n",
      "temporal 2020-10-10T22:29:05.586445: step 1006, loss 0.0700921, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:05.828845: step 1007, loss 0.0231182, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:29:06.037398: step 1008, loss 0.101724, acc 0.979167, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:29:06.278611: step 1009, loss 0.112595, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:29:06.518485: step 1010, loss 0.181938, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:29:06.760449: step 1011, loss 0.142641, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:29:07.022110: step 1012, loss 0.117311, acc 0.96875, precision 1, recall 0.931035\n",
      "temporal 2020-10-10T22:29:07.285918: step 1013, loss 0.0688624, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:29:07.529268: step 1014, loss 0.135986, acc 0.953125, precision 0.951219, recall 0.975\n",
      "temporal 2020-10-10T22:29:07.768000: step 1015, loss 0.121827, acc 0.96875, precision 0.973684, recall 0.973684\n",
      "temporal 2020-10-10T22:29:08.012200: step 1016, loss 0.124007, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:29:08.273515: step 1017, loss 0.227823, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:29:08.541932: step 1018, loss 0.143242, acc 0.953125, precision 0.933333, recall 0.965517\n",
      "temporal 2020-10-10T22:29:08.785091: step 1019, loss 0.0591748, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:29:09.047714: step 1020, loss 0.0832177, acc 0.953125, precision 0.967742, recall 0.9375\n",
      "temporal 2020-10-10T22:29:09.313341: step 1021, loss 0.0708854, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:29:09.557230: step 1022, loss 0.0798563, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:29:09.798930: step 1023, loss 0.0261548, acc 0.984375, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:29:10.044663: step 1024, loss 0.213712, acc 0.9375, precision 0.941176, recall 0.941176\n",
      "temporal 2020-10-10T22:29:10.287988: step 1025, loss 0.0417825, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:29:10.544710: step 1026, loss 0.1394, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:29:10.786196: step 1027, loss 0.0344235, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:29:11.025620: step 1028, loss 0.034812, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:29:11.265286: step 1029, loss 0.0118017, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:11.531433: step 1030, loss 0.0829253, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:29:11.793904: step 1031, loss 0.13087, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:29:12.004120: step 1032, loss 0.00514862, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:12.264874: step 1033, loss 0.086138, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:29:12.533127: step 1034, loss 0.196598, acc 0.953125, precision 0.933333, recall 0.965517\n",
      "temporal 2020-10-10T22:29:12.801949: step 1035, loss 0.0974057, acc 0.953125, precision 0.909091, recall 1\n",
      "temporal 2020-10-10T22:29:13.045466: step 1036, loss 0.110622, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:29:13.284917: step 1037, loss 6.22213e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:13.526900: step 1038, loss 0.0018504, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:13.773684: step 1039, loss 0.139388, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:29:14.053120: step 1040, loss 0.216826, acc 0.953125, precision 1, recall 0.916667\n",
      "temporal 2020-10-10T22:29:14.296272: step 1041, loss 0.00547578, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:14.561098: step 1042, loss 0.0406218, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:29:14.803355: step 1043, loss 0.0547572, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:29:15.045157: step 1044, loss 0.00883998, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:15.313218: step 1045, loss 0.272936, acc 0.953125, precision 0.935484, recall 0.966667\n",
      "temporal 2020-10-10T22:29:15.552420: step 1046, loss 0.143813, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:29:15.811378: step 1047, loss 0.116392, acc 0.96875, precision 0.947368, recall 1\n",
      "temporal 2020-10-10T22:29:16.050396: step 1048, loss 0.346115, acc 0.953125, precision 0.916667, recall 1\n",
      "temporal 2020-10-10T22:29:16.288009: step 1049, loss 0.000408437, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:16.530747: step 1050, loss 0.174904, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:29:16.771251: step 1051, loss 0.137877, acc 0.96875, precision 0.96, recall 0.96\n",
      "temporal 2020-10-10T22:29:17.033153: step 1052, loss 0.0996462, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:29:17.288514: step 1053, loss 0.115168, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:29:17.583800: step 1054, loss 0.0515559, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:29:17.827701: step 1055, loss 0.00343788, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:18.037816: step 1056, loss 0.173817, acc 0.958333, precision 0.913043, recall 1\n",
      "temporal 2020-10-10T22:29:18.277694: step 1057, loss 0.015173, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:18.519419: step 1058, loss 0.103713, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:29:18.757645: step 1059, loss 0.00105632, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:18.995656: step 1060, loss 0.0368389, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:19.258726: step 1061, loss 0.288173, acc 0.9375, precision 0.969697, recall 0.914286\n",
      "temporal 2020-10-10T22:29:19.502105: step 1062, loss 0.0515882, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:29:19.761820: step 1063, loss 0.00856889, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:20.003352: step 1064, loss 0.452917, acc 0.921875, precision 0.923077, recall 0.888889\n",
      "temporal 2020-10-10T22:29:20.266263: step 1065, loss 0.0974098, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:29:20.510793: step 1066, loss 0.145185, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:29:20.768184: step 1067, loss 0.122975, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:29:21.031245: step 1068, loss 0.233221, acc 0.953125, precision 1, recall 0.909091\n",
      "temporal 2020-10-10T22:29:21.274745: step 1069, loss 0.0253575, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:29:21.533198: step 1070, loss 0.00445407, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:21.790183: step 1071, loss 0.160901, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:29:22.029975: step 1072, loss 0.0213739, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:29:22.271743: step 1073, loss 0.0557329, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:29:22.514164: step 1074, loss 0.0112888, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:22.752690: step 1075, loss 0.0130773, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:22.990229: step 1076, loss 0.00179674, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:23.233543: step 1077, loss 0.0048191, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:23.472522: step 1078, loss 0.056366, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:29:23.713790: step 1079, loss 0.102136, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:29:23.921881: step 1080, loss 0.0987849, acc 0.979167, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:29:24.160498: step 1081, loss 0.220348, acc 0.9375, precision 0.944444, recall 0.944444\n",
      "temporal 2020-10-10T22:29:24.426599: step 1082, loss 0.164444, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:29:24.664995: step 1083, loss 0.340075, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:29:24.929976: step 1084, loss 0.0260427, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:29:25.178176: step 1085, loss 0.00353564, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:25.417124: step 1086, loss 0.0700136, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:29:25.681356: step 1087, loss 0.171498, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:29:25.923929: step 1088, loss 0.00911347, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:26.189358: step 1089, loss 0.00436749, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:26.435918: step 1090, loss 0.0921636, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:29:26.678364: step 1091, loss 0.204449, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:29:26.942630: step 1092, loss 0.000935269, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:27.183817: step 1093, loss 0.0874342, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:29:27.420808: step 1094, loss 0.000695618, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:27.683949: step 1095, loss 0.0867186, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:29:27.924582: step 1096, loss 0.00510851, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:28.159747: step 1097, loss 0.0691568, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:29:28.399379: step 1098, loss 0.161928, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:29:28.640050: step 1099, loss 0.229785, acc 0.953125, precision 0.90625, recall 1\n",
      "temporal 2020-10-10T22:29:28.879574: step 1100, loss 0.107998, acc 0.984375, precision 1, recall 0.967742\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:29:30.026401: step 1100, loss 2.6277, acc 0.606119, precision 0.137472, recall 0.729412, F1 0.231343\n",
      "\n",
      "temporal 2020-10-10T22:29:30.273375: step 1101, loss 0.0145201, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:29:30.538047: step 1102, loss 0.00298696, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:30.795827: step 1103, loss 0.118126, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:29:31.030855: step 1104, loss 0.383217, acc 0.916667, precision 0.962963, recall 0.896552\n",
      "temporal 2020-10-10T22:29:31.299198: step 1105, loss 0.313479, acc 0.9375, precision 0.894737, recall 1\n",
      "temporal 2020-10-10T22:29:31.542545: step 1106, loss 0.520264, acc 0.953125, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:29:31.784513: step 1107, loss 0.0237834, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:29:32.051898: step 1108, loss 0.253446, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:29:32.295480: step 1109, loss 0.0351906, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:29:32.561149: step 1110, loss 0.176159, acc 0.953125, precision 0.952381, recall 0.909091\n",
      "temporal 2020-10-10T22:29:32.824790: step 1111, loss 0.0983241, acc 0.9375, precision 0.969697, recall 0.914286\n",
      "temporal 2020-10-10T22:29:33.080979: step 1112, loss 0.00289104, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:33.347457: step 1113, loss 0.00106854, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:33.589131: step 1114, loss 0.0921481, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:29:33.828453: step 1115, loss 0.023371, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:29:34.069038: step 1116, loss 0.0292849, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:29:34.312620: step 1117, loss 0.248785, acc 0.90625, precision 0.888889, recall 0.888889\n",
      "temporal 2020-10-10T22:29:34.551207: step 1118, loss 0.0240733, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:29:34.807431: step 1119, loss 0.142625, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:29:35.049961: step 1120, loss 0.0703649, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:29:35.312976: step 1121, loss 0.108502, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:29:35.575513: step 1122, loss 0.0590696, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:29:35.816526: step 1123, loss 0.0913426, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:29:36.068146: step 1124, loss 0.0131704, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:36.298966: step 1125, loss 0.0752863, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:29:36.531901: step 1126, loss 0.123483, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:36.765028: step 1127, loss 0.0876701, acc 0.984375, precision 0.976744, recall 1\n",
      "temporal 2020-10-10T22:29:36.958587: step 1128, loss 0.017588, acc 0.979167, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:29:37.192582: step 1129, loss 0.0489775, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:29:37.422706: step 1130, loss 0.00151688, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:37.651214: step 1131, loss 0.252538, acc 0.953125, precision 0.962963, recall 0.928571\n",
      "temporal 2020-10-10T22:29:37.882687: step 1132, loss 0.106154, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:29:38.129898: step 1133, loss 0.0453736, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:29:38.382240: step 1134, loss 0.0655566, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:29:38.611950: step 1135, loss 0.161356, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:29:38.860113: step 1136, loss 0.000145149, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:39.100255: step 1137, loss 0.0144509, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:39.366235: step 1138, loss 0.0303079, acc 0.96875, precision 0.958333, recall 0.958333\n",
      "temporal 2020-10-10T22:29:39.602961: step 1139, loss 0.0742991, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:29:39.844670: step 1140, loss 0.00209474, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:40.080505: step 1141, loss 0.0620986, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:29:40.316608: step 1142, loss 0.0444109, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:29:40.570183: step 1143, loss 0.0735262, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:29:40.799649: step 1144, loss 0.0165753, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:29:41.034486: step 1145, loss 0.0060273, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:41.266429: step 1146, loss 0.00115615, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:41.507176: step 1147, loss 0.0271488, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:29:41.759616: step 1148, loss 0.0155865, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:29:41.986218: step 1149, loss 0.164643, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:29:42.210926: step 1150, loss 0.0474618, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:29:42.442120: step 1151, loss 0.428337, acc 0.9375, precision 1, recall 0.894737\n",
      "temporal 2020-10-10T22:29:42.645013: step 1152, loss 0.193619, acc 0.9375, precision 0.958333, recall 0.92\n",
      "temporal 2020-10-10T22:29:42.899349: step 1153, loss 0.0825748, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:29:43.129683: step 1154, loss 0.0105708, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:43.357513: step 1155, loss 0.14995, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:29:43.596159: step 1156, loss 0.588475, acc 0.9375, precision 0.894737, recall 1\n",
      "temporal 2020-10-10T22:29:43.858009: step 1157, loss 0.138414, acc 0.9375, precision 0.897436, recall 1\n",
      "temporal 2020-10-10T22:29:44.087841: step 1158, loss 0.0912157, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:29:44.316799: step 1159, loss 0.155733, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:29:44.550130: step 1160, loss 0.0857715, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:44.803864: step 1161, loss 0.188832, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:45.046923: step 1162, loss 0.151404, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:29:45.289672: step 1163, loss 0.0564819, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:29:45.528634: step 1164, loss 0.0909567, acc 0.953125, precision 0.970588, recall 0.942857\n",
      "temporal 2020-10-10T22:29:45.761737: step 1165, loss 0.354547, acc 0.9375, precision 1, recall 0.875\n",
      "temporal 2020-10-10T22:29:46.024108: step 1166, loss 0.178938, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:29:46.305159: step 1167, loss 0.220722, acc 0.9375, precision 1, recall 0.875\n",
      "temporal 2020-10-10T22:29:46.545692: step 1168, loss 0.100071, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:29:46.779760: step 1169, loss 0.327051, acc 0.921875, precision 1, recall 0.852941\n",
      "temporal 2020-10-10T22:29:47.026172: step 1170, loss 0.296458, acc 0.9375, precision 0.894737, recall 1\n",
      "temporal 2020-10-10T22:29:47.285644: step 1171, loss 0.225636, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:29:47.547309: step 1172, loss 0.118723, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:29:47.785107: step 1173, loss 0.118655, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:29:48.047020: step 1174, loss 0.07409, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:29:48.310957: step 1175, loss 0.34714, acc 0.953125, precision 0.942857, recall 0.970588\n",
      "temporal 2020-10-10T22:29:48.513813: step 1176, loss 0.262617, acc 0.9375, precision 0.956522, recall 0.916667\n",
      "temporal 2020-10-10T22:29:48.766971: step 1177, loss 0.124834, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:29:48.999742: step 1178, loss 0.00785969, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:49.262358: step 1179, loss 0.221753, acc 0.9375, precision 0.956522, recall 0.88\n",
      "temporal 2020-10-10T22:29:49.508399: step 1180, loss 0.00818863, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:49.744000: step 1181, loss 0.195847, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:29:49.973808: step 1182, loss 0.000210442, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:50.198286: step 1183, loss 0.0349055, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:29:50.437516: step 1184, loss 0.331916, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:29:50.675113: step 1185, loss 0.0445874, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:29:50.912896: step 1186, loss 0.00661232, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:51.147560: step 1187, loss 0.0432241, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:29:51.404644: step 1188, loss 0.0248354, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:29:51.684696: step 1189, loss 0.216604, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:29:51.942714: step 1190, loss 0.210523, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:29:52.182755: step 1191, loss 0.080573, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:29:52.422569: step 1192, loss 0.12545, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:29:52.661039: step 1193, loss 0.0191472, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:29:52.903851: step 1194, loss 0.167781, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:29:53.138687: step 1195, loss 0.00302547, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:53.378189: step 1196, loss 0.36873, acc 0.9375, precision 1, recall 0.875\n",
      "temporal 2020-10-10T22:29:53.635436: step 1197, loss 0.153313, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:29:53.868342: step 1198, loss 0.0343024, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:29:54.102672: step 1199, loss 0.210372, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:29:54.326312: step 1200, loss 0.0838903, acc 0.979167, precision 1, recall 0.954545\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:29:55.449426: step 1200, loss 3.38003, acc 0.571702, precision 0.133333, recall 0.776471, F1 0.227586\n",
      "\n",
      "temporal 2020-10-10T22:29:55.656699: step 1201, loss 0.0387594, acc 0.96875, precision 0.928571, recall 1\n",
      "temporal 2020-10-10T22:29:55.904831: step 1202, loss 0.0694377, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:29:56.152339: step 1203, loss 0.105565, acc 0.9375, precision 0.944444, recall 0.944444\n",
      "temporal 2020-10-10T22:29:56.382168: step 1204, loss 0.563664, acc 0.9375, precision 0.875, recall 1\n",
      "temporal 2020-10-10T22:29:56.601418: step 1205, loss 0.00123515, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:56.854471: step 1206, loss 0.010616, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:57.085626: step 1207, loss 0.000647261, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:57.321476: step 1208, loss 0.361166, acc 0.953125, precision 0.933333, recall 0.965517\n",
      "temporal 2020-10-10T22:29:57.556019: step 1209, loss 0.0100546, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:57.809958: step 1210, loss 0.204633, acc 0.984375, precision 1, recall 0.976744\n",
      "temporal 2020-10-10T22:29:58.052175: step 1211, loss 0.089508, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:29:58.283666: step 1212, loss 0.56676, acc 0.9375, precision 0.956522, recall 0.88\n",
      "temporal 2020-10-10T22:29:58.517720: step 1213, loss 0.00389578, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:58.773844: step 1214, loss 0.0137266, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:29:59.000861: step 1215, loss 0.000583189, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:59.253131: step 1216, loss 0.293566, acc 0.9375, precision 0.972222, recall 0.921053\n",
      "temporal 2020-10-10T22:29:59.476635: step 1217, loss 0.282073, acc 0.953125, precision 0.965517, recall 0.933333\n",
      "temporal 2020-10-10T22:29:59.719418: step 1218, loss 0.00108808, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:29:59.945164: step 1219, loss 0.0374267, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:00.194414: step 1220, loss 0.146182, acc 0.953125, precision 0.914286, recall 1\n",
      "temporal 2020-10-10T22:30:00.410183: step 1221, loss 0.581929, acc 0.953125, precision 0.945946, recall 0.972222\n",
      "temporal 2020-10-10T22:30:00.636174: step 1222, loss 0.0183326, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:00.884803: step 1223, loss 0.0303808, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:01.109132: step 1224, loss 0.00110231, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:01.348139: step 1225, loss 0.0336096, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:30:01.586889: step 1226, loss 0.0994541, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:01.825444: step 1227, loss 0.0997969, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:30:02.081910: step 1228, loss 0.000554886, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:02.327738: step 1229, loss 0.0117379, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:02.558299: step 1230, loss 0.047911, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:30:02.791605: step 1231, loss 0.199852, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:30:03.052242: step 1232, loss 0.0209958, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:03.307202: step 1233, loss 0.0159715, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:03.560438: step 1234, loss 0.116719, acc 0.96875, precision 1, recall 0.948718\n",
      "temporal 2020-10-10T22:30:03.787213: step 1235, loss 0.0222629, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:30:04.018365: step 1236, loss 0.00367468, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:04.244840: step 1237, loss 0.0126545, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:30:04.476232: step 1238, loss 0.197273, acc 0.96875, precision 0.928571, recall 1\n",
      "temporal 2020-10-10T22:30:04.699528: step 1239, loss 0.0313214, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:04.928414: step 1240, loss 0.229436, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:30:05.166007: step 1241, loss 8.80522e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:05.402545: step 1242, loss 0.0509968, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:05.659682: step 1243, loss 0.164599, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:05.914628: step 1244, loss 0.00231022, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:06.143357: step 1245, loss 0.00211691, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:06.381445: step 1246, loss 0.258499, acc 0.9375, precision 0.9, recall 0.964286\n",
      "temporal 2020-10-10T22:30:06.672985: step 1247, loss 0.204014, acc 0.953125, precision 0.92, recall 0.958333\n",
      "temporal 2020-10-10T22:30:06.931224: step 1248, loss 0.411456, acc 0.916667, precision 0.884615, recall 0.958333\n",
      "temporal 2020-10-10T22:30:07.171791: step 1249, loss 0.0165111, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:07.406260: step 1250, loss 0.00651317, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:07.648262: step 1251, loss 0.156977, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:30:07.880026: step 1252, loss 0.096863, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:30:08.110096: step 1253, loss 0.000228053, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:08.339645: step 1254, loss 0.000156604, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:08.576905: step 1255, loss 0.0328611, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:08.815938: step 1256, loss 0.0290524, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:30:09.072802: step 1257, loss 0.0843944, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:30:09.307454: step 1258, loss 0.0116223, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:30:09.579921: step 1259, loss 0.219774, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:30:09.823270: step 1260, loss 0.0120096, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:10.064434: step 1261, loss 0.0165857, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:30:10.361905: step 1262, loss 0.000147244, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:10.603760: step 1263, loss 0.0313343, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:30:10.848364: step 1264, loss 0.224684, acc 0.96875, precision 0.95, recall 1\n",
      "temporal 2020-10-10T22:30:11.091308: step 1265, loss 0.106489, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:30:11.354297: step 1266, loss 0.033872, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:11.613601: step 1267, loss 0.00315399, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:11.878885: step 1268, loss 0.0407804, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:30:12.122496: step 1269, loss 0.0952289, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:30:12.361351: step 1270, loss 0.110059, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:30:12.620696: step 1271, loss 0.0672555, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:12.831437: step 1272, loss 0.00350046, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:13.068581: step 1273, loss 0.0599595, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:13.331184: step 1274, loss 0.101721, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:30:13.585591: step 1275, loss 0.0252665, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:13.842763: step 1276, loss 0.270113, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:30:14.112739: step 1277, loss 0.0661049, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:30:14.375403: step 1278, loss 0.3812, acc 0.921875, precision 0.84375, recall 1\n",
      "temporal 2020-10-10T22:30:14.637949: step 1279, loss 0.0268191, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:14.875118: step 1280, loss 0.00335691, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:15.115208: step 1281, loss 0.0507982, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:30:15.375716: step 1282, loss 0.0021597, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:15.626482: step 1283, loss 0.106322, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:30:15.893462: step 1284, loss 0.0772093, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:30:16.154330: step 1285, loss 0.0137545, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:30:16.392630: step 1286, loss 0.00131025, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:16.631987: step 1287, loss 0.580617, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:30:16.877849: step 1288, loss 0.0736008, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:30:17.142710: step 1289, loss 2.87632e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:17.386357: step 1290, loss 0.021646, acc 0.984375, precision 1, recall 0.97619\n",
      "temporal 2020-10-10T22:30:17.629761: step 1291, loss 0.124903, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:30:17.887806: step 1292, loss 0.238692, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:30:18.129865: step 1293, loss 0.0230404, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:30:18.371654: step 1294, loss 0.0304452, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:18.632470: step 1295, loss 0.031437, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:18.842409: step 1296, loss 0.00535744, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:19.079688: step 1297, loss 0.174511, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:19.316443: step 1298, loss 0.0121344, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:19.552432: step 1299, loss 0.064235, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:30:19.803504: step 1300, loss 0.000730532, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:30:20.955190: step 1300, loss 1.89045, acc 0.709369, precision 0.143322, recall 0.517647, F1 0.22449\n",
      "\n",
      "temporal 2020-10-10T22:30:21.180003: step 1301, loss 0.01066, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:21.430162: step 1302, loss 0.0629275, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:21.673135: step 1303, loss 0.0966047, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:30:21.912786: step 1304, loss 0.0349951, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:22.156238: step 1305, loss 0.0155007, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:22.415192: step 1306, loss 0.131195, acc 0.984375, precision 1, recall 0.97619\n",
      "temporal 2020-10-10T22:30:22.658029: step 1307, loss 0.020942, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:30:22.901884: step 1308, loss 0.0243891, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:30:23.140683: step 1309, loss 0.00265787, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:23.400285: step 1310, loss 0.195418, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:30:23.640532: step 1311, loss 0.019915, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:23.879012: step 1312, loss 0.00037263, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:24.118947: step 1313, loss 0.194707, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:30:24.360545: step 1314, loss 0.0648605, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:24.598637: step 1315, loss 0.000105787, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:24.839052: step 1316, loss 0.0621534, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:30:25.077131: step 1317, loss 0.000453793, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:25.334818: step 1318, loss 0.142227, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:30:25.573744: step 1319, loss 0.165254, acc 0.96875, precision 1, recall 0.9\n",
      "temporal 2020-10-10T22:30:25.782321: step 1320, loss 0.0606903, acc 0.979167, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:30:26.046889: step 1321, loss 0.0614109, acc 0.984375, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:30:26.312550: step 1322, loss 0.183354, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:26.563721: step 1323, loss 0.0784729, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:30:26.803300: step 1324, loss 0.00771981, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:27.062377: step 1325, loss 0.226657, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:30:27.302897: step 1326, loss 0.170187, acc 0.9375, precision 0.903226, recall 0.965517\n",
      "temporal 2020-10-10T22:30:27.572177: step 1327, loss 0.034841, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:30:27.815094: step 1328, loss 0.299902, acc 0.921875, precision 0.868421, recall 1\n",
      "temporal 2020-10-10T22:30:28.076765: step 1329, loss 0.0358436, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:30:28.316286: step 1330, loss 0.000156018, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:28.572503: step 1331, loss 0.186787, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:30:28.814770: step 1332, loss 0.00141356, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:29.049969: step 1333, loss 0.0721555, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:30:29.290976: step 1334, loss 0.0780788, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:29.555732: step 1335, loss 0.00194881, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:29.794876: step 1336, loss 0.0571256, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:30.059815: step 1337, loss 0.00153561, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:30.298469: step 1338, loss 0.00243819, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:30.558011: step 1339, loss 0.000125963, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:30.823508: step 1340, loss 0.06774, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:30:31.083976: step 1341, loss 0.0900518, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:30:31.346548: step 1342, loss 0.036927, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:30:31.585088: step 1343, loss 0.146623, acc 0.953125, precision 0.947368, recall 0.972973\n",
      "temporal 2020-10-10T22:30:31.793582: step 1344, loss 0.0984639, acc 0.979167, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:30:32.034147: step 1345, loss 0.0369104, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:32.292269: step 1346, loss 0.0461851, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:30:32.533896: step 1347, loss 0.0356144, acc 0.96875, precision 0.974359, recall 0.974359\n",
      "temporal 2020-10-10T22:30:32.799185: step 1348, loss 0.0181993, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:33.037904: step 1349, loss 0.0376661, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:33.283966: step 1350, loss 0.000482788, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:33.528327: step 1351, loss 0.0646378, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:30:33.785970: step 1352, loss 0.220513, acc 0.953125, precision 0.967742, recall 0.9375\n",
      "temporal 2020-10-10T22:30:34.028036: step 1353, loss 0.340925, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:30:34.282978: step 1354, loss 0.0214411, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:34.528554: step 1355, loss 0.398331, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:30:34.770801: step 1356, loss 0.376505, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:30:35.008919: step 1357, loss 0.122063, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:30:35.275391: step 1358, loss 0.0619422, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:30:35.513233: step 1359, loss 0.0151529, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:35.776854: step 1360, loss 0.0242822, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:30:36.019141: step 1361, loss 0.0458258, acc 0.96875, precision 0.928571, recall 1\n",
      "temporal 2020-10-10T22:30:36.259031: step 1362, loss 0.559486, acc 0.953125, precision 0.888889, recall 1\n",
      "temporal 2020-10-10T22:30:36.500498: step 1363, loss 0.0884655, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:30:36.738562: step 1364, loss 0.000224333, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:36.977755: step 1365, loss 0.10844, acc 0.953125, precision 0.967742, recall 0.9375\n",
      "temporal 2020-10-10T22:30:37.218198: step 1366, loss 0.0925905, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:30:37.481926: step 1367, loss 0.107613, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:30:37.691797: step 1368, loss 0.0012872, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:37.931204: step 1369, loss 0.222769, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:30:38.193179: step 1370, loss 0.28342, acc 0.90625, precision 1, recall 0.818182\n",
      "temporal 2020-10-10T22:30:38.457205: step 1371, loss 0.00621678, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:38.717394: step 1372, loss 0.073018, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:30:38.954965: step 1373, loss 0.000418591, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:39.193979: step 1374, loss 0.00168165, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:39.463139: step 1375, loss 0.000154111, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:39.702156: step 1376, loss 0.000447571, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:39.941502: step 1377, loss 0.253651, acc 0.9375, precision 0.894737, recall 1\n",
      "temporal 2020-10-10T22:30:40.182378: step 1378, loss 0.0288107, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:40.439668: step 1379, loss 0.0108507, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:40.680307: step 1380, loss 0.0589341, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:30:40.944641: step 1381, loss 0.019526, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:30:41.202929: step 1382, loss 0.0310234, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:41.444330: step 1383, loss 0.211803, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:30:41.685183: step 1384, loss 0.217015, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:30:41.936473: step 1385, loss 7.5879e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:42.173679: step 1386, loss 0.0528105, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:30:42.441349: step 1387, loss 0.0553431, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:42.683890: step 1388, loss 0.105026, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:30:42.940516: step 1389, loss 0.0300522, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:43.184126: step 1390, loss 0.0346703, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:43.447781: step 1391, loss 0.0414024, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:30:43.654750: step 1392, loss 0.176826, acc 0.979167, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:30:43.893741: step 1393, loss 0.0147693, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:44.136408: step 1394, loss 0.198185, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:30:44.377773: step 1395, loss 0.000140801, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:44.617744: step 1396, loss 0.18812, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:30:44.857628: step 1397, loss 0.00339091, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:45.096953: step 1398, loss 0.0124226, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:45.334002: step 1399, loss 0.00882472, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:45.594454: step 1400, loss 0.00226293, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:30:46.717487: step 1400, loss 3.19818, acc 0.599426, precision 0.138528, recall 0.752941, F1 0.234004\n",
      "Saved model checkpoint to /data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/runs/explicit_1602339881/checkpoints/explicit-model-1400\n",
      "\n",
      "\n",
      "temporal 2020-10-10T22:30:47.155514: step 1401, loss 0.0407928, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:30:47.402921: step 1402, loss 0.165149, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:30:47.642441: step 1403, loss 0.0999639, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:47.880501: step 1404, loss 0.0741395, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:48.122598: step 1405, loss 0.01331, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:30:48.394332: step 1406, loss 0.0171221, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:48.659233: step 1407, loss 0.202219, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:30:48.899179: step 1408, loss 0.00480401, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:49.138888: step 1409, loss 0.100439, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:30:49.377174: step 1410, loss 0.0307506, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:30:49.614602: step 1411, loss 1.11851e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:49.878491: step 1412, loss 0.00117194, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:50.136484: step 1413, loss 0.011066, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:30:50.376026: step 1414, loss 0.023035, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:30:50.617336: step 1415, loss 0.139823, acc 0.96875, precision 0.973684, recall 0.973684\n",
      "temporal 2020-10-10T22:30:50.848128: step 1416, loss 0.193478, acc 0.958333, precision 0.956522, recall 0.956522\n",
      "temporal 2020-10-10T22:30:51.086944: step 1417, loss 0.268001, acc 0.953125, precision 0.969697, recall 0.941176\n",
      "temporal 2020-10-10T22:30:51.328592: step 1418, loss 0.000314965, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:51.588193: step 1419, loss 0.0497887, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:30:51.827582: step 1420, loss 0.0793065, acc 0.9375, precision 0.939394, recall 0.939394\n",
      "temporal 2020-10-10T22:30:52.073991: step 1421, loss 0.0186458, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:30:52.310259: step 1422, loss 0.000170647, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:52.573827: step 1423, loss 0.117491, acc 0.984375, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:30:52.835582: step 1424, loss 0.16082, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:30:53.123533: step 1425, loss 0.00360774, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:53.384788: step 1426, loss 0.0763779, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:30:53.626533: step 1427, loss 0.0270414, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:30:53.889563: step 1428, loss 0.0364328, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:30:54.128871: step 1429, loss 0.126304, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:30:54.364802: step 1430, loss 0.323147, acc 0.953125, precision 1, recall 0.911765\n",
      "temporal 2020-10-10T22:30:54.599077: step 1431, loss 0.00571932, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:54.839260: step 1432, loss 0.145463, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:30:55.132304: step 1433, loss 0.0831323, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:30:55.362195: step 1434, loss 0.378955, acc 0.9375, precision 0.925, recall 0.973684\n",
      "temporal 2020-10-10T22:30:55.594436: step 1435, loss 0.000644268, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:55.824096: step 1436, loss 0.00696119, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:56.054020: step 1437, loss 0.03737, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:30:56.284040: step 1438, loss 0.0427842, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:30:56.510194: step 1439, loss 0.0396115, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:30:56.706520: step 1440, loss 0.000155044, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:56.939440: step 1441, loss 0.000246706, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:57.176634: step 1442, loss 0.000619788, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:57.441583: step 1443, loss 0.00166797, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:57.686754: step 1444, loss 0.000905473, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:57.913675: step 1445, loss 0.0550197, acc 0.96875, precision 0.928571, recall 1\n",
      "temporal 2020-10-10T22:30:58.164588: step 1446, loss 0.0325413, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:30:58.403122: step 1447, loss 0.000283339, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:58.666640: step 1448, loss 0.00103705, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:58.905681: step 1449, loss 0.00388224, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:30:59.143228: step 1450, loss 0.17428, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:30:59.406452: step 1451, loss 0.0297372, acc 0.96875, precision 1, recall 0.92\n",
      "temporal 2020-10-10T22:30:59.642786: step 1452, loss 0.312506, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:30:59.874854: step 1453, loss 0.0034469, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:00.116606: step 1454, loss 0.060329, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:31:00.350717: step 1455, loss 9.62388e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:00.582735: step 1456, loss 0.0252517, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:31:00.815251: step 1457, loss 0.00215691, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:01.048828: step 1458, loss 0.0892754, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:31:01.278078: step 1459, loss 0.174086, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:31:01.504975: step 1460, loss 0.0268379, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:31:01.736866: step 1461, loss 0.000306923, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:01.966407: step 1462, loss 0.199746, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:31:02.218167: step 1463, loss 0.179164, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:31:02.415691: step 1464, loss 0.249795, acc 0.916667, precision 0.966667, recall 0.90625\n",
      "temporal 2020-10-10T22:31:02.660569: step 1465, loss 0.0115156, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:02.889038: step 1466, loss 0.0117124, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:03.112437: step 1467, loss 0.0372048, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:31:03.334424: step 1468, loss 0.00654625, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:03.570495: step 1469, loss 0.0660098, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:31:03.831416: step 1470, loss 0.0238962, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:31:04.102530: step 1471, loss 0.252273, acc 0.9375, precision 0.928571, recall 0.928571\n",
      "temporal 2020-10-10T22:31:04.344965: step 1472, loss 0.277167, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:31:04.583092: step 1473, loss 0.0478392, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:04.843179: step 1474, loss 0.000984728, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:05.084495: step 1475, loss 0.0345745, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:31:05.353025: step 1476, loss 0.0875072, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:05.618034: step 1477, loss 0.0706667, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:31:05.886301: step 1478, loss 0.112249, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:31:06.154608: step 1479, loss 0.0158166, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:06.395927: step 1480, loss 0.0574009, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:31:06.635456: step 1481, loss 0.195087, acc 0.953125, precision 0.933333, recall 0.965517\n",
      "temporal 2020-10-10T22:31:06.874799: step 1482, loss 0.057254, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:31:07.112367: step 1483, loss 0.008945, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:07.350549: step 1484, loss 0.0125296, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:07.590551: step 1485, loss 0.004958, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:07.825471: step 1486, loss 0.00658905, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:08.064322: step 1487, loss 0.000264409, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:08.297204: step 1488, loss 0.0236742, acc 0.979167, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:31:08.537409: step 1489, loss 0.341593, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:31:08.793137: step 1490, loss 0.0149723, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:31:09.029558: step 1491, loss 0.142823, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:31:09.340842: step 1492, loss 0.00932361, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:09.606574: step 1493, loss 0.00297972, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:09.846482: step 1494, loss 0.000120103, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:10.128985: step 1495, loss 0.180002, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:31:10.392615: step 1496, loss 0.0686024, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:31:10.630454: step 1497, loss 0.0574151, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:31:10.870840: step 1498, loss 0.114266, acc 0.953125, precision 0.931035, recall 0.964286\n",
      "temporal 2020-10-10T22:31:11.112230: step 1499, loss 0.0892422, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:31:11.350790: step 1500, loss 0.00376482, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:31:12.507451: step 1500, loss 3.04605, acc 0.614723, precision 0.128505, recall 0.647059, F1 0.214425\n",
      "\n",
      "temporal 2020-10-10T22:31:12.726094: step 1501, loss 0.0112503, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:12.956443: step 1502, loss 0.0685882, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:13.193262: step 1503, loss 0.0253963, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:13.434451: step 1504, loss 0.00975734, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:13.673413: step 1505, loss 0.00671695, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:13.913750: step 1506, loss 0.31466, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:31:14.151358: step 1507, loss 0.231348, acc 0.96875, precision 0.923077, recall 1\n",
      "temporal 2020-10-10T22:31:14.414759: step 1508, loss 0.00533071, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:14.679843: step 1509, loss 0.141967, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:31:14.927720: step 1510, loss 0.0208109, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:15.168900: step 1511, loss 0.10784, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:31:15.377787: step 1512, loss 2.90067e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:15.619015: step 1513, loss 0.0990517, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:15.880023: step 1514, loss 0.151344, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:16.172705: step 1515, loss 0.225546, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:31:16.436473: step 1516, loss 0.000124258, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:16.676715: step 1517, loss 0.000292184, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:16.915193: step 1518, loss 0.147286, acc 0.953125, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:31:17.154649: step 1519, loss 0.0783207, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:31:17.396018: step 1520, loss 0.0555928, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:31:17.634669: step 1521, loss 0.0552859, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:31:17.873829: step 1522, loss 0.000614111, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:18.115238: step 1523, loss 0.0168137, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:31:18.389951: step 1524, loss 0.171701, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:31:18.635109: step 1525, loss 0.0552643, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:31:18.895329: step 1526, loss 0.0028554, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:19.134979: step 1527, loss 6.43029e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:19.398609: step 1528, loss 0.243337, acc 0.953125, precision 1, recall 0.884615\n",
      "temporal 2020-10-10T22:31:19.640608: step 1529, loss 0.0849168, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:19.878775: step 1530, loss 0.00587792, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:20.142914: step 1531, loss 0.121954, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:31:20.419562: step 1532, loss 0.000559923, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:20.708978: step 1533, loss 0.00154652, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:20.950907: step 1534, loss 0.0512331, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:31:21.189228: step 1535, loss 0.00269406, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:21.395367: step 1536, loss 2.10227e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:21.655181: step 1537, loss 6.1222e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:21.920753: step 1538, loss 0.0770432, acc 0.953125, precision 0.9, recall 1\n",
      "temporal 2020-10-10T22:31:22.155605: step 1539, loss 0.000429072, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:22.394793: step 1540, loss 0.080055, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:31:22.635830: step 1541, loss 0.0419557, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:31:22.900142: step 1542, loss 0.000954205, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:23.142139: step 1543, loss 0.000451924, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:23.402035: step 1544, loss 0.0694824, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:31:23.636670: step 1545, loss 0.156456, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:31:23.898820: step 1546, loss 0.106179, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:31:24.163188: step 1547, loss 0.0658487, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:31:24.425439: step 1548, loss 0.116046, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:31:24.662465: step 1549, loss 0.000478962, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:24.902023: step 1550, loss 0.00533922, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:25.158894: step 1551, loss 0.219981, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:31:25.403725: step 1552, loss 0.0105083, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:25.641057: step 1553, loss 0.00174521, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:25.903177: step 1554, loss 0.165617, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:26.163235: step 1555, loss 0.0314279, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:31:26.406583: step 1556, loss 0.49123, acc 0.90625, precision 0.842105, recall 1\n",
      "temporal 2020-10-10T22:31:26.664263: step 1557, loss 0.0124666, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:26.931887: step 1558, loss 0.00344936, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:27.196668: step 1559, loss 0.00663764, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:27.424587: step 1560, loss 0.000123591, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:27.664771: step 1561, loss 0.0863398, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:31:27.905123: step 1562, loss 0.593163, acc 0.953125, precision 1, recall 0.884615\n",
      "temporal 2020-10-10T22:31:28.171974: step 1563, loss 0.113081, acc 0.953125, precision 1, recall 0.918919\n",
      "temporal 2020-10-10T22:31:28.429608: step 1564, loss 0.439529, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:31:28.666652: step 1565, loss 0.0013366, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:28.903529: step 1566, loss 0.000326359, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:29.162467: step 1567, loss 0.0746628, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:31:29.402180: step 1568, loss 0.116096, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:31:29.641373: step 1569, loss 8.24061e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:29.900086: step 1570, loss 0.0178455, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:30.140943: step 1571, loss 0.000397627, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:30.401827: step 1572, loss 0.00156913, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:30.642469: step 1573, loss 0.00388006, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:30.882898: step 1574, loss 0.0052233, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:31.124408: step 1575, loss 0.11654, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:31.366901: step 1576, loss 0.149594, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:31:31.608837: step 1577, loss 0.0310702, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:31:31.868302: step 1578, loss 0.0480828, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:31:32.111500: step 1579, loss 0.00174095, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:32.368203: step 1580, loss 0.0228037, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:32.611214: step 1581, loss 5.60509e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:32.854843: step 1582, loss 0.107003, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:31:33.117879: step 1583, loss 0.0358335, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:31:33.329215: step 1584, loss 0.352854, acc 0.979167, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:31:33.589037: step 1585, loss 0.00258771, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:33.854529: step 1586, loss 0.0228225, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:31:34.115399: step 1587, loss 0.181715, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:31:34.373891: step 1588, loss 0.0591004, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:31:34.613655: step 1589, loss 0.0415914, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:31:34.852641: step 1590, loss 0.00013119, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:35.090829: step 1591, loss 0.0552974, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:31:35.354800: step 1592, loss 9.44938e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:35.619489: step 1593, loss 0.0439682, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:31:35.860016: step 1594, loss 0.00109002, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:36.096160: step 1595, loss 0.00900288, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:36.356816: step 1596, loss 0.0519774, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:31:36.620458: step 1597, loss 0.000336358, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:36.879561: step 1598, loss 0.203876, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:31:37.117909: step 1599, loss 0.102557, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:31:37.356608: step 1600, loss 0.0407973, acc 0.984375, precision 0.973684, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:31:38.496391: step 1600, loss 3.58632, acc 0.590822, precision 0.134328, recall 0.741176, F1 0.227437\n",
      "\n",
      "temporal 2020-10-10T22:31:38.702814: step 1601, loss 0.141434, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:31:38.941160: step 1602, loss 0.105454, acc 0.96875, precision 0.948718, recall 1\n",
      "temporal 2020-10-10T22:31:39.203863: step 1603, loss 0.0282609, acc 0.984375, precision 0.956522, recall 1\n",
      "temporal 2020-10-10T22:31:39.439865: step 1604, loss 0.0154761, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:31:39.681757: step 1605, loss 0.0235863, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:31:39.922054: step 1606, loss 0.512208, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:31:40.158543: step 1607, loss 0.0296872, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:31:40.365346: step 1608, loss 0.122162, acc 0.979167, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:31:40.616252: step 1609, loss 0.0425141, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:31:40.857432: step 1610, loss 0.149415, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:31:41.121012: step 1611, loss 0.000737128, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:41.362083: step 1612, loss 0.267485, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:31:41.601378: step 1613, loss 0.0414211, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:31:41.838924: step 1614, loss 0.000258762, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:42.076827: step 1615, loss 0.000849965, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:42.315996: step 1616, loss 0.0125191, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:31:42.555599: step 1617, loss 0.0282472, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:31:42.793114: step 1618, loss 0.0618258, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:31:43.056438: step 1619, loss 0.0209221, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:43.315092: step 1620, loss 0.000552092, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:43.581263: step 1621, loss 0.152842, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:31:43.842684: step 1622, loss 0.00506977, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:44.086476: step 1623, loss 0.00850328, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:44.324639: step 1624, loss 0.00655342, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:44.575402: step 1625, loss 0.115741, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:31:44.823654: step 1626, loss 0.0158054, acc 0.984375, precision 0.956522, recall 1\n",
      "temporal 2020-10-10T22:31:45.065096: step 1627, loss 0.0160791, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:31:45.301585: step 1628, loss 0.00010679, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:45.538471: step 1629, loss 0.0629423, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:31:45.777384: step 1630, loss 0.000161113, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:46.017366: step 1631, loss 0.0644932, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:31:46.223212: step 1632, loss 1.08556e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:46.480798: step 1633, loss 0.00573047, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:46.741600: step 1634, loss 0.104324, acc 0.96875, precision 1, recall 0.948718\n",
      "temporal 2020-10-10T22:31:46.979865: step 1635, loss 0.242865, acc 0.9375, precision 1, recall 0.902439\n",
      "temporal 2020-10-10T22:31:47.243371: step 1636, loss 0.0437412, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:31:47.484127: step 1637, loss 0.00134657, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:47.725152: step 1638, loss 0.125769, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:31:47.961858: step 1639, loss 0.00742545, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:48.224717: step 1640, loss 0.0502897, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:31:48.465681: step 1641, loss 0.00551377, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:48.705015: step 1642, loss 0.0273981, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:31:48.946880: step 1643, loss 0.0612174, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:31:49.185269: step 1644, loss 0.273501, acc 0.953125, precision 0.921053, recall 1\n",
      "temporal 2020-10-10T22:31:49.425997: step 1645, loss 0.256017, acc 0.953125, precision 0.909091, recall 1\n",
      "temporal 2020-10-10T22:31:49.669921: step 1646, loss 0.0484435, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:31:49.907829: step 1647, loss 0.405644, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:31:50.147379: step 1648, loss 0.0375129, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:31:50.410399: step 1649, loss 0.00166152, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:50.648642: step 1650, loss 0.000215961, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:50.897892: step 1651, loss 0.173461, acc 0.953125, precision 1, recall 0.888889\n",
      "temporal 2020-10-10T22:31:51.139453: step 1652, loss 0.169492, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:31:51.377083: step 1653, loss 0.151251, acc 0.96875, precision 1, recall 0.928571\n",
      "temporal 2020-10-10T22:31:51.640343: step 1654, loss 0.565694, acc 0.953125, precision 1, recall 0.88\n",
      "temporal 2020-10-10T22:31:51.901982: step 1655, loss 0.00264274, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:52.110671: step 1656, loss 0.0128438, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:52.347626: step 1657, loss 0.320324, acc 0.9375, precision 1, recall 0.885714\n",
      "temporal 2020-10-10T22:31:52.621244: step 1658, loss 0.0048491, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:52.858511: step 1659, loss 0.0420556, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:31:53.097778: step 1660, loss 0.205567, acc 0.96875, precision 0.96, recall 0.96\n",
      "temporal 2020-10-10T22:31:53.341202: step 1661, loss 0.0131235, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:53.634859: step 1662, loss 0.140289, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:31:53.874468: step 1663, loss 0.00270095, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:54.117543: step 1664, loss 0.085049, acc 0.953125, precision 0.921053, recall 1\n",
      "temporal 2020-10-10T22:31:54.373618: step 1665, loss 0.255138, acc 0.953125, precision 0.923077, recall 1\n",
      "temporal 2020-10-10T22:31:54.637374: step 1666, loss 0.00703384, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:54.903189: step 1667, loss 0.307091, acc 0.9375, precision 0.882353, recall 1\n",
      "temporal 2020-10-10T22:31:55.166289: step 1668, loss 4.9748e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:55.423013: step 1669, loss 0.0061639, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:55.683209: step 1670, loss 0.0994318, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:31:55.928577: step 1671, loss 0.0968675, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:31:56.188907: step 1672, loss 0.468463, acc 0.953125, precision 1, recall 0.914286\n",
      "temporal 2020-10-10T22:31:56.452120: step 1673, loss 0.0719924, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:31:56.691019: step 1674, loss 0.187853, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:31:56.951832: step 1675, loss 0.0306996, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:31:57.190314: step 1676, loss 1.74673e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:57.427161: step 1677, loss 0.00238378, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:57.666657: step 1678, loss 0.00302021, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:57.905115: step 1679, loss 0.0934462, acc 0.953125, precision 0.925, recall 1\n",
      "temporal 2020-10-10T22:31:58.112211: step 1680, loss 0.00214023, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:58.352315: step 1681, loss 0.00200199, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:58.596498: step 1682, loss 0.00207645, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:58.835247: step 1683, loss 0.0114667, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:59.073034: step 1684, loss 0.0191769, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:31:59.309976: step 1685, loss 0.0196569, acc 0.984375, precision 0.97619, recall 1\n",
      "temporal 2020-10-10T22:31:59.561025: step 1686, loss 0.00122887, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:31:59.817821: step 1687, loss 0.000151553, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:00.083630: step 1688, loss 0.175013, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:32:00.321204: step 1689, loss 0.29493, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:00.557588: step 1690, loss 0.00018057, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:00.799131: step 1691, loss 0.000365611, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:01.063754: step 1692, loss 0.0384991, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:32:01.318624: step 1693, loss 0.0394273, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:01.560309: step 1694, loss 0.000130154, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:01.818004: step 1695, loss 0.0572149, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:32:02.106943: step 1696, loss 0.136351, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:32:02.341159: step 1697, loss 0.224313, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:32:02.597988: step 1698, loss 0.000348537, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:02.865411: step 1699, loss 2.96115e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:03.104913: step 1700, loss 8.67296e-06, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:32:04.224800: step 1700, loss 2.5934, acc 0.67304, precision 0.135977, recall 0.564706, F1 0.219178\n",
      "\n",
      "temporal 2020-10-10T22:32:04.466003: step 1701, loss 0.0201153, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:32:04.688488: step 1702, loss 0.000194583, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:04.960745: step 1703, loss 0.0382488, acc 0.96875, precision 0.939394, recall 1\n",
      "temporal 2020-10-10T22:32:05.190854: step 1704, loss 0.0412229, acc 0.979167, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:32:05.426597: step 1705, loss 0.134188, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:32:05.662075: step 1706, loss 0.101239, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:32:05.926214: step 1707, loss 0.212624, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:32:06.168128: step 1708, loss 0.0465626, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:06.408117: step 1709, loss 6.42638e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:06.651828: step 1710, loss 3.10812e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:06.894812: step 1711, loss 0.218094, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:32:07.132856: step 1712, loss 0.00194907, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:07.389994: step 1713, loss 0.00510827, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:07.631127: step 1714, loss 0.0587115, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:32:07.892710: step 1715, loss 0.0044243, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:08.129567: step 1716, loss 0.0402712, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:08.368372: step 1717, loss 0.0368805, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:32:08.630937: step 1718, loss 3.17889e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:08.874008: step 1719, loss 0.0853665, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:32:09.134299: step 1720, loss 0.128364, acc 0.953125, precision 0.961538, recall 0.925926\n",
      "temporal 2020-10-10T22:32:09.371043: step 1721, loss 0.0179987, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:09.614323: step 1722, loss 0.149517, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:32:09.855415: step 1723, loss 0.00175799, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:10.092795: step 1724, loss 1.27478e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:10.332770: step 1725, loss 0.0175506, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:32:10.569978: step 1726, loss 8.67981e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:10.831364: step 1727, loss 0.000135766, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:11.036900: step 1728, loss 3.86574e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:11.289898: step 1729, loss 0.00022071, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:11.530340: step 1730, loss 0.175709, acc 0.96875, precision 0.962963, recall 0.962963\n",
      "temporal 2020-10-10T22:32:11.770740: step 1731, loss 0.221808, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:32:12.009222: step 1732, loss 0.0176164, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:12.270365: step 1733, loss 0.0138129, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:32:12.528214: step 1734, loss 0.0831293, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:32:12.763608: step 1735, loss 0.00173986, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:13.022261: step 1736, loss 0.0676428, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:32:13.260459: step 1737, loss 0.102678, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:32:13.524346: step 1738, loss 0.000286562, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:13.789087: step 1739, loss 0.188688, acc 0.96875, precision 0.969697, recall 0.969697\n",
      "temporal 2020-10-10T22:32:14.031047: step 1740, loss 0.0203972, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:14.275079: step 1741, loss 0.0799924, acc 0.96875, precision 0.947368, recall 1\n",
      "temporal 2020-10-10T22:32:14.535082: step 1742, loss 0.175813, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:32:14.799421: step 1743, loss 0.00024911, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:15.051957: step 1744, loss 0.199549, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:32:15.292292: step 1745, loss 0.156627, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:32:15.554799: step 1746, loss 0.00710899, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:15.790679: step 1747, loss 0.00709411, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:16.040533: step 1748, loss 0.00032302, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:16.281185: step 1749, loss 0.0332684, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:32:16.520246: step 1750, loss 0.00275, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:16.781322: step 1751, loss 0.380346, acc 0.9375, precision 1, recall 0.875\n",
      "temporal 2020-10-10T22:32:16.991401: step 1752, loss 0.0746891, acc 0.979167, precision 1, recall 0.954545\n",
      "temporal 2020-10-10T22:32:17.248482: step 1753, loss 0.209717, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:32:17.502803: step 1754, loss 0.000344373, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:17.742297: step 1755, loss 0.000168471, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:18.003599: step 1756, loss 0.000801581, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:18.287412: step 1757, loss 0.255278, acc 0.9375, precision 0.888889, recall 1\n",
      "temporal 2020-10-10T22:32:18.529363: step 1758, loss 0.132448, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:32:18.769146: step 1759, loss 0.171126, acc 0.953125, precision 0.903226, recall 1\n",
      "temporal 2020-10-10T22:32:19.007080: step 1760, loss 0.000687168, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:19.265384: step 1761, loss 0.00113721, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:19.506136: step 1762, loss 0.00679292, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:19.743305: step 1763, loss 0.0163673, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:19.980766: step 1764, loss 0.0317344, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:32:20.220748: step 1765, loss 0.0283998, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:32:20.469727: step 1766, loss 0.205452, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:32:20.709007: step 1767, loss 0.0151814, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:32:20.963650: step 1768, loss 0.0516394, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:32:21.224586: step 1769, loss 0.0234975, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:21.461376: step 1770, loss 0.164182, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:32:21.700902: step 1771, loss 0.0869918, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:21.939607: step 1772, loss 0.00131983, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:22.174570: step 1773, loss 0.0006493, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:22.420957: step 1774, loss 0.0141753, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:32:22.661295: step 1775, loss 0.0054745, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:22.866563: step 1776, loss 6.58826e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:23.104478: step 1777, loss 0.000227985, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:23.401183: step 1778, loss 0.12462, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:32:23.642311: step 1779, loss 0.176973, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:32:23.930045: step 1780, loss 0.0208649, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:32:24.167210: step 1781, loss 0.00259917, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:24.409024: step 1782, loss 0.000343746, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:24.646847: step 1783, loss 0.000241549, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:24.883859: step 1784, loss 0.000325308, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:25.147927: step 1785, loss 0.0161959, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:25.387452: step 1786, loss 0.0167395, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:32:25.627439: step 1787, loss 0.0187996, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:32:25.894458: step 1788, loss 8.75719e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:26.154640: step 1789, loss 0.000284018, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:26.394525: step 1790, loss 0.131179, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:32:26.639309: step 1791, loss 0.038133, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:26.877823: step 1792, loss 0.0608491, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:32:27.131334: step 1793, loss 0.0732298, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:32:27.372732: step 1794, loss 0.0934466, acc 0.96875, precision 1, recall 0.913043\n",
      "temporal 2020-10-10T22:32:27.612043: step 1795, loss 0.000686736, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:27.849280: step 1796, loss 0.0105981, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:28.085395: step 1797, loss 0.0239964, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:32:28.325945: step 1798, loss 0.0728997, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:28.587276: step 1799, loss 0.0954253, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:32:28.798826: step 1800, loss 0.213485, acc 0.979167, precision 0.962963, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:32:30.047048: step 1800, loss 3.50704, acc 0.622371, precision 0.139535, recall 0.705882, F1 0.23301\n",
      "\n",
      "temporal 2020-10-10T22:32:30.267137: step 1801, loss 0.091257, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:32:30.521351: step 1802, loss 0.00609638, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:30.765369: step 1803, loss 0.0280776, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:32:31.027481: step 1804, loss 0.0503129, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:32:31.269590: step 1805, loss 0.0775819, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:32:31.506470: step 1806, loss 0.000208633, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:31.744027: step 1807, loss 0.0316289, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:32:31.982484: step 1808, loss 2.27972e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:32.215124: step 1809, loss 2.27789e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:32.456669: step 1810, loss 0.241625, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:32:32.716463: step 1811, loss 2.76031e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:32.957450: step 1812, loss 0.000819359, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:33.218646: step 1813, loss 0.000801603, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:33.475438: step 1814, loss 0.00083095, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:33.738547: step 1815, loss 0.00181915, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:33.978213: step 1816, loss 0.034321, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:32:34.218722: step 1817, loss 0.0232453, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:34.459344: step 1818, loss 0.0245788, acc 0.984375, precision 1, recall 0.97619\n",
      "temporal 2020-10-10T22:32:34.720408: step 1819, loss 0.0159392, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:34.981517: step 1820, loss 0.135226, acc 0.953125, precision 0.888889, recall 1\n",
      "temporal 2020-10-10T22:32:35.244040: step 1821, loss 0.0200105, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:32:35.512113: step 1822, loss 1.97995e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:35.751722: step 1823, loss 0.00010344, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:35.960625: step 1824, loss 0.318529, acc 0.958333, precision 0.958333, recall 0.958333\n",
      "temporal 2020-10-10T22:32:36.197039: step 1825, loss 0.0767071, acc 0.96875, precision 1, recall 0.92\n",
      "temporal 2020-10-10T22:32:36.460358: step 1826, loss 0.0524334, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:32:36.703196: step 1827, loss 0.000487132, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:36.941560: step 1828, loss 0.274173, acc 0.9375, precision 0.95, recall 0.95\n",
      "temporal 2020-10-10T22:32:37.207519: step 1829, loss 0.0639996, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:32:37.490638: step 1830, loss 0.208947, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:32:37.739259: step 1831, loss 0.00636277, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:37.980547: step 1832, loss 0.000320194, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:38.241871: step 1833, loss 3.24296e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:38.480826: step 1834, loss 0.097958, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:38.739884: step 1835, loss 0.0664404, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:32:39.002865: step 1836, loss 0.0400612, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:32:39.262947: step 1837, loss 0.0645339, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:39.505887: step 1838, loss 0.0858613, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:32:39.741422: step 1839, loss 0.000136387, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:40.002793: step 1840, loss 0.0140388, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:40.239438: step 1841, loss 0.212809, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:32:40.498524: step 1842, loss 0.00583986, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:40.736321: step 1843, loss 0.0720031, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:32:40.975625: step 1844, loss 0.00254553, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:41.235852: step 1845, loss 0.00111052, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:41.481423: step 1846, loss 3.81637e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:41.721420: step 1847, loss 0.0128217, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:41.929841: step 1848, loss 0.000912351, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:42.167945: step 1849, loss 0.223172, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:32:42.410291: step 1850, loss 0.00151772, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:42.649391: step 1851, loss 0.140875, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:32:42.884642: step 1852, loss 0.0152857, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:43.119763: step 1853, loss 0.00330594, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:43.381507: step 1854, loss 2.57016e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:43.619446: step 1855, loss 1.59938e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:43.879167: step 1856, loss 0.0101415, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:44.119551: step 1857, loss 0.00224025, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:44.363647: step 1858, loss 3.09289e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:44.616931: step 1859, loss 0.0409127, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:32:44.879757: step 1860, loss 0.0181127, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:32:45.142438: step 1861, loss 0.0448895, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:45.404502: step 1862, loss 0.0393436, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:32:45.646226: step 1863, loss 9.86483e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:45.885277: step 1864, loss 0.00125231, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:46.119399: step 1865, loss 0.0890068, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:32:46.357299: step 1866, loss 7.50945e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:46.618450: step 1867, loss 0.00066717, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:46.862447: step 1868, loss 0.113201, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:47.102013: step 1869, loss 4.74078e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:47.339549: step 1870, loss 0.000101104, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:47.600999: step 1871, loss 0.000278256, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:47.813356: step 1872, loss 0.117093, acc 0.979167, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:32:48.050512: step 1873, loss 0.026385, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:32:48.290430: step 1874, loss 0.00107655, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:48.553955: step 1875, loss 0.000338943, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:48.795007: step 1876, loss 0.000384624, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:49.063715: step 1877, loss 0.0290157, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:32:49.325777: step 1878, loss 0.034779, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:49.565026: step 1879, loss 0.201457, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:32:49.827116: step 1880, loss 0.00047925, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:50.064110: step 1881, loss 1.81603e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:50.325890: step 1882, loss 0.00338192, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:50.575431: step 1883, loss 0.00172357, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:50.816218: step 1884, loss 0.000441668, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:51.055897: step 1885, loss 0.00139999, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:51.293693: step 1886, loss 0.025966, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:32:51.529405: step 1887, loss 0.00038845, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:51.766934: step 1888, loss 0.0232443, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:32:52.003789: step 1889, loss 0.0266816, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:52.243253: step 1890, loss 0.0624729, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:52.499786: step 1891, loss 0.000316709, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:52.738491: step 1892, loss 0.0979785, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:32:53.002305: step 1893, loss 0.00447545, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:53.245173: step 1894, loss 0.120511, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:32:53.510356: step 1895, loss 0.0973268, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:32:53.724659: step 1896, loss 0.000872326, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:53.966021: step 1897, loss 0.007564, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:54.219468: step 1898, loss 2.9801e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:54.459894: step 1899, loss 0.0116339, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:54.726670: step 1900, loss 0.0456555, acc 0.984375, precision 1, recall 0.972222\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:32:55.879492: step 1900, loss 2.73021, acc 0.679732, precision 0.127976, recall 0.505882, F1 0.204276\n",
      "\n",
      "temporal 2020-10-10T22:32:56.105183: step 1901, loss 0.000810517, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:56.360042: step 1902, loss 0.0879175, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:32:56.596859: step 1903, loss 0.000695496, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:56.860844: step 1904, loss 0.00133389, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:57.104584: step 1905, loss 0.145054, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:32:57.358498: step 1906, loss 2.38085e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:57.618603: step 1907, loss 2.37479e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:57.869292: step 1908, loss 0.0096955, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:58.108465: step 1909, loss 0.0252635, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:58.348364: step 1910, loss 0.0258116, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:32:58.610919: step 1911, loss 0.0585307, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:32:58.849058: step 1912, loss 7.07804e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:59.089342: step 1913, loss 0.000795945, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:59.329658: step 1914, loss 6.37527e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:32:59.568953: step 1915, loss 0.197363, acc 0.953125, precision 0.966667, recall 0.935484\n",
      "temporal 2020-10-10T22:32:59.831691: step 1916, loss 3.2093e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:00.095772: step 1917, loss 3.37626e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:00.335919: step 1918, loss 0.168643, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:33:00.570162: step 1919, loss 0.0014496, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:00.777848: step 1920, loss 0.101797, acc 0.979167, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:01.018716: step 1921, loss 0.262798, acc 0.96875, precision 0.952381, recall 1\n",
      "temporal 2020-10-10T22:33:01.259581: step 1922, loss 0.00623023, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:01.499405: step 1923, loss 0.0302096, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:33:01.738550: step 1924, loss 0.00329809, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:01.977984: step 1925, loss 0.00510175, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:02.218932: step 1926, loss 0.0229697, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:33:02.463380: step 1927, loss 0.0141322, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:33:02.702387: step 1928, loss 0.296159, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:33:02.963821: step 1929, loss 0.000590557, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:03.258206: step 1930, loss 0.000638583, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:03.535595: step 1931, loss 0.146145, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:33:03.800351: step 1932, loss 0.386572, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:33:04.037814: step 1933, loss 0.00125224, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:04.279878: step 1934, loss 0.0296793, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:33:04.518492: step 1935, loss 1.40996e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:04.753465: step 1936, loss 0.000436559, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:05.039928: step 1937, loss 0.00936405, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:05.284426: step 1938, loss 0.1021, acc 0.953125, precision 0.90625, recall 1\n",
      "temporal 2020-10-10T22:33:05.523353: step 1939, loss 2.98197e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:05.759796: step 1940, loss 0.0992417, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:33:05.997258: step 1941, loss 0.0375115, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:33:06.259368: step 1942, loss 0.0176319, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:06.502517: step 1943, loss 0.198304, acc 0.9375, precision 0.931035, recall 0.931035\n",
      "temporal 2020-10-10T22:33:06.713863: step 1944, loss 0.122784, acc 0.979167, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:33:06.953977: step 1945, loss 0.101152, acc 0.96875, precision 1, recall 0.944444\n",
      "temporal 2020-10-10T22:33:07.198261: step 1946, loss 0.00402999, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:07.438710: step 1947, loss 0.000373205, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:07.700415: step 1948, loss 0.000640714, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:07.961470: step 1949, loss 0.00074816, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:08.201589: step 1950, loss 0.0246408, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:33:08.445031: step 1951, loss 0.00492768, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:08.719318: step 1952, loss 0.057864, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:33:08.957774: step 1953, loss 2.78704e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:09.214475: step 1954, loss 0.000391893, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:09.452331: step 1955, loss 3.83246e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:09.692412: step 1956, loss 0.000264875, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:09.928523: step 1957, loss 0.153046, acc 0.953125, precision 0.916667, recall 1\n",
      "temporal 2020-10-10T22:33:10.164699: step 1958, loss 3.03576e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:10.402399: step 1959, loss 0.0735581, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:10.659798: step 1960, loss 0.00102532, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:10.897443: step 1961, loss 0.000413823, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:11.135250: step 1962, loss 0.0870033, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:33:11.392226: step 1963, loss 0.0346694, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:33:11.650912: step 1964, loss 0.209935, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:33:11.892356: step 1965, loss 0.038332, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:33:12.148777: step 1966, loss 0.107795, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:33:12.387610: step 1967, loss 0.00208147, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:12.599827: step 1968, loss 3.9449e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:12.838616: step 1969, loss 0.0897571, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:33:13.076077: step 1970, loss 4.81834e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:13.315456: step 1971, loss 0.000880617, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:13.557955: step 1972, loss 0.0267426, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:13.806082: step 1973, loss 0.00611361, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:14.065155: step 1974, loss 0.000149424, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:14.303588: step 1975, loss 2.63354e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:14.561779: step 1976, loss 0.0623001, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:33:14.824859: step 1977, loss 0.14827, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:33:15.064437: step 1978, loss 0.15774, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:15.305377: step 1979, loss 0.188505, acc 0.953125, precision 0.964286, recall 0.931035\n",
      "temporal 2020-10-10T22:33:15.542127: step 1980, loss 0.125742, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:33:15.804754: step 1981, loss 0.0564488, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:33:16.099741: step 1982, loss 0.000422208, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:16.338341: step 1983, loss 0.028826, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:33:16.573214: step 1984, loss 0.00483262, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:16.813079: step 1985, loss 0.000127849, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:17.050803: step 1986, loss 1.05236e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:17.335244: step 1987, loss 0.0387623, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:33:17.594145: step 1988, loss 0.000433575, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:17.883988: step 1989, loss 0.192942, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:33:18.122485: step 1990, loss 4.27047e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:18.359841: step 1991, loss 0.090874, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:33:18.569722: step 1992, loss 0.00752161, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:18.808430: step 1993, loss 0.080741, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:19.046939: step 1994, loss 5.4755e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:19.283308: step 1995, loss 0.212023, acc 0.96875, precision 0.95, recall 1\n",
      "temporal 2020-10-10T22:33:19.519137: step 1996, loss 4.10476e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:19.755277: step 1997, loss 8.57166e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:19.993328: step 1998, loss 0.0149948, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:33:20.233310: step 1999, loss 0.243703, acc 0.953125, precision 0.931035, recall 0.964286\n",
      "temporal 2020-10-10T22:33:20.471181: step 2000, loss 0.000927163, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:33:21.611602: step 2000, loss 5.44827, acc 0.530593, precision 0.126838, recall 0.811765, F1 0.219396\n",
      "\n",
      "temporal 2020-10-10T22:33:21.843237: step 2001, loss 0.164542, acc 0.953125, precision 0.923077, recall 1\n",
      "temporal 2020-10-10T22:33:22.085847: step 2002, loss 0.0489679, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:33:22.349280: step 2003, loss 0.00536155, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:22.610143: step 2004, loss 0.094602, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:22.852685: step 2005, loss 0.00107224, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:23.093644: step 2006, loss 0.000975573, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:23.331342: step 2007, loss 0.000977576, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:23.574616: step 2008, loss 0.0165628, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:23.832755: step 2009, loss 0.00380894, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:24.078839: step 2010, loss 0.126538, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:33:24.316410: step 2011, loss 0.156186, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:33:24.553848: step 2012, loss 0.0307225, acc 0.96875, precision 1, recall 0.925926\n",
      "temporal 2020-10-10T22:33:24.791952: step 2013, loss 0.00083859, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:25.056235: step 2014, loss 0.0600316, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:33:25.331468: step 2015, loss 0.000753063, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:25.563705: step 2016, loss 0.204776, acc 0.9375, precision 0.888889, recall 1\n",
      "temporal 2020-10-10T22:33:25.805137: step 2017, loss 0.233554, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:33:26.042755: step 2018, loss 0.00470934, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:26.280319: step 2019, loss 0.107048, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:33:26.542604: step 2020, loss 8.35285e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:26.779036: step 2021, loss 0.0214854, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:33:27.019476: step 2022, loss 0.00161439, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:27.278605: step 2023, loss 0.0267261, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:33:27.517738: step 2024, loss 0.00730484, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:27.759014: step 2025, loss 0.0223147, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:33:28.014615: step 2026, loss 0.00513204, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:28.254263: step 2027, loss 1.09994e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:28.512914: step 2028, loss 1.65062e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:28.749480: step 2029, loss 6.06061e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:28.987589: step 2030, loss 0.00092435, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:29.224391: step 2031, loss 0.000166805, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:29.486409: step 2032, loss 0.000175052, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:29.746560: step 2033, loss 0.243245, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:30.008413: step 2034, loss 3.73734e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:30.270030: step 2035, loss 0.0462001, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:33:30.527202: step 2036, loss 0.000409058, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:30.768967: step 2037, loss 0.000677386, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:31.008783: step 2038, loss 0.184917, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:33:31.265561: step 2039, loss 0.0160511, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:33:31.478870: step 2040, loss 0.0788717, acc 0.979167, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:33:31.717279: step 2041, loss 0.0254942, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:33:31.954859: step 2042, loss 0.0980403, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:33:32.195158: step 2043, loss 0.0156271, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:32.432107: step 2044, loss 1.1511e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:32.668869: step 2045, loss 0.000165145, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:32.910570: step 2046, loss 0.0254082, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:33:33.147313: step 2047, loss 0.000226352, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:33.383042: step 2048, loss 0.103872, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:33:33.627399: step 2049, loss 0.0207778, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:33:33.875634: step 2050, loss 0.0749092, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:33:34.128670: step 2051, loss 0.000210257, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:34.367778: step 2052, loss 0.12432, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:34.602426: step 2053, loss 0.0520501, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:33:34.866426: step 2054, loss 7.12122e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:35.106505: step 2055, loss 0.114233, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:33:35.345396: step 2056, loss 4.91735e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:35.581527: step 2057, loss 6.81742e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:35.817934: step 2058, loss 5.65412e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:36.086974: step 2059, loss 0.000341601, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:36.326853: step 2060, loss 0.173112, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:33:36.564353: step 2061, loss 1.55154e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:36.826217: step 2062, loss 0.0284747, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:33:37.069601: step 2063, loss 0.0884839, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:33:37.300592: step 2064, loss 4.96705e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:37.560896: step 2065, loss 4.1836e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:37.801372: step 2066, loss 0.154056, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:33:38.037752: step 2067, loss 0.0174635, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:38.298247: step 2068, loss 0.00408227, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:38.540023: step 2069, loss 0.0199758, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:33:38.779180: step 2070, loss 3.82187e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:39.040444: step 2071, loss 0.00140137, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:39.308314: step 2072, loss 1.70789e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:39.547829: step 2073, loss 0.105141, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:33:39.784448: step 2074, loss 6.27268e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:40.045296: step 2075, loss 0.00089865, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:40.285554: step 2076, loss 0.0172643, acc 0.984375, precision 1, recall 0.976744\n",
      "temporal 2020-10-10T22:33:40.538039: step 2077, loss 0.0237444, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:33:40.777836: step 2078, loss 0.000298953, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:41.033327: step 2079, loss 0.000657002, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:41.341267: step 2080, loss 6.95681e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:41.578508: step 2081, loss 5.92316e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:41.816227: step 2082, loss 3.15151e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:42.074739: step 2083, loss 0.247392, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:33:42.335084: step 2084, loss 0.00128255, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:42.575040: step 2085, loss 0.000148628, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:42.866146: step 2086, loss 0.00012492, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:43.107185: step 2087, loss 2.18669e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:43.318768: step 2088, loss 0.0199674, acc 0.979167, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:33:43.552979: step 2089, loss 0.120313, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:33:43.832638: step 2090, loss 0.0373037, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:33:44.077704: step 2091, loss 0.00603963, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:44.314318: step 2092, loss 0.000816252, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:44.550722: step 2093, loss 0.000190911, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:44.823238: step 2094, loss 0.0996877, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:33:45.066208: step 2095, loss 0.0467952, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:33:45.327351: step 2096, loss 0.000248884, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:45.566425: step 2097, loss 0.212142, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:33:45.818567: step 2098, loss 0.000104539, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:46.056620: step 2099, loss 0.000413177, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:46.295823: step 2100, loss 0.013379, acc 0.984375, precision 1, recall 0.970588\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:33:47.451235: step 2100, loss 4.05901, acc 0.615679, precision 0.133949, recall 0.682353, F1 0.223938\n",
      "\n",
      "temporal 2020-10-10T22:33:47.673594: step 2101, loss 0.0200966, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:47.934041: step 2102, loss 0.00105283, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:48.175685: step 2103, loss 0.0887797, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:33:48.414408: step 2104, loss 0.119914, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:33:48.653556: step 2105, loss 0.119219, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:33:48.889027: step 2106, loss 0.00037661, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:49.124611: step 2107, loss 7.88184e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:49.379477: step 2108, loss 0.122641, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:33:49.616007: step 2109, loss 0.00556082, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:49.864755: step 2110, loss 0.000103578, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:50.118945: step 2111, loss 0.169244, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:33:50.330281: step 2112, loss 0.00038016, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:50.565652: step 2113, loss 0.000311134, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:50.822699: step 2114, loss 0.0298968, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:51.065724: step 2115, loss 0.00731302, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:51.303024: step 2116, loss 0.00356833, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:51.541979: step 2117, loss 0.0158553, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:51.783269: step 2118, loss 7.45841e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:52.023585: step 2119, loss 0.101766, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:33:52.261923: step 2120, loss 7.19751e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:52.504331: step 2121, loss 0.00557461, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:52.743607: step 2122, loss 4.3094e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:52.999978: step 2123, loss 0.000176179, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:53.264627: step 2124, loss 0.17967, acc 0.96875, precision 0.948718, recall 1\n",
      "temporal 2020-10-10T22:33:53.501861: step 2125, loss 0.0920611, acc 0.984375, precision 0.96, recall 1\n",
      "temporal 2020-10-10T22:33:53.739759: step 2126, loss 0.000700076, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:53.979921: step 2127, loss 2.78961e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:54.217437: step 2128, loss 0.00023191, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:54.468239: step 2129, loss 0.190406, acc 0.96875, precision 1, recall 0.942857\n",
      "temporal 2020-10-10T22:33:54.704607: step 2130, loss 1.12808e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:54.950830: step 2131, loss 1.64092e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:55.200213: step 2132, loss 0.0257609, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:33:55.438071: step 2133, loss 0.00142044, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:55.699634: step 2134, loss 0.000255862, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:55.937476: step 2135, loss 1.58318e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:56.152969: step 2136, loss 1.52982e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:56.391871: step 2137, loss 0.00133542, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:56.652386: step 2138, loss 0.00156422, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:56.892531: step 2139, loss 9.73462e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:57.158465: step 2140, loss 0.0878327, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:57.394229: step 2141, loss 0.00191391, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:57.655508: step 2142, loss 0.00128824, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:57.893702: step 2143, loss 0.000240396, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:58.132122: step 2144, loss 0.000519976, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:58.369051: step 2145, loss 3.05473e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:58.629188: step 2146, loss 0.103238, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:33:58.892240: step 2147, loss 0.000234305, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:59.134488: step 2148, loss 0.117842, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:33:59.374099: step 2149, loss 0.00422178, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:59.610117: step 2150, loss 9.11541e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:33:59.859837: step 2151, loss 6.06674e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:00.097925: step 2152, loss 0.000242574, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:00.354558: step 2153, loss 5.53688e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:00.594866: step 2154, loss 0.0602688, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:34:00.848040: step 2155, loss 5.89219e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:01.085199: step 2156, loss 2.44733e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:01.338795: step 2157, loss 0.000790527, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:01.603933: step 2158, loss 2.64395e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:01.845775: step 2159, loss 1.3206e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:02.052791: step 2160, loss 0.00139236, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:02.287582: step 2161, loss 0.000610854, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:02.547524: step 2162, loss 0.0649203, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:34:02.788597: step 2163, loss 2.8198e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:03.051498: step 2164, loss 4.38407e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:03.292537: step 2165, loss 0.0368655, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:34:03.528610: step 2166, loss 7.01353e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:03.787607: step 2167, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:04.051728: step 2168, loss 0.00594239, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:04.313559: step 2169, loss 0.0159736, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:34:04.574892: step 2170, loss 3.35904e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:04.836174: step 2171, loss 1.86264e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:05.079776: step 2172, loss 0.000814732, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:05.336133: step 2173, loss 5.14842e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:05.577054: step 2174, loss 0.00044913, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:05.819080: step 2175, loss 0.0111964, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:06.081238: step 2176, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:06.340335: step 2177, loss 6.0163e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:06.581368: step 2178, loss 0.0216994, acc 0.984375, precision 1, recall 0.97619\n",
      "temporal 2020-10-10T22:34:06.842555: step 2179, loss 0.00137983, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:07.103116: step 2180, loss 8.24607e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:07.362790: step 2181, loss 0.000500303, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:07.600092: step 2182, loss 0.00067265, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:07.856084: step 2183, loss 0.105912, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:34:08.068436: step 2184, loss 0.00010272, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:08.305002: step 2185, loss 0.00147455, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:08.543185: step 2186, loss 0.0331668, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:34:08.778873: step 2187, loss 0.000873784, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:09.041139: step 2188, loss 0.0058572, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:09.289686: step 2189, loss 6.82771e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:09.530532: step 2190, loss 0.0885149, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:34:09.768583: step 2191, loss 0.000135263, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:10.011960: step 2192, loss 0.023922, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:34:10.265345: step 2193, loss 7.78575e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:10.504982: step 2194, loss 1.93284e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:10.760806: step 2195, loss 0.000780174, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:11.025765: step 2196, loss 0.0566053, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:34:11.287085: step 2197, loss 9.48229e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:11.544851: step 2198, loss 0.00107499, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:11.783883: step 2199, loss 0.0148759, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:34:12.047558: step 2200, loss 0.134501, acc 0.953125, precision 0.962963, recall 0.928571\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:34:13.164422: step 2200, loss 2.7873, acc 0.701721, precision 0.135048, recall 0.494118, F1 0.212121\n",
      "\n",
      "temporal 2020-10-10T22:34:13.391382: step 2201, loss 0.125916, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:34:13.627549: step 2202, loss 0.00134302, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:13.885989: step 2203, loss 0.000269066, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:14.145564: step 2204, loss 3.18314e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:14.383139: step 2205, loss 7.63168e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:14.619217: step 2206, loss 0.000904225, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:14.858416: step 2207, loss 3.93304e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:15.088673: step 2208, loss 3.22858e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:15.328295: step 2209, loss 0.0265987, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:34:15.568670: step 2210, loss 0.00643513, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:15.808640: step 2211, loss 0.00906649, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:16.045806: step 2212, loss 0.017144, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:34:16.307042: step 2213, loss 0.0249318, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:34:16.545829: step 2214, loss 4.7054e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:16.782572: step 2215, loss 0.204639, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:34:17.020203: step 2216, loss 0.00152756, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:17.256840: step 2217, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:17.489372: step 2218, loss 4.50757e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:17.727291: step 2219, loss 0.105877, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:34:17.987398: step 2220, loss 6.05336e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:18.227812: step 2221, loss 0.000459493, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:18.467176: step 2222, loss 0.0317702, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:34:18.704117: step 2223, loss 2.78257e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:18.938604: step 2224, loss 0.167627, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:34:19.179609: step 2225, loss 0.085294, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:34:19.438691: step 2226, loss 9.42513e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:19.699300: step 2227, loss 0.00233766, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:19.936812: step 2228, loss 2.91554e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:20.185906: step 2229, loss 0.0880591, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:34:20.427648: step 2230, loss 0.0277345, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:34:20.666574: step 2231, loss 1.06171e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:20.876231: step 2232, loss 0.0139748, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:21.135768: step 2233, loss 6.37012e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:21.375196: step 2234, loss 6.92892e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:21.640422: step 2235, loss 0.0168473, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:21.882252: step 2236, loss 0.119992, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:34:22.116526: step 2237, loss 1.16726e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:22.350569: step 2238, loss 0.000142707, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:22.589032: step 2239, loss 0.199951, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:34:22.832579: step 2240, loss 0.00334109, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:23.094155: step 2241, loss 0.237111, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:34:23.355271: step 2242, loss 0.00102179, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:23.610926: step 2243, loss 5.67193e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:23.854826: step 2244, loss 0.000397764, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:24.163836: step 2245, loss 0.0504537, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:34:24.403000: step 2246, loss 0.00130802, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:24.646196: step 2247, loss 0.29107, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:34:24.916626: step 2248, loss 0.00108135, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:25.178647: step 2249, loss 0.233608, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:34:25.440198: step 2250, loss 0.113451, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:34:25.685933: step 2251, loss 0.0433708, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:34:25.923075: step 2252, loss 8.59004e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:26.184877: step 2253, loss 7.52203e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:26.446866: step 2254, loss 0.0043298, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:26.687118: step 2255, loss 0.000625942, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:26.896215: step 2256, loss 7.43654e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:27.138133: step 2257, loss 0.000406791, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:27.373899: step 2258, loss 0.115934, acc 0.96875, precision 1, recall 0.931035\n",
      "temporal 2020-10-10T22:34:27.634344: step 2259, loss 0.113594, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:34:27.877573: step 2260, loss 0.0182032, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:34:28.146606: step 2261, loss 0.115336, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:34:28.384456: step 2262, loss 3.27824e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:28.641400: step 2263, loss 0.0567989, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:34:28.903072: step 2264, loss 0.0074823, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:29.164308: step 2265, loss 9.841e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:29.403774: step 2266, loss 0.0562027, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:34:29.645271: step 2267, loss 0.0188717, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:34:29.880001: step 2268, loss 0.00937446, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:30.115937: step 2269, loss 1.94234e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:30.349709: step 2270, loss 0.101379, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:34:30.591088: step 2271, loss 2.5292e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:30.848420: step 2272, loss 0.0343785, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:34:31.087734: step 2273, loss 0.242036, acc 0.9375, precision 0.875, recall 1\n",
      "temporal 2020-10-10T22:34:31.324269: step 2274, loss 0.00297788, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:31.584672: step 2275, loss 3.15898e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:31.841700: step 2276, loss 0.0544055, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:34:32.075484: step 2277, loss 4.19065e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:32.337349: step 2278, loss 0.000939436, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:32.597805: step 2279, loss 0.0382464, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:34:32.834047: step 2280, loss 2.26127e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:33.076273: step 2281, loss 0.174286, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:34:33.340825: step 2282, loss 1.10826e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:33.603509: step 2283, loss 0.0003806, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:33.841747: step 2284, loss 0.00418294, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:34.083346: step 2285, loss 0.128969, acc 0.984375, precision 1, recall 0.952381\n",
      "temporal 2020-10-10T22:34:34.338172: step 2286, loss 0.217476, acc 0.96875, precision 1, recall 0.931035\n",
      "temporal 2020-10-10T22:34:34.578793: step 2287, loss 0.0037087, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:34.835123: step 2288, loss 0.0539931, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:34:35.073373: step 2289, loss 3.88112e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:35.311549: step 2290, loss 0.168973, acc 0.96875, precision 1, recall 0.95\n",
      "temporal 2020-10-10T22:34:35.549022: step 2291, loss 3.22236e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:35.784829: step 2292, loss 8.1271e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:36.023920: step 2293, loss 1.70662e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:36.277655: step 2294, loss 0.124897, acc 0.984375, precision 0.977273, recall 1\n",
      "temporal 2020-10-10T22:34:36.517953: step 2295, loss 0.000401013, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:36.753360: step 2296, loss 0.0433006, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:34:36.990141: step 2297, loss 0.0319447, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:34:37.225412: step 2298, loss 0.00564452, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:37.482674: step 2299, loss 0.114701, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:34:37.710530: step 2300, loss 1.35973e-07, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:34:38.828735: step 2300, loss 5.25323, acc 0.572658, precision 0.129098, recall 0.741176, F1 0.219895\n",
      "\n",
      "temporal 2020-10-10T22:34:39.031808: step 2301, loss 0.0060619, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:39.255840: step 2302, loss 0.0499148, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:34:39.482660: step 2303, loss 0.05018, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:34:39.676725: step 2304, loss 1.85913e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:39.916080: step 2305, loss 0.022946, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:34:40.141388: step 2306, loss 0.000828722, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:40.393457: step 2307, loss 0.0416996, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:34:40.621992: step 2308, loss 0.000114259, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:40.857517: step 2309, loss 0.000146756, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:41.095774: step 2310, loss 0.0125798, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:41.362516: step 2311, loss 0.231379, acc 0.96875, precision 0.97561, recall 0.97561\n",
      "temporal 2020-10-10T22:34:41.606544: step 2312, loss 0.13186, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:34:41.844015: step 2313, loss 0.0187087, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:34:42.097610: step 2314, loss 0.00732441, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:42.337861: step 2315, loss 0.0619719, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:34:42.569227: step 2316, loss 7.56796e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:42.799912: step 2317, loss 0.000194266, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:43.052177: step 2318, loss 0.000801915, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:43.277853: step 2319, loss 0.000336304, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:43.490679: step 2320, loss 0.0184907, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:34:43.737794: step 2321, loss 0.0190097, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:34:43.960241: step 2322, loss 0.328377, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:34:44.206787: step 2323, loss 0.177952, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:34:44.421302: step 2324, loss 0.000104382, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:44.641359: step 2325, loss 6.56831e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:44.864174: step 2326, loss 0.000271791, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:45.087625: step 2327, loss 8.12657e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:45.278837: step 2328, loss 4.38306e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:45.522191: step 2329, loss 0.00565662, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:45.748035: step 2330, loss 0.000474442, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:46.006606: step 2331, loss 0.000324785, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:46.264547: step 2332, loss 0.00591038, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:46.524440: step 2333, loss 3.11047e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:46.763368: step 2334, loss 0.0347588, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:34:47.020620: step 2335, loss 0.000880749, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:47.260808: step 2336, loss 0.285538, acc 0.953125, precision 0.941176, recall 0.969697\n",
      "temporal 2020-10-10T22:34:47.496045: step 2337, loss 0.000143014, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:47.750484: step 2338, loss 0.0580936, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:34:47.976713: step 2339, loss 9.82776e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:48.209117: step 2340, loss 0.0120158, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:34:48.439680: step 2341, loss 0.120511, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:34:48.682711: step 2342, loss 0.00106512, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:48.920390: step 2343, loss 0.000109257, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:49.160407: step 2344, loss 0.232327, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:34:49.402998: step 2345, loss 0.0666295, acc 0.96875, precision 0.972222, recall 0.972222\n",
      "temporal 2020-10-10T22:34:49.661829: step 2346, loss 0.0012315, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:49.897319: step 2347, loss 0.00311793, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:50.135733: step 2348, loss 0.0844787, acc 0.96875, precision 0.958333, recall 0.958333\n",
      "temporal 2020-10-10T22:34:50.372226: step 2349, loss 0.076664, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:34:50.606991: step 2350, loss 0.00289156, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:50.835444: step 2351, loss 0.202974, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:34:51.036103: step 2352, loss 0.0465374, acc 0.979167, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:34:51.273481: step 2353, loss 0.0125233, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:51.534478: step 2354, loss 0.0018895, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:51.792383: step 2355, loss 2.87769e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:52.029565: step 2356, loss 5.26149e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:52.289887: step 2357, loss 0.000732013, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:52.527640: step 2358, loss 0.000636844, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:52.762226: step 2359, loss 0.000153319, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:53.000753: step 2360, loss 0.0139877, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:34:53.235967: step 2361, loss 0.0343543, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:34:53.479465: step 2362, loss 0.0225068, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:34:53.715236: step 2363, loss 0.0855805, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:34:53.973509: step 2364, loss 0.0442895, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:34:54.215625: step 2365, loss 0.133537, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:34:54.453762: step 2366, loss 2.46595e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:54.739449: step 2367, loss 0.000853156, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:54.977447: step 2368, loss 0.00251788, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:55.215924: step 2369, loss 0.0126864, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:34:55.454741: step 2370, loss 0.0459825, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:34:55.693601: step 2371, loss 0.190842, acc 0.953125, precision 0.939394, recall 0.96875\n",
      "temporal 2020-10-10T22:34:55.953562: step 2372, loss 0.000177107, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:56.232110: step 2373, loss 0.000243948, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:56.491463: step 2374, loss 0.000890196, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:56.733792: step 2375, loss 0.292379, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:34:56.939055: step 2376, loss 1.43792e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:57.196135: step 2377, loss 0.104571, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:34:57.434716: step 2378, loss 0.0203382, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:34:57.671922: step 2379, loss 0.0233199, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:34:57.929521: step 2380, loss 0.125482, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:34:58.167251: step 2381, loss 3.30035e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:58.427848: step 2382, loss 0.0550582, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:34:58.684497: step 2383, loss 0.00269444, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:58.925076: step 2384, loss 5.77692e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:59.164531: step 2385, loss 0.00324696, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:59.415537: step 2386, loss 0.00358218, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:59.651391: step 2387, loss 0.000219771, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:34:59.914938: step 2388, loss 0.0563458, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:35:00.155625: step 2389, loss 0.0208136, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:00.412976: step 2390, loss 0.0437006, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:35:00.648887: step 2391, loss 3.66713e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:00.885501: step 2392, loss 0.0672412, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:35:01.123841: step 2393, loss 0.0356673, acc 0.96875, precision 0.970588, recall 0.970588\n",
      "temporal 2020-10-10T22:35:01.392587: step 2394, loss 0.0129517, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:35:01.658857: step 2395, loss 0.00283749, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:01.903359: step 2396, loss 0.0478342, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:35:02.158517: step 2397, loss 1.28522e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:02.398910: step 2398, loss 0.0355382, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:35:02.635335: step 2399, loss 4.70274e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:02.841671: step 2400, loss 0.029492, acc 0.979167, precision 0.956522, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:35:03.970287: step 2400, loss 4.46354, acc 0.629063, precision 0.133172, recall 0.647059, F1 0.220884\n",
      "\n",
      "temporal 2020-10-10T22:35:04.208413: step 2401, loss 0.000937066, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:04.426692: step 2402, loss 7.20122e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:04.675390: step 2403, loss 2.05914e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:04.929528: step 2404, loss 1.73775e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:05.166753: step 2405, loss 6.929e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:05.431443: step 2406, loss 0.111728, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:35:05.672160: step 2407, loss 8.72104e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:05.911508: step 2408, loss 0.000412847, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:06.153710: step 2409, loss 0.150221, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:06.391627: step 2410, loss 0.00319568, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:06.653441: step 2411, loss 4.41442e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:06.891759: step 2412, loss 0.000476476, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:07.130319: step 2413, loss 0.0372912, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:35:07.376106: step 2414, loss 4.44803e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:07.608835: step 2415, loss 5.40167e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:07.848396: step 2416, loss 0.000854976, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:08.084780: step 2417, loss 0.127567, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:35:08.323406: step 2418, loss 0.0256181, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:35:08.559520: step 2419, loss 0.0258371, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:08.801202: step 2420, loss 3.52038e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:09.061757: step 2421, loss 0.00371601, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:09.297235: step 2422, loss 0.000718124, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:09.560454: step 2423, loss 0.140725, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:35:09.793604: step 2424, loss 6.45717e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:10.055136: step 2425, loss 3.2115e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:10.294209: step 2426, loss 0.000892327, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:10.556275: step 2427, loss 0.00138066, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:10.792631: step 2428, loss 0.0472213, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:35:11.019475: step 2429, loss 0.0198437, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:35:11.246386: step 2430, loss 0.0383519, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:35:11.488350: step 2431, loss 0.000168445, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:11.713157: step 2432, loss 0.122683, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:35:11.944047: step 2433, loss 4.92396e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:12.170424: step 2434, loss 0.041034, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:35:12.397427: step 2435, loss 0.00629664, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:12.625551: step 2436, loss 0.0679029, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:35:12.851777: step 2437, loss 0.000179488, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:13.104827: step 2438, loss 0.00413202, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:13.332218: step 2439, loss 0.00202405, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:13.562933: step 2440, loss 3.74721e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:13.790391: step 2441, loss 1.04912e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:14.037435: step 2442, loss 0.000474523, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:14.320532: step 2443, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:14.553881: step 2444, loss 3.72527e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:14.801929: step 2445, loss 0.00182173, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:15.025325: step 2446, loss 0.0420519, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:35:15.270971: step 2447, loss 3.9354e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:15.468713: step 2448, loss 1.77526e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:15.715857: step 2449, loss 0.0783573, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:35:15.994824: step 2450, loss 8.32944e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:16.211387: step 2451, loss 0.00173922, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:16.433843: step 2452, loss 0.541184, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:35:16.667731: step 2453, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:16.912866: step 2454, loss 1.03374e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:17.139582: step 2455, loss 0.104133, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:35:17.365888: step 2456, loss 3.12923e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:17.602208: step 2457, loss 5.29768e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:17.828196: step 2458, loss 0.00219323, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:18.056658: step 2459, loss 0.0012205, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:18.307617: step 2460, loss 0.0584645, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:18.533802: step 2461, loss 0.0205121, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:35:18.763191: step 2462, loss 0.0729893, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:35:19.012126: step 2463, loss 0.00315218, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:19.241485: step 2464, loss 1.50123e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:19.468276: step 2465, loss 2.94296e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:19.716960: step 2466, loss 0.00505818, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:19.959641: step 2467, loss 1.95577e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:20.178664: step 2468, loss 4.08976e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:20.410854: step 2469, loss 0.000758738, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:20.637440: step 2470, loss 4.12295e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:20.862827: step 2471, loss 6.50056e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:21.078164: step 2472, loss 1.6761e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:21.308776: step 2473, loss 0.00387399, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:21.536154: step 2474, loss 1.42491e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:21.776409: step 2475, loss 0.000317799, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:22.024263: step 2476, loss 8.59647e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:22.273802: step 2477, loss 0.000174384, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:22.501319: step 2478, loss 1.9772e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:22.732867: step 2479, loss 0.280634, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:22.956045: step 2480, loss 5.47525e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:23.181975: step 2481, loss 0.0327385, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:35:23.415785: step 2482, loss 0.00178902, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:23.696233: step 2483, loss 0.0331497, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:35:23.923751: step 2484, loss 1.93806e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:24.150292: step 2485, loss 9.9463e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:24.378770: step 2486, loss 0.0975082, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:35:24.603474: step 2487, loss 0.000537999, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:24.826449: step 2488, loss 0.114475, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:35:25.049753: step 2489, loss 0.105429, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:35:25.284521: step 2490, loss 0.124624, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:35:25.513122: step 2491, loss 0.00331572, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:25.745355: step 2492, loss 6.14003e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:25.973683: step 2493, loss 0.000711432, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:26.196925: step 2494, loss 0.0402062, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:35:26.424016: step 2495, loss 3.09182e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:26.627938: step 2496, loss 2.61579e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:26.864865: step 2497, loss 0.0720782, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:35:27.094841: step 2498, loss 2.69326e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:27.332277: step 2499, loss 0.00468226, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:27.563681: step 2500, loss 0.0459476, acc 0.96875, precision 1, recall 0.941176\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:35:28.670328: step 2500, loss 3.03625, acc 0.708413, precision 0.135762, recall 0.482353, F1 0.211886\n",
      "\n",
      "temporal 2020-10-10T22:35:28.885234: step 2501, loss 0.0147161, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:35:29.107103: step 2502, loss 3.74558e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:29.327087: step 2503, loss 0.00191008, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:29.549799: step 2504, loss 0.100127, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:35:29.801223: step 2505, loss 0.00567767, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:30.045795: step 2506, loss 0.000367472, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:30.273461: step 2507, loss 3.00775e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:30.502573: step 2508, loss 0.000804028, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:30.727505: step 2509, loss 0.000251543, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:30.973112: step 2510, loss 0.000164267, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:31.196854: step 2511, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:31.430235: step 2512, loss 7.24829e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:31.705568: step 2513, loss 0.183481, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:35:31.945256: step 2514, loss 0.0825899, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:32.204192: step 2515, loss 0.00990901, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:32.442250: step 2516, loss 0.00673988, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:32.686232: step 2517, loss 0.0434564, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:32.924051: step 2518, loss 0.067944, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:33.179550: step 2519, loss 4.02785e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:33.387284: step 2520, loss 0.0175694, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:33.624101: step 2521, loss 3.5421e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:33.862389: step 2522, loss 0.0174799, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:34.099416: step 2523, loss 0.00314716, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:34.338676: step 2524, loss 0.0170413, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:35:34.580302: step 2525, loss 0.000137206, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:34.817629: step 2526, loss 0.0152525, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:35:35.080086: step 2527, loss 0.173229, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:35:35.322908: step 2528, loss 0.0888734, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:35:35.561165: step 2529, loss 0.0013926, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:35.796405: step 2530, loss 0.0002847, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:36.033976: step 2531, loss 1.1625e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:36.271096: step 2532, loss 0.0333742, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:35:36.508112: step 2533, loss 0.00215873, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:36.745932: step 2534, loss 1.09896e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:37.005177: step 2535, loss 0.000637139, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:37.245412: step 2536, loss 0.0244574, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:35:37.507744: step 2537, loss 8.1125e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:37.745520: step 2538, loss 2.73606e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:37.999979: step 2539, loss 9.34594e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:38.238395: step 2540, loss 0.126223, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:35:38.497303: step 2541, loss 0.000512985, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:38.734348: step 2542, loss 1.36399e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:38.997163: step 2543, loss 1.24981e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:39.207323: step 2544, loss 0.0736071, acc 0.958333, precision 0.92, recall 1\n",
      "temporal 2020-10-10T22:35:39.443339: step 2545, loss 0.0800611, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:35:39.676155: step 2546, loss 0.0849228, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:35:39.911951: step 2547, loss 9.31298e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:40.149223: step 2548, loss 2.10009e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:40.407987: step 2549, loss 0.0404589, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:35:40.712041: step 2550, loss 0.16976, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:35:40.949558: step 2551, loss 0.000193307, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:41.211376: step 2552, loss 0.00083051, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:41.444145: step 2553, loss 2.99885e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:41.681673: step 2554, loss 0.051689, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:35:41.919414: step 2555, loss 0.00109405, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:42.157436: step 2556, loss 0.0402714, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:35:42.415146: step 2557, loss 0.0115361, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:35:42.678967: step 2558, loss 0.0426112, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:35:42.926330: step 2559, loss 1.487e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:43.164770: step 2560, loss 0.00549883, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:43.408037: step 2561, loss 0.00013366, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:43.669508: step 2562, loss 0.149743, acc 0.953125, precision 0.911765, recall 1\n",
      "temporal 2020-10-10T22:35:43.906691: step 2563, loss 6.51924e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:44.142806: step 2564, loss 7.35057e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:44.384942: step 2565, loss 1.48074e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:44.609248: step 2566, loss 4.02327e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:44.861261: step 2567, loss 0.000300529, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:45.056780: step 2568, loss 2.79359e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:45.294592: step 2569, loss 0.0121927, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:35:45.521150: step 2570, loss 0.0116938, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:35:45.745819: step 2571, loss 1.53475e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:45.969386: step 2572, loss 4.61716e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:46.195757: step 2573, loss 0.0148282, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:35:46.429404: step 2574, loss 0.219247, acc 0.96875, precision 0.975, recall 0.975\n",
      "temporal 2020-10-10T22:35:46.663099: step 2575, loss 0.0286348, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:35:46.896442: step 2576, loss 0.199612, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:47.137692: step 2577, loss 5.3058e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:47.365846: step 2578, loss 1.57948e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:47.594993: step 2579, loss 0.00199802, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:47.822021: step 2580, loss 8.866e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:48.037152: step 2581, loss 0.315453, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:35:48.258565: step 2582, loss 1.1433e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:48.484668: step 2583, loss 0.0212281, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:35:48.736229: step 2584, loss 0.000703018, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:48.967571: step 2585, loss 0.289978, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:35:49.195492: step 2586, loss 1.37647e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:49.437596: step 2587, loss 5.46869e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:49.664251: step 2588, loss 6.14926e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:49.892093: step 2589, loss 0.107642, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:35:50.144000: step 2590, loss 0.000999012, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:50.412508: step 2591, loss 0.000169766, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:50.622613: step 2592, loss 0.000528913, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:50.884021: step 2593, loss 0.000246301, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:51.126902: step 2594, loss 0.0341023, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:51.386830: step 2595, loss 0.056168, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:35:51.623319: step 2596, loss 1.21069e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:51.860887: step 2597, loss 0.0275888, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:35:52.119412: step 2598, loss 0.013935, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:35:52.357106: step 2599, loss 0.0122594, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:35:52.596763: step 2600, loss 0.0665254, acc 0.984375, precision 0.965517, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:35:53.731402: step 2600, loss 5.82238, acc 0.562141, precision 0.124748, recall 0.729412, F1 0.213058\n",
      "\n",
      "temporal 2020-10-10T22:35:53.972833: step 2601, loss 0.000500743, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:54.208346: step 2602, loss 0.0016105, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:54.464971: step 2603, loss 0.000502254, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:54.698945: step 2604, loss 0.0117637, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:35:54.936228: step 2605, loss 7.00629e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:55.197377: step 2606, loss 1.09896e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:55.434773: step 2607, loss 0.0002308, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:55.676932: step 2608, loss 0.138861, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:55.913772: step 2609, loss 7.71409e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:56.146652: step 2610, loss 0.000347096, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:56.383474: step 2611, loss 6.45716e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:56.623188: step 2612, loss 0.168463, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:35:56.885740: step 2613, loss 0.0682451, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:35:57.121306: step 2614, loss 6.89177e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:57.413670: step 2615, loss 0.00686629, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:57.641572: step 2616, loss 1.25877e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:57.880515: step 2617, loss 0.000289744, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:58.114830: step 2618, loss 0.0690464, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:35:58.350998: step 2619, loss 1.41555e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:58.585409: step 2620, loss 3.71554e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:58.822007: step 2621, loss 0.000507566, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:59.057961: step 2622, loss 7.82301e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:59.298560: step 2623, loss 0.00740077, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:35:59.536026: step 2624, loss 0.0923506, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:35:59.797396: step 2625, loss 2.12341e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:00.039105: step 2626, loss 7.28288e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:00.274919: step 2627, loss 0.00706761, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:00.512117: step 2628, loss 9.32306e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:00.750981: step 2629, loss 0.0531656, acc 0.984375, precision 1, recall 0.954545\n",
      "temporal 2020-10-10T22:36:00.988559: step 2630, loss 0.0807754, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:36:01.250436: step 2631, loss 0.0626971, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:36:01.509383: step 2632, loss 4.54482e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:01.745494: step 2633, loss 0.0171498, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:01.984918: step 2634, loss 0.0970074, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:02.225128: step 2635, loss 0.000189296, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:02.463743: step 2636, loss 4.08172e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:02.699000: step 2637, loss 1.04378e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:02.935886: step 2638, loss 1.2666e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:03.177165: step 2639, loss 0.00022719, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:03.384600: step 2640, loss 3.47692e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:03.642517: step 2641, loss 0.155376, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:03.878743: step 2642, loss 1.1306e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:04.116084: step 2643, loss 0.066522, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:04.354663: step 2644, loss 9.58996e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:04.589807: step 2645, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:04.830099: step 2646, loss 9.815e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:05.063178: step 2647, loss 1.27773e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:05.325573: step 2648, loss 2.22571e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:05.587800: step 2649, loss 0.007325, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:05.822510: step 2650, loss 0.161229, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:36:06.063331: step 2651, loss 0.0633553, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:06.306109: step 2652, loss 0.00375796, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:06.545041: step 2653, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:06.778603: step 2654, loss 8.94505e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:07.017635: step 2655, loss 0.0448642, acc 0.96875, precision 1, recall 0.931035\n",
      "temporal 2020-10-10T22:36:07.274376: step 2656, loss 0.0625194, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:07.534738: step 2657, loss 1.63912e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:07.772357: step 2658, loss 1.93705e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:08.010008: step 2659, loss 0.0196083, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:36:08.258430: step 2660, loss 0.000729298, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:08.492638: step 2661, loss 0.00138792, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:08.757115: step 2662, loss 0.0592261, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:36:09.000362: step 2663, loss 1.65322e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:09.229721: step 2664, loss 0.265262, acc 0.958333, precision 0.95, recall 0.95\n",
      "temporal 2020-10-10T22:36:09.484855: step 2665, loss 0.0349722, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:36:09.720927: step 2666, loss 0.0256449, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:36:09.953644: step 2667, loss 0.0608138, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:36:10.189989: step 2668, loss 9.59238e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:10.452256: step 2669, loss 0.000715524, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:10.686704: step 2670, loss 0.191254, acc 0.984375, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:36:10.927782: step 2671, loss 0.0815841, acc 0.96875, precision 1, recall 0.933333\n",
      "temporal 2020-10-10T22:36:11.165634: step 2672, loss 0.000113765, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:11.405353: step 2673, loss 0.00664299, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:11.643688: step 2674, loss 0.0433588, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:36:11.886790: step 2675, loss 0.000696816, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:12.127170: step 2676, loss 0.0101717, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:12.366795: step 2677, loss 0.0547932, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:36:12.629394: step 2678, loss 3.2596e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:12.896289: step 2679, loss 0.000101705, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:13.167946: step 2680, loss 0.136457, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:36:13.407227: step 2681, loss 0.0643039, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:36:13.667877: step 2682, loss 0.000622981, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:13.905731: step 2683, loss 0.0291716, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:36:14.147532: step 2684, loss 0.0538231, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:36:14.381552: step 2685, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:14.643875: step 2686, loss 5.43888e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:14.909087: step 2687, loss 6.58205e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:15.119275: step 2688, loss 1.82936e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:15.374653: step 2689, loss 0.00113257, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:15.642238: step 2690, loss 0.0290895, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:15.880197: step 2691, loss 0.0402564, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:36:16.142108: step 2692, loss 0.0898629, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:16.379843: step 2693, loss 8.94067e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:16.617193: step 2694, loss 0.104572, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:36:16.905639: step 2695, loss 0.00287359, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:17.143764: step 2696, loss 0.000213577, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:17.380214: step 2697, loss 1.11197e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:17.618881: step 2698, loss 3.03634e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:17.874403: step 2699, loss 0.0138069, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:36:18.134057: step 2700, loss 8.30527e-06, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:36:19.339142: step 2700, loss 7.9464, acc 0.486616, precision 0.118243, recall 0.823529, F1 0.206795\n",
      "\n",
      "temporal 2020-10-10T22:36:19.570349: step 2701, loss 0.000139608, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:19.807214: step 2702, loss 0.0362162, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:20.043655: step 2703, loss 0.00118101, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:20.290648: step 2704, loss 0.113777, acc 0.96875, precision 0.931035, recall 1\n",
      "temporal 2020-10-10T22:36:20.526236: step 2705, loss 0.0214921, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:36:20.760410: step 2706, loss 0.00332342, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:21.020333: step 2707, loss 0.00240713, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:21.258734: step 2708, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:21.495016: step 2709, loss 1.13619e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:21.751729: step 2710, loss 0.00068655, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:22.010855: step 2711, loss 0.00422272, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:22.216155: step 2712, loss 3.47693e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:22.455188: step 2713, loss 0.000473668, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:22.716755: step 2714, loss 0.000644574, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:22.978226: step 2715, loss 0.00758103, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:23.213469: step 2716, loss 0.0624747, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:23.452188: step 2717, loss 0.000102954, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:23.688232: step 2718, loss 6.92883e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:23.935533: step 2719, loss 0.00243052, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:24.188704: step 2720, loss 0.0234064, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:24.427694: step 2721, loss 0.00164648, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:24.685748: step 2722, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:24.921901: step 2723, loss 0.000586463, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:25.158427: step 2724, loss 0.00259104, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:25.398470: step 2725, loss 0.0134496, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:36:25.635334: step 2726, loss 0.000124511, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:25.895402: step 2727, loss 0.000389431, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:26.133958: step 2728, loss 0.00308476, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:26.393347: step 2729, loss 6.69003e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:26.661378: step 2730, loss 1.51612e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:26.897411: step 2731, loss 0.114752, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:36:27.134661: step 2732, loss 0.0210927, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:27.390215: step 2733, loss 2.35608e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:27.650425: step 2734, loss 0.000164677, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:27.909901: step 2735, loss 1.52736e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:28.119046: step 2736, loss 0.0945684, acc 0.958333, precision 0.961538, recall 0.961538\n",
      "temporal 2020-10-10T22:36:28.421743: step 2737, loss 0.0341248, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:36:28.666562: step 2738, loss 9.38963e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:28.904017: step 2739, loss 0.0689681, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:36:29.158203: step 2740, loss 7.55887e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:29.392584: step 2741, loss 2.37029e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:29.637525: step 2742, loss 0.00116768, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:29.897357: step 2743, loss 5.50129e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:30.139036: step 2744, loss 0.00502607, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:30.366524: step 2745, loss 0.000455849, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:30.592649: step 2746, loss 9.27697e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:30.820623: step 2747, loss 0.0504117, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:36:31.047622: step 2748, loss 0.000236907, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:31.274365: step 2749, loss 0.0341977, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:31.502964: step 2750, loss 7.06746e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:31.748263: step 2751, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:31.974556: step 2752, loss 1.11385e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:32.199931: step 2753, loss 2.34692e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:32.428683: step 2754, loss 0.000308231, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:32.657139: step 2755, loss 5.02914e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:32.884532: step 2756, loss 0.203917, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:36:33.138474: step 2757, loss 0.177285, acc 0.96875, precision 0.925926, recall 1\n",
      "temporal 2020-10-10T22:36:33.370712: step 2758, loss 0.0163522, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:36:33.609075: step 2759, loss 0.0330152, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:36:33.812494: step 2760, loss 0.0010285, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:34.042983: step 2761, loss 0.00394323, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:34.290165: step 2762, loss 8.99008e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:34.535825: step 2763, loss 0.0723689, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:36:34.759324: step 2764, loss 2.0489e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:34.986727: step 2765, loss 0.000115921, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:35.224681: step 2766, loss 0.00092451, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:35.451176: step 2767, loss 1.91918e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:35.663920: step 2768, loss 4.61932e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:35.885136: step 2769, loss 0.0152561, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:36.106436: step 2770, loss 0.010992, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:36.330838: step 2771, loss 1.86264e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:36.558528: step 2772, loss 9.36883e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:36.790009: step 2773, loss 4.84586e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:37.054590: step 2774, loss 0.0430702, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:36:37.292341: step 2775, loss 0.0204214, acc 0.984375, precision 1, recall 0.954545\n",
      "temporal 2020-10-10T22:36:37.527763: step 2776, loss 0.000521077, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:37.764764: step 2777, loss 0.000896316, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:38.002079: step 2778, loss 7.64225e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:38.254924: step 2779, loss 0.0187124, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:36:38.490483: step 2780, loss 0.0241386, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:36:38.724246: step 2781, loss 3.84616e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:38.980175: step 2782, loss 6.50153e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:39.229290: step 2783, loss 0.00506135, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:39.421958: step 2784, loss 0.0844722, acc 0.958333, precision 0.952381, recall 0.952381\n",
      "temporal 2020-10-10T22:36:39.650831: step 2785, loss 7.89747e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:39.896481: step 2786, loss 3.13455e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:40.131744: step 2787, loss 0.000140236, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:40.361710: step 2788, loss 0.0390335, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:36:40.602593: step 2789, loss 0.00363418, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:40.885919: step 2790, loss 0.0659513, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:36:41.089595: step 2791, loss 0.122837, acc 0.984375, precision 0.96, recall 1\n",
      "temporal 2020-10-10T22:36:41.313549: step 2792, loss 0.000773976, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:41.540499: step 2793, loss 0.00024428, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:41.775489: step 2794, loss 2.19791e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:42.015030: step 2795, loss 0.0610547, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:36:42.262583: step 2796, loss 0.0128531, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:36:42.494303: step 2797, loss 2.8198e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:42.754675: step 2798, loss 0.00206855, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:42.992864: step 2799, loss 3.74319e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:43.228439: step 2800, loss 1.71363e-07, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:36:44.418302: step 2800, loss 3.49969, acc 0.701721, precision 0.130293, recall 0.470588, F1 0.204082\n",
      "\n",
      "temporal 2020-10-10T22:36:44.630582: step 2801, loss 0.000116219, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:44.862780: step 2802, loss 1.69864e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:45.104574: step 2803, loss 0.024174, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:36:45.345261: step 2804, loss 0.0978361, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:36:45.603525: step 2805, loss 0.0139301, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:45.840477: step 2806, loss 0.000293973, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:46.099729: step 2807, loss 6.70141e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:46.328441: step 2808, loss 5.63753e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:46.587860: step 2809, loss 0.000799017, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:46.845491: step 2810, loss 0.000118077, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:47.087506: step 2811, loss 0.0765024, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:36:47.325682: step 2812, loss 0.00202192, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:47.582777: step 2813, loss 0.0010613, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:47.819460: step 2814, loss 5.71866e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:48.069096: step 2815, loss 0.0500774, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:36:48.326999: step 2816, loss 0.000123812, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:48.568872: step 2817, loss 0.225039, acc 0.96875, precision 0.947368, recall 1\n",
      "temporal 2020-10-10T22:36:48.804246: step 2818, loss 0.0020828, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:49.062202: step 2819, loss 2.03174e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:49.323315: step 2820, loss 0.00627481, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:49.554138: step 2821, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:49.779101: step 2822, loss 0.0468207, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:36:50.005412: step 2823, loss 0.176912, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:36:50.254205: step 2824, loss 0.127463, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:36:50.485813: step 2825, loss 0.104466, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:50.709799: step 2826, loss 7.07805e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:50.931164: step 2827, loss 0.0562774, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:36:51.170138: step 2828, loss 2.59267e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:51.405708: step 2829, loss 0.000843951, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:51.647836: step 2830, loss 0.0687065, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:36:51.869783: step 2831, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:52.087935: step 2832, loss 1.24176e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:52.316558: step 2833, loss 0.00500562, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:52.565534: step 2834, loss 0.0458029, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:52.795465: step 2835, loss 0.0501445, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:36:53.043474: step 2836, loss 3.22236e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:53.268542: step 2837, loss 0.0233799, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:36:53.493694: step 2838, loss 5.88485e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:53.717318: step 2839, loss 5.28985e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:53.960191: step 2840, loss 8.00927e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:54.182037: step 2841, loss 6.54153e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:54.403120: step 2842, loss 3.76252e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:54.629203: step 2843, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:54.851295: step 2844, loss 0.00028397, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:55.096406: step 2845, loss 0.00893433, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:55.351148: step 2846, loss 0.00011083, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:55.610149: step 2847, loss 1.10078e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:55.848544: step 2848, loss 0.0261564, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:36:56.086698: step 2849, loss 0.0845674, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:36:56.344896: step 2850, loss 0.0226583, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:36:56.608586: step 2851, loss 7.18998e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:56.849982: step 2852, loss 1.40853e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:57.086245: step 2853, loss 0.0935387, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:36:57.325242: step 2854, loss 0.0123828, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:36:57.558407: step 2855, loss 2.42881e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:57.760398: step 2856, loss 0.00271628, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:57.996682: step 2857, loss 3.85538e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:58.230209: step 2858, loss 2.37283e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:58.469195: step 2859, loss 0.00937991, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:58.709601: step 2860, loss 0.0827039, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:36:58.949593: step 2861, loss 3.49035e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:59.187312: step 2862, loss 0.000169888, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:59.423951: step 2863, loss 9.89039e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:59.676572: step 2864, loss 1.00205e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:36:59.911914: step 2865, loss 0.0765248, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:37:00.196154: step 2866, loss 4.47034e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:00.492141: step 2867, loss 7.45057e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:00.731684: step 2868, loss 0.000450155, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:00.970190: step 2869, loss 0.000317357, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:01.203857: step 2870, loss 3.37135e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:01.443311: step 2871, loss 0.352757, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:37:01.680170: step 2872, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:01.941894: step 2873, loss 0.147882, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:37:02.182043: step 2874, loss 0.274576, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:37:02.440781: step 2875, loss 0.0420448, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:37:02.675012: step 2876, loss 0.127749, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:37:02.930164: step 2877, loss 2.90739e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:03.165764: step 2878, loss 1.81231e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:03.403231: step 2879, loss 0.0415329, acc 0.96875, precision 0.966667, recall 0.966667\n",
      "temporal 2020-10-10T22:37:03.608124: step 2880, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:03.866270: step 2881, loss 0.0918173, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:37:04.105962: step 2882, loss 0.29515, acc 0.96875, precision 0.947368, recall 1\n",
      "temporal 2020-10-10T22:37:04.365190: step 2883, loss 0.0222312, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:37:04.600223: step 2884, loss 0.00092853, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:04.837546: step 2885, loss 3.53901e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:05.076820: step 2886, loss 1.54599e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:05.319722: step 2887, loss 0.000104266, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:05.588471: step 2888, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:05.821969: step 2889, loss 0.00380797, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:06.073935: step 2890, loss 1.4156e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:06.310161: step 2891, loss 7.91611e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:06.556780: step 2892, loss 0.00345024, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:06.797769: step 2893, loss 0.000104205, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:07.036097: step 2894, loss 0.0187546, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:37:07.306663: step 2895, loss 0.042756, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:37:07.544704: step 2896, loss 0.0755956, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:37:07.781596: step 2897, loss 6.71583e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:08.015345: step 2898, loss 9.99617e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:08.257621: step 2899, loss 0.00392736, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:08.490786: step 2900, loss 0.000425804, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:37:09.653535: step 2900, loss 3.37451, acc 0.721797, precision 0.137324, recall 0.458824, F1 0.211382\n",
      "\n",
      "temporal 2020-10-10T22:37:09.856261: step 2901, loss 0.107597, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:37:10.114776: step 2902, loss 0.000190511, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:10.371733: step 2903, loss 0.000199921, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:10.602097: step 2904, loss 1.98682e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:10.839566: step 2905, loss 1.24049e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:11.098946: step 2906, loss 0.0503865, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:37:11.335647: step 2907, loss 0.0797121, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:37:11.598430: step 2908, loss 2.55455e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:11.838089: step 2909, loss 0.00518703, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:12.070560: step 2910, loss 1.75888e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:12.332419: step 2911, loss 1.06507e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:12.569139: step 2912, loss 0.000331749, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:12.805480: step 2913, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:13.066420: step 2914, loss 6.55303e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:13.301909: step 2915, loss 2.68219e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:13.556372: step 2916, loss 0.0129358, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:37:13.789583: step 2917, loss 3.94346e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:14.046785: step 2918, loss 2.40279e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:14.305174: step 2919, loss 1.33548e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:14.538211: step 2920, loss 3.09751e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:14.773550: step 2921, loss 1.4584e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:15.013084: step 2922, loss 5.36436e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:15.244042: step 2923, loss 1.51955e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:15.479896: step 2924, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:15.718505: step 2925, loss 9.23845e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:15.956617: step 2926, loss 0.0293182, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:37:16.194229: step 2927, loss 1.86258e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:16.422545: step 2928, loss 0.000233115, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:16.681646: step 2929, loss 0.000891054, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:16.922371: step 2930, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:17.165989: step 2931, loss 2.84982e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:17.402998: step 2932, loss 4.00988e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:17.639415: step 2933, loss 0.000417503, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:17.889285: step 2934, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:18.124711: step 2935, loss 8.94069e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:18.388639: step 2936, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:18.654126: step 2937, loss 0.0144916, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:37:18.892919: step 2938, loss 0.0357092, acc 0.984375, precision 0.958333, recall 1\n",
      "temporal 2020-10-10T22:37:19.152795: step 2939, loss 0.00147702, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:19.411897: step 2940, loss 0.0349773, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:37:19.648165: step 2941, loss 2.12532e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:19.884965: step 2942, loss 3.16649e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:20.123372: step 2943, loss 0.00255304, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:20.364241: step 2944, loss 9.32276e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:20.599746: step 2945, loss 3.54608e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:20.851979: step 2946, loss 5.75689e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:21.089125: step 2947, loss 2.79397e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:21.330409: step 2948, loss 0.0185133, acc 0.984375, precision 0.954545, recall 1\n",
      "temporal 2020-10-10T22:37:21.563801: step 2949, loss 1.27031e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:21.799800: step 2950, loss 0.0143353, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:37:22.038349: step 2951, loss 6.78219e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:22.243622: step 2952, loss 0.00765828, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:22.477495: step 2953, loss 0.0196162, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:37:22.736548: step 2954, loss 1.88802e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:22.972966: step 2955, loss 1.88127e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:23.209199: step 2956, loss 2.76582e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:23.468946: step 2957, loss 0.000149194, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:23.705175: step 2958, loss 5.83278e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:23.967946: step 2959, loss 1.76012e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:24.203847: step 2960, loss 0.165826, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:37:24.457870: step 2961, loss 0.00438327, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:24.718890: step 2962, loss 0.0025216, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:24.956288: step 2963, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:25.207043: step 2964, loss 0.123969, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:37:25.467366: step 2965, loss 2.29105e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:25.723739: step 2966, loss 2.36164e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:25.963431: step 2967, loss 3.78113e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:26.198508: step 2968, loss 0.00295722, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:26.431808: step 2969, loss 1.19685e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:26.717864: step 2970, loss 0.164019, acc 0.984375, precision 0.956522, recall 1\n",
      "temporal 2020-10-10T22:37:26.955085: step 2971, loss 0.00597125, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:27.214957: step 2972, loss 3.00245e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:27.476439: step 2973, loss 3.88081e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:27.717143: step 2974, loss 5.08918e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:27.953649: step 2975, loss 0.0340251, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:37:28.158005: step 2976, loss 0.0156226, acc 0.979167, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:37:28.389692: step 2977, loss 6.33298e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:28.624396: step 2978, loss 0.000439718, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:28.858033: step 2979, loss 2.04891e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:29.094883: step 2980, loss 0.0576081, acc 0.96875, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:37:29.328584: step 2981, loss 1.56326e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:29.567394: step 2982, loss 0.0024716, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:29.803800: step 2983, loss 2.43062e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:30.064671: step 2984, loss 5.27407e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:30.321564: step 2985, loss 0.000733997, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:30.555352: step 2986, loss 2.33561e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:30.793873: step 2987, loss 1.44642e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:31.027973: step 2988, loss 1.15368e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:31.312395: step 2989, loss 0.0752103, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:37:31.573787: step 2990, loss 0.316204, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:37:31.833468: step 2991, loss 5.60878e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:32.070180: step 2992, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:32.305146: step 2993, loss 3.52038e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:32.565619: step 2994, loss 0.0153249, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:37:32.802712: step 2995, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:33.039260: step 2996, loss 0.000648017, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:33.275267: step 2997, loss 1.53741e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:33.510760: step 2998, loss 0.00149003, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:33.741797: step 2999, loss 6.22115e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:33.950603: step 3000, loss 0.0316715, acc 0.979167, precision 0.966667, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:37:35.077490: step 3000, loss 4.63126, acc 0.656788, precision 0.13369, recall 0.588235, F1 0.217865\n",
      "\n",
      "temporal 2020-10-10T22:37:35.288673: step 3001, loss 1.80676e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:35.524858: step 3002, loss 0.00108433, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:35.766464: step 3003, loss 2.3017e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:36.002109: step 3004, loss 0.000228587, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:36.240805: step 3005, loss 0.0230725, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:37:36.506297: step 3006, loss 2.29104e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:36.741951: step 3007, loss 5.2154e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:36.977737: step 3008, loss 0.00207711, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:37.214500: step 3009, loss 2.96882e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:37.453576: step 3010, loss 4.86368e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:37.714146: step 3011, loss 0.00123992, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:37.948828: step 3012, loss 1.13061e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:38.185633: step 3013, loss 6.46327e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:38.440146: step 3014, loss 0.000779285, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:38.676508: step 3015, loss 4.66251e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:38.909109: step 3016, loss 0.0012744, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:39.145337: step 3017, loss 0.00042224, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:39.382555: step 3018, loss 4.61929e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:39.643976: step 3019, loss 2.96243e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:39.904236: step 3020, loss 0.00157882, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:40.144868: step 3021, loss 6.46763e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:40.380380: step 3022, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:40.634818: step 3023, loss 9.7601e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:40.839714: step 3024, loss 1.24176e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:41.077746: step 3025, loss 1.3761e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:41.317204: step 3026, loss 0.00413774, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:41.551469: step 3027, loss 0.000543681, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:41.814114: step 3028, loss 9.62271e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:42.052618: step 3029, loss 1.43438e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:42.310313: step 3030, loss 1.09896e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:42.548832: step 3031, loss 4.65661e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:42.785543: step 3032, loss 0.156715, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:37:43.020695: step 3033, loss 0.0997748, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:37:43.257858: step 3034, loss 9.70788e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:43.493919: step 3035, loss 2.17928e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:43.723015: step 3036, loss 0.00102022, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:43.951912: step 3037, loss 0.00015921, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:44.183649: step 3038, loss 1.12873e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:44.430406: step 3039, loss 0.0463039, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:37:44.660545: step 3040, loss 2.516e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:44.932905: step 3041, loss 0.0100555, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:45.162569: step 3042, loss 0.00114076, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:45.413855: step 3043, loss 0.0067955, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:45.641716: step 3044, loss 1.54599e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:45.887386: step 3045, loss 0.312268, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:37:46.114728: step 3046, loss 0.000256467, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:46.361920: step 3047, loss 4.30566e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:46.556200: step 3048, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:46.804298: step 3049, loss 3.3395e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:47.031113: step 3050, loss 0.0220499, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:37:47.253296: step 3051, loss 3.36125e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:47.481809: step 3052, loss 0.0858109, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:37:47.718024: step 3053, loss 1.60179e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:47.953900: step 3054, loss 3.77658e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:48.190306: step 3055, loss 1.46322e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:48.465747: step 3056, loss 0.0980289, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:37:48.701707: step 3057, loss 1.46964e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:48.931837: step 3058, loss 1.30871e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:49.158444: step 3059, loss 4.87562e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:49.418032: step 3060, loss 0.00166645, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:49.643952: step 3061, loss 0.0388668, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:37:49.894611: step 3062, loss 0.144671, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:37:50.122033: step 3063, loss 7.43653e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:50.350606: step 3064, loss 0.000123299, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:50.575356: step 3065, loss 0.00431875, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:50.804756: step 3066, loss 0.00103591, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:51.027251: step 3067, loss 0.0441583, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:37:51.244267: step 3068, loss 0.0325227, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:37:51.477453: step 3069, loss 5.25426e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:51.697395: step 3070, loss 1.13489e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:51.915930: step 3071, loss 0.155466, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:37:52.134474: step 3072, loss 0.0532888, acc 0.979167, precision 1, recall 0.954545\n",
      "temporal 2020-10-10T22:37:52.392220: step 3073, loss 1.38762e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:52.615271: step 3074, loss 3.50175e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:52.841921: step 3075, loss 0.000414858, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:53.069498: step 3076, loss 0.000731413, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:53.297626: step 3077, loss 3.63368e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:53.531168: step 3078, loss 0.0422852, acc 0.96875, precision 0.933333, recall 1\n",
      "temporal 2020-10-10T22:37:53.763335: step 3079, loss 6.38878e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:54.020215: step 3080, loss 5.1036e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:54.280778: step 3081, loss 0.146198, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:37:54.512936: step 3082, loss 3.96669e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:54.740671: step 3083, loss 0.000440507, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:54.965754: step 3084, loss 0.179045, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:37:55.218367: step 3085, loss 0.141543, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:37:55.465164: step 3086, loss 7.21416e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:55.695811: step 3087, loss 4.47034e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:55.922876: step 3088, loss 0.0230175, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:37:56.154499: step 3089, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:56.404295: step 3090, loss 0.104676, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:37:56.629248: step 3091, loss 0.00175268, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:56.882940: step 3092, loss 2.68099e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:57.129725: step 3093, loss 0.0279707, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:37:57.345193: step 3094, loss 0.0162928, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:37:57.559419: step 3095, loss 1.92668e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:57.749781: step 3096, loss 3.62307e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:57.988240: step 3097, loss 3.57625e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:37:58.213392: step 3098, loss 0.0738991, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:37:58.439039: step 3099, loss 0.713982, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:37:58.661984: step 3100, loss 1.40029e-05, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:37:59.737713: step 3100, loss 3.64862, acc 0.706501, precision 0.119863, recall 0.411765, F1 0.185676\n",
      "\n",
      "temporal 2020-10-10T22:37:59.936044: step 3101, loss 0.0441368, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:00.138961: step 3102, loss 0.0448281, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:38:00.364345: step 3103, loss 0.00950721, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:00.613380: step 3104, loss 0.0853716, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:38:00.826116: step 3105, loss 0.01067, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:01.047886: step 3106, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:01.267093: step 3107, loss 4.97319e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:01.488777: step 3108, loss 0.0938036, acc 0.96875, precision 0.95, recall 1\n",
      "temporal 2020-10-10T22:38:01.718329: step 3109, loss 0.311052, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:38:01.939659: step 3110, loss 0.01574, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:38:02.162463: step 3111, loss 0.174899, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:38:02.395420: step 3112, loss 0.0239213, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:38:02.619674: step 3113, loss 0.00740573, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:02.838124: step 3114, loss 0.0974196, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:38:03.048454: step 3115, loss 6.39595e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:03.295088: step 3116, loss 0.160677, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:38:03.522476: step 3117, loss 0.00168338, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:03.744838: step 3118, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:03.986788: step 3119, loss 0.295784, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:04.182748: step 3120, loss 4.12263e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:04.406396: step 3121, loss 2.04891e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:04.636720: step 3122, loss 0.0950478, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:38:04.884615: step 3123, loss 0.021136, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:05.138527: step 3124, loss 4.42701e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:05.361785: step 3125, loss 1.76293e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:05.584962: step 3126, loss 0.140609, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:38:05.812563: step 3127, loss 6.22418e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:06.060492: step 3128, loss 1.99302e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:06.306844: step 3129, loss 0.0519401, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:38:06.529590: step 3130, loss 0.017734, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:06.751727: step 3131, loss 0.000365998, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:06.975113: step 3132, loss 1.78813e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:07.260882: step 3133, loss 0.00854899, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:07.484022: step 3134, loss 0.00636322, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:07.731412: step 3135, loss 0.0335455, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:07.957421: step 3136, loss 5.35061e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:08.168687: step 3137, loss 2.30967e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:08.416017: step 3138, loss 1.76951e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:08.625226: step 3139, loss 0.000481585, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:08.847377: step 3140, loss 0.0852318, acc 0.96875, precision 0.948718, recall 1\n",
      "temporal 2020-10-10T22:38:09.064608: step 3141, loss 0.0326987, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:38:09.286225: step 3142, loss 7.83235e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:09.508485: step 3143, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:09.691961: step 3144, loss 3.55142e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:09.914433: step 3145, loss 0.0185264, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:38:10.160651: step 3146, loss 5.49336e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:10.380784: step 3147, loss 0.0019903, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:10.606339: step 3148, loss 0.00407418, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:10.830423: step 3149, loss 0.1184, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:38:11.052900: step 3150, loss 0.0440582, acc 0.984375, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:38:11.277230: step 3151, loss 0.112673, acc 0.96875, precision 0.973684, recall 0.973684\n",
      "temporal 2020-10-10T22:38:11.498871: step 3152, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:11.775563: step 3153, loss 0.000247081, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:12.003256: step 3154, loss 0.10119, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:38:12.230246: step 3155, loss 1.43423e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:12.459086: step 3156, loss 3.89469e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:12.705416: step 3157, loss 0.00119983, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:12.951955: step 3158, loss 0.0647966, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:38:13.201037: step 3159, loss 2.86644e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:13.442036: step 3160, loss 0.000197462, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:13.668669: step 3161, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:13.920401: step 3162, loss 1.88126e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:14.152395: step 3163, loss 0.00013501, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:14.377000: step 3164, loss 6.74505e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:14.620446: step 3165, loss 6.25165e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:14.868483: step 3166, loss 0.0404779, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:38:15.111009: step 3167, loss 0.0193614, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:38:15.304949: step 3168, loss 2.48353e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:15.516859: step 3169, loss 0.00250452, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:15.757824: step 3170, loss 0.246048, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:38:15.985695: step 3171, loss 0.0496302, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:16.212325: step 3172, loss 0.0271103, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:16.440135: step 3173, loss 0.00264084, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:16.664992: step 3174, loss 0.0342344, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:16.924020: step 3175, loss 0.000235479, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:17.150386: step 3176, loss 0.0620127, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:38:17.394430: step 3177, loss 0.000231168, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:17.623852: step 3178, loss 0.111461, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:38:17.873642: step 3179, loss 0.0794508, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:38:18.148076: step 3180, loss 1.87568e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:18.376830: step 3181, loss 1.73225e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:18.596954: step 3182, loss 4.20746e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:18.839505: step 3183, loss 4.48043e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:19.067034: step 3184, loss 0.0568674, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:19.291781: step 3185, loss 0.0700499, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:38:19.544440: step 3186, loss 3.18189e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:19.768139: step 3187, loss 0.0246177, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:38:19.991391: step 3188, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:20.234320: step 3189, loss 3.87756e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:20.455661: step 3190, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:20.676863: step 3191, loss 5.65397e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:20.869211: step 3192, loss 5.2154e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:21.090656: step 3193, loss 0.14434, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:38:21.317861: step 3194, loss 9.41302e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:21.541862: step 3195, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:21.789237: step 3196, loss 0.281841, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:38:22.007124: step 3197, loss 0.00749367, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:22.251723: step 3198, loss 0.000631984, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:22.473773: step 3199, loss 0.155522, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:22.725387: step 3200, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:38:23.839734: step 3200, loss 3.97927, acc 0.697897, precision 0.121311, recall 0.435294, F1 0.189744\n",
      "\n",
      "temporal 2020-10-10T22:38:24.054005: step 3201, loss 0.00045138, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:24.264865: step 3202, loss 0.248425, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:38:24.497495: step 3203, loss 4.02466e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:24.723022: step 3204, loss 3.69691e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:24.968897: step 3205, loss 0.0217998, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:38:25.193032: step 3206, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:25.419359: step 3207, loss 1.93715e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:25.641701: step 3208, loss 1.27473e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:25.869880: step 3209, loss 0.000858219, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:26.120432: step 3210, loss 4.88623e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:26.369640: step 3211, loss 0.277252, acc 0.9375, precision 0.9, recall 1\n",
      "temporal 2020-10-10T22:38:26.601898: step 3212, loss 0.105357, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:38:26.826941: step 3213, loss 0.0267527, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:38:27.055132: step 3214, loss 0.051354, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:38:27.278206: step 3215, loss 2.16238e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:27.473850: step 3216, loss 0.0563088, acc 0.979167, precision 1, recall 0.947368\n",
      "temporal 2020-10-10T22:38:27.697907: step 3217, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:27.923491: step 3218, loss 0.247018, acc 0.96875, precision 1, recall 0.928571\n",
      "temporal 2020-10-10T22:38:28.149738: step 3219, loss 1.36692e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:28.372816: step 3220, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:28.593619: step 3221, loss 0.0435824, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:38:28.821097: step 3222, loss 0.0348905, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:38:29.047114: step 3223, loss 0.00111608, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:29.280188: step 3224, loss 1.02673e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:29.504361: step 3225, loss 2.04891e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:29.736750: step 3226, loss 0.000107158, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:29.984959: step 3227, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:30.216256: step 3228, loss 0.149883, acc 0.96875, precision 0.967742, recall 0.967742\n",
      "temporal 2020-10-10T22:38:30.466795: step 3229, loss 1.05984e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:30.689890: step 3230, loss 9.23846e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:30.898117: step 3231, loss 1.81973e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:31.120473: step 3232, loss 5.91694e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:31.340352: step 3233, loss 5.4818e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:31.562533: step 3234, loss 0.23922, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:38:31.787266: step 3235, loss 0.245671, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:38:32.034929: step 3236, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:32.255391: step 3237, loss 1.75868e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:32.490830: step 3238, loss 0.218736, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:38:32.712241: step 3239, loss 1.86264e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:32.925213: step 3240, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:33.149051: step 3241, loss 0.0200747, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:38:33.407594: step 3242, loss 5.14792e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:33.629496: step 3243, loss 0.00027881, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:33.872852: step 3244, loss 0.0121244, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:38:34.092533: step 3245, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:34.337707: step 3246, loss 0.129589, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:38:34.562861: step 3247, loss 0.00174163, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:34.787475: step 3248, loss 6.25836e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:35.018684: step 3249, loss 0.0107314, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:35.238939: step 3250, loss 0.0378068, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:38:35.467628: step 3251, loss 5.42312e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:35.716852: step 3252, loss 0.0596744, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:38:35.943863: step 3253, loss 0.0396082, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:38:36.182516: step 3254, loss 0.0828274, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:38:36.431543: step 3255, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:36.659813: step 3256, loss 1.03659e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:36.901597: step 3257, loss 0.000391786, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:37.122462: step 3258, loss 3.24626e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:37.341140: step 3259, loss 0.0143716, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:38:37.591696: step 3260, loss 0.131886, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:38:37.879920: step 3261, loss 7.93664e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:38.111667: step 3262, loss 0.00367054, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:38.344852: step 3263, loss 0.13965, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:38:38.541127: step 3264, loss 3.05472e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:38.764856: step 3265, loss 0.00186989, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:38.992941: step 3266, loss 0.0081036, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:39.214642: step 3267, loss 0.00331499, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:39.422770: step 3268, loss 2.34693e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:39.668804: step 3269, loss 5.4761e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:39.891017: step 3270, loss 0.000692699, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:40.112692: step 3271, loss 0.00154708, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:40.363357: step 3272, loss 9.23246e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:40.596590: step 3273, loss 2.00594e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:40.864011: step 3274, loss 0.109592, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:38:41.097728: step 3275, loss 0.0285214, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:38:41.333772: step 3276, loss 1.63347e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:41.589672: step 3277, loss 2.05071e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:41.829027: step 3278, loss 0.00642615, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:42.089283: step 3279, loss 1.78434e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:42.341340: step 3280, loss 4.47034e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:42.577842: step 3281, loss 1.82538e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:42.842291: step 3282, loss 7.97579e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:43.100359: step 3283, loss 0.0743394, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:38:43.334100: step 3284, loss 0.0742906, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:38:43.597369: step 3285, loss 0.0103433, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:43.840686: step 3286, loss 0.00206332, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:44.076092: step 3287, loss 4.16375e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:44.277232: step 3288, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:44.539795: step 3289, loss 0.00355372, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:44.801684: step 3290, loss 0.00677381, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:45.035001: step 3291, loss 0.0573388, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:38:45.265234: step 3292, loss 5.94444e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:45.490973: step 3293, loss 0.000917888, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:45.743642: step 3294, loss 7.84154e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:45.970539: step 3295, loss 3.72529e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:46.213450: step 3296, loss 0.0738376, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:38:46.439448: step 3297, loss 0.000666639, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:46.659987: step 3298, loss 1.35781e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:46.867934: step 3299, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:47.088589: step 3300, loss 0, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:38:48.189327: step 3300, loss 6.09801, acc 0.634799, precision 0.129676, recall 0.611765, F1 0.213992\n",
      "\n",
      "temporal 2020-10-10T22:38:48.387324: step 3301, loss 0.0166896, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:38:48.623984: step 3302, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:48.835422: step 3303, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:49.060999: step 3304, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:49.305983: step 3305, loss 6.43412e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:49.552900: step 3306, loss 3.37135e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:49.802885: step 3307, loss 1.02605e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:50.038010: step 3308, loss 0.379512, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:38:50.258891: step 3309, loss 0.00194334, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:50.480199: step 3310, loss 0.0856504, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:38:50.700590: step 3311, loss 8.88977e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:50.921644: step 3312, loss 0.321031, acc 0.979167, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:38:51.143388: step 3313, loss 2.44361e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:51.366617: step 3314, loss 0.101627, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:38:51.591300: step 3315, loss 0.0227154, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:51.804606: step 3316, loss 0.0818163, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:38:52.024976: step 3317, loss 0.000391286, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:52.261388: step 3318, loss 3.11059e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:52.484285: step 3319, loss 0.0162362, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:52.707515: step 3320, loss 0.00108395, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:52.921423: step 3321, loss 4.15501e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:53.143395: step 3322, loss 1.33733e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:53.370811: step 3323, loss 0.0025361, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:53.627194: step 3324, loss 0.000356564, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:53.864536: step 3325, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:54.100285: step 3326, loss 0.0487763, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:38:54.354325: step 3327, loss 0.00445992, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:54.615852: step 3328, loss 0.0713065, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:38:54.850454: step 3329, loss 7.82309e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:55.089553: step 3330, loss 0.0836518, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:38:55.350533: step 3331, loss 0.124165, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:38:55.589979: step 3332, loss 3.53902e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:55.825878: step 3333, loss 0.000286386, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:56.060767: step 3334, loss 0.0882771, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:38:56.323565: step 3335, loss 0.000228466, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:56.542226: step 3336, loss 0.00736716, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:56.847032: step 3337, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:57.109614: step 3338, loss 0.000486649, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:57.369850: step 3339, loss 6.35087e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:57.608038: step 3340, loss 0.0490184, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:38:57.842413: step 3341, loss 0.438994, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:38:58.077091: step 3342, loss 0.000443862, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:58.314467: step 3343, loss 5.7741e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:58.549916: step 3344, loss 0.0577519, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:38:58.788320: step 3345, loss 1.07658e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:59.042870: step 3346, loss 1.4156e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:38:59.283265: step 3347, loss 0.0278567, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:38:59.522862: step 3348, loss 0.271416, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:38:59.760291: step 3349, loss 0.111444, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:00.009581: step 3350, loss 5.58793e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:00.249025: step 3351, loss 0.124336, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:39:00.498505: step 3352, loss 0.000972045, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:00.760807: step 3353, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:01.030674: step 3354, loss 2.24288e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:01.268696: step 3355, loss 0.000292264, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:01.529413: step 3356, loss 0.000474891, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:01.764673: step 3357, loss 0.000394914, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:02.018514: step 3358, loss 1.46025e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:02.282201: step 3359, loss 0.000657806, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:02.489843: step 3360, loss 1.45839e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:02.742926: step 3361, loss 8.07601e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:03.006803: step 3362, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:03.263093: step 3363, loss 7.45056e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:03.502274: step 3364, loss 0.00589288, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:03.765840: step 3365, loss 0.102656, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:39:04.025929: step 3366, loss 1.63119e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:04.283216: step 3367, loss 0.0059711, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:04.521150: step 3368, loss 0.000439373, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:04.779513: step 3369, loss 1.30385e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:05.044280: step 3370, loss 0.00274246, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:05.297911: step 3371, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:05.532590: step 3372, loss 7.6125e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:05.791267: step 3373, loss 0.00220413, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:06.049797: step 3374, loss 0.0018049, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:06.309961: step 3375, loss 0.093096, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:39:06.549170: step 3376, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:06.781593: step 3377, loss 1.30089e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:07.019411: step 3378, loss 0.177927, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:39:07.260979: step 3379, loss 1.24878e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:07.497447: step 3380, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:07.737431: step 3381, loss 5.39188e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:07.992862: step 3382, loss 6.96539e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:08.232307: step 3383, loss 3.65074e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:08.460775: step 3384, loss 1.53978e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:08.709289: step 3385, loss 5.21532e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:08.943755: step 3386, loss 0.477392, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:39:09.197773: step 3387, loss 0.00152929, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:09.432710: step 3388, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:09.670308: step 3389, loss 0.00328261, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:09.905600: step 3390, loss 4.00465e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:10.144371: step 3391, loss 1.7729e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:10.402221: step 3392, loss 8.86596e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:10.652048: step 3393, loss 0.000933226, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:10.888483: step 3394, loss 0.000107553, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:11.142850: step 3395, loss 8.61966e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:11.400690: step 3396, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:11.659936: step 3397, loss 0.00256756, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:11.919530: step 3398, loss 1.9744e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:12.162113: step 3399, loss 0.000521055, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:12.422986: step 3400, loss 1.32617e-06, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:39:13.606711: step 3400, loss 5.41744, acc 0.661568, precision 0.135501, recall 0.588235, F1 0.220264\n",
      "\n",
      "temporal 2020-10-10T22:39:13.856955: step 3401, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:14.096529: step 3402, loss 0.149618, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:39:14.352504: step 3403, loss 6.47753e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:14.613042: step 3404, loss 2.4773e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:14.872421: step 3405, loss 0.0381731, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:39:15.105684: step 3406, loss 7.05043e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:15.362475: step 3407, loss 3.91155e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:15.565701: step 3408, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:15.834442: step 3409, loss 0.00522613, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:16.089306: step 3410, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:16.326083: step 3411, loss 0.00581871, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:16.561170: step 3412, loss 4.22817e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:16.805727: step 3413, loss 0.0462091, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:39:17.044496: step 3414, loss 1.99716e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:17.282808: step 3415, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:17.543590: step 3416, loss 7.30146e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:17.799282: step 3417, loss 3.11059e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:18.037575: step 3418, loss 6.16839e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:18.298104: step 3419, loss 0.0528422, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:39:18.558605: step 3420, loss 1.07829e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:18.822319: step 3421, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:19.062859: step 3422, loss 0.0440759, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:39:19.295672: step 3423, loss 2.04891e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:19.529832: step 3424, loss 0.0108069, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:19.768281: step 3425, loss 0.116777, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:39:20.004905: step 3426, loss 2.81732e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:20.243532: step 3427, loss 2.02789e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:20.483983: step 3428, loss 4.28408e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:20.722942: step 3429, loss 0.0105543, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:20.982273: step 3430, loss 0.130405, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:39:21.237861: step 3431, loss 0.000108871, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:21.462494: step 3432, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:21.701256: step 3433, loss 2.84982e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:21.936517: step 3434, loss 1.87934e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:22.196275: step 3435, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:22.429478: step 3436, loss 0.0923392, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:39:22.687666: step 3437, loss 0.0316611, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:39:22.941813: step 3438, loss 5.55058e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:23.205654: step 3439, loss 0.245379, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:39:23.446324: step 3440, loss 0.127388, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:39:23.709472: step 3441, loss 4.41236e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:23.966831: step 3442, loss 1.02445e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:24.202619: step 3443, loss 0.00114848, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:24.438599: step 3444, loss 0.0308565, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:39:24.676745: step 3445, loss 1.03933e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:24.912603: step 3446, loss 4.24089e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:25.147313: step 3447, loss 0.346513, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:39:25.384260: step 3448, loss 2.28158e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:25.619513: step 3449, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:25.873189: step 3450, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:26.123205: step 3451, loss 0.00810376, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:26.358429: step 3452, loss 0.0530616, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:39:26.590429: step 3453, loss 2.92434e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:26.824368: step 3454, loss 0.00552286, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:27.115873: step 3455, loss 1.14922e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:27.320561: step 3456, loss 1.15428e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:27.575536: step 3457, loss 0.182069, acc 0.953125, precision 0.918919, recall 1\n",
      "temporal 2020-10-10T22:39:27.836241: step 3458, loss 0.0303259, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:28.070401: step 3459, loss 0.161326, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:39:28.363986: step 3460, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:28.600924: step 3461, loss 0.129317, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:39:28.834828: step 3462, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:29.068137: step 3463, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:29.304291: step 3464, loss 0.00833091, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:29.565159: step 3465, loss 0.0126256, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:29.804215: step 3466, loss 0.0975297, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:39:30.039630: step 3467, loss 7.11622e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:30.276372: step 3468, loss 0.0653484, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:39:30.517351: step 3469, loss 0.0781024, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:39:30.757387: step 3470, loss 0.271905, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:39:30.992624: step 3471, loss 1.04869e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:31.231005: step 3472, loss 0.0162867, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:31.469458: step 3473, loss 0.126456, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:39:31.731830: step 3474, loss 9.10807e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:31.988459: step 3475, loss 2.64494e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:32.250618: step 3476, loss 0.00307886, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:32.489269: step 3477, loss 8.49485e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:32.728650: step 3478, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:32.976209: step 3479, loss 0.00148967, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:33.180838: step 3480, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:33.415902: step 3481, loss 2.03027e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:33.671603: step 3482, loss 0.00156054, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:33.910153: step 3483, loss 0.000199195, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:34.170501: step 3484, loss 4.28408e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:34.426868: step 3485, loss 0.00164866, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:34.678467: step 3486, loss 0.165654, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:39:34.915457: step 3487, loss 3.46449e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:35.176391: step 3488, loss 0.148891, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:39:35.435209: step 3489, loss 0.0473093, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:39:35.692952: step 3490, loss 7.65031e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:35.927406: step 3491, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:36.163674: step 3492, loss 5.77419e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:36.398867: step 3493, loss 0.000389078, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:36.632367: step 3494, loss 0.0266269, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:39:36.893641: step 3495, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:37.131554: step 3496, loss 0.0561486, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:39:37.367239: step 3497, loss 1.39606e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:37.594897: step 3498, loss 0.0265257, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:37.829995: step 3499, loss 7.23969e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:38.080370: step 3500, loss 0.000176393, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:39:39.204774: step 3500, loss 4.53166, acc 0.695985, precision 0.130159, recall 0.482353, F1 0.205\n",
      "\n",
      "temporal 2020-10-10T22:39:39.417164: step 3501, loss 0.00578081, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:39.630210: step 3502, loss 0.0279322, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:39:39.851037: step 3503, loss 0.018668, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:39:40.041884: step 3504, loss 0.0786343, acc 0.979167, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:39:40.261108: step 3505, loss 0.124204, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:39:40.493657: step 3506, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:40.738279: step 3507, loss 0.048871, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:39:40.973771: step 3508, loss 2.6466e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:41.206218: step 3509, loss 0.0637869, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:41.441220: step 3510, loss 0.000122742, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:41.676057: step 3511, loss 0.019951, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:39:41.907357: step 3512, loss 0.160659, acc 0.96875, precision 0.942857, recall 1\n",
      "temporal 2020-10-10T22:39:42.159883: step 3513, loss 0.00646992, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:42.413438: step 3514, loss 1.17346e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:42.646801: step 3515, loss 2.19777e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:42.902896: step 3516, loss 0.00421453, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:43.139437: step 3517, loss 0.147445, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:39:43.393816: step 3518, loss 3.4086e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:43.686846: step 3519, loss 0.00025478, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:43.935585: step 3520, loss 2.29275e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:44.161947: step 3521, loss 0.00022554, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:44.410779: step 3522, loss 0.00642444, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:44.634585: step 3523, loss 1.04308e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:44.847127: step 3524, loss 0.0507963, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:39:45.094470: step 3525, loss 0.000556466, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:45.314553: step 3526, loss 9.49947e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:45.570053: step 3527, loss 1.84564e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:45.787612: step 3528, loss 0.00311336, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:46.006986: step 3529, loss 0.320203, acc 0.96875, precision 1, recall 0.9375\n",
      "temporal 2020-10-10T22:39:46.233198: step 3530, loss 1.93891e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:46.472110: step 3531, loss 0.00780127, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:46.711120: step 3532, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:46.967428: step 3533, loss 3.50213e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:47.192484: step 3534, loss 9.44574e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:47.423251: step 3535, loss 0.0232028, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:39:47.651287: step 3536, loss 0.00103346, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:47.874495: step 3537, loss 4.20954e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:48.109629: step 3538, loss 0.00319041, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:48.344220: step 3539, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:48.582236: step 3540, loss 0.00147837, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:48.831202: step 3541, loss 0.142709, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:39:49.071569: step 3542, loss 0.00096661, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:49.333729: step 3543, loss 0.000218401, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:49.561625: step 3544, loss 5.40166e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:49.793735: step 3545, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:50.060793: step 3546, loss 0.000271673, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:50.294701: step 3547, loss 9.3132e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:50.529660: step 3548, loss 1.34816e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:50.765209: step 3549, loss 6.14672e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:51.001636: step 3550, loss 0.0750465, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:39:51.263983: step 3551, loss 9.27851e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:51.469524: step 3552, loss 3.33793e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:51.734795: step 3553, loss 1.86264e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:52.049641: step 3554, loss 5.2712e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:52.307763: step 3555, loss 0.000825734, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:52.545008: step 3556, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:52.782138: step 3557, loss 0.141029, acc 0.96875, precision 0.974359, recall 0.974359\n",
      "temporal 2020-10-10T22:39:53.014284: step 3558, loss 2.34865e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:53.266730: step 3559, loss 0.0358489, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:39:53.500468: step 3560, loss 6.99772e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:53.736354: step 3561, loss 1.58324e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:53.971111: step 3562, loss 0.0200779, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:39:54.206338: step 3563, loss 0.00889261, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:54.440879: step 3564, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:54.675027: step 3565, loss 0.22626, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:54.910074: step 3566, loss 2.61307e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:55.166381: step 3567, loss 0.125972, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:39:55.402751: step 3568, loss 0.215333, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:39:55.634871: step 3569, loss 0.00181707, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:55.890742: step 3570, loss 0.000579767, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:56.127882: step 3571, loss 0.00462808, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:56.389815: step 3572, loss 0.089993, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:39:56.649444: step 3573, loss 0.000326213, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:56.886221: step 3574, loss 3.03608e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:57.118804: step 3575, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:57.325373: step 3576, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:57.580903: step 3577, loss 0.000997678, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:57.828121: step 3578, loss 0.189652, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:39:58.063314: step 3579, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:58.295241: step 3580, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:58.549318: step 3581, loss 7.86624e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:58.783447: step 3582, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:59.018835: step 3583, loss 5.05895e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:59.300122: step 3584, loss 0.000664686, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:39:59.536714: step 3585, loss 0.0574078, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:39:59.845428: step 3586, loss 0.0271237, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:40:00.080426: step 3587, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:00.339560: step 3588, loss 0.00217277, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:00.577223: step 3589, loss 0.000277157, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:00.806683: step 3590, loss 8.81531e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:01.064954: step 3591, loss 0.23736, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:40:01.301311: step 3592, loss 7.71118e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:01.560103: step 3593, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:01.797842: step 3594, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:02.051106: step 3595, loss 0.0285888, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:40:02.287872: step 3596, loss 4.84281e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:02.522123: step 3597, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:02.773578: step 3598, loss 0.011979, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:40:02.998443: step 3599, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:03.223347: step 3600, loss 3.62563e-06, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:40:04.303119: step 3600, loss 7.92724, acc 0.590822, precision 0.138947, recall 0.776471, F1 0.235714\n",
      "Saved model checkpoint to /data/zjdou/jupyter/root/DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/runs/explicit_1602339881/checkpoints/explicit-model-3600\n",
      "\n",
      "\n",
      "temporal 2020-10-10T22:40:04.744085: step 3601, loss 2.52055e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:04.986028: step 3602, loss 0.272914, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:40:05.205590: step 3603, loss 0.211508, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:40:05.436141: step 3604, loss 4.78014e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:05.682008: step 3605, loss 0.12106, acc 0.96875, precision 0.965517, recall 0.965517\n",
      "temporal 2020-10-10T22:40:05.957415: step 3606, loss 2.25379e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:06.200963: step 3607, loss 0.000140137, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:06.437908: step 3608, loss 0.0315283, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:40:06.680354: step 3609, loss 0.0211531, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:40:06.919414: step 3610, loss 1.75454e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:07.170997: step 3611, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:07.406111: step 3612, loss 0.014015, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:07.638324: step 3613, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:07.892333: step 3614, loss 2.45488e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:08.127489: step 3615, loss 0.00033329, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:08.361221: step 3616, loss 2.79397e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:08.594149: step 3617, loss 0.252214, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:40:08.829361: step 3618, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:09.062808: step 3619, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:09.320597: step 3620, loss 7.42842e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:09.581626: step 3621, loss 0.171621, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:40:09.820521: step 3622, loss 0.0145391, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:10.056444: step 3623, loss 0.026203, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:40:10.283111: step 3624, loss 0.00027609, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:10.517468: step 3625, loss 1.96742e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:10.751251: step 3626, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:10.985593: step 3627, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:11.217270: step 3628, loss 0.0100945, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:11.451987: step 3629, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:11.711911: step 3630, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:11.944690: step 3631, loss 1.52736e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:12.178883: step 3632, loss 0.124473, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:40:12.476771: step 3633, loss 6.14672e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:12.710200: step 3634, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:12.963108: step 3635, loss 3.12921e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:13.207132: step 3636, loss 0.0144219, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:13.439519: step 3637, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:13.697723: step 3638, loss 0.0491681, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:40:13.929719: step 3639, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:14.163458: step 3640, loss 0.0247033, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:40:14.435102: step 3641, loss 8.62167e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:14.670580: step 3642, loss 0.313279, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:40:14.906056: step 3643, loss 0.221997, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:40:15.163887: step 3644, loss 4.86261e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:15.403649: step 3645, loss 0.000415288, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:15.633143: step 3646, loss 2.72493e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:15.867598: step 3647, loss 7.98895e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:16.069704: step 3648, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:16.302212: step 3649, loss 0.00935294, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:16.584422: step 3650, loss 2.42144e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:16.820526: step 3651, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:17.077881: step 3652, loss 0.0838883, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:40:17.322673: step 3653, loss 0.000443372, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:17.586118: step 3654, loss 0.0756829, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:40:17.822420: step 3655, loss 0.00432084, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:18.055647: step 3656, loss 8.41896e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:18.309591: step 3657, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:18.552572: step 3658, loss 0.0154462, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:40:18.792263: step 3659, loss 0.180216, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:40:19.052496: step 3660, loss 4.47034e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:19.289490: step 3661, loss 3.72529e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:19.555529: step 3662, loss 1.52777e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:19.822249: step 3663, loss 0.0116671, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:40:20.057754: step 3664, loss 0.0165987, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:40:20.295314: step 3665, loss 1.86264e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:20.533174: step 3666, loss 3.47165e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:20.764233: step 3667, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:21.020857: step 3668, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:21.260379: step 3669, loss 0.0317567, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:40:21.508214: step 3670, loss 0.145229, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:21.744884: step 3671, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:21.953160: step 3672, loss 7.94727e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:22.188793: step 3673, loss 0.00336165, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:22.420988: step 3674, loss 3.69058e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:22.680112: step 3675, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:22.916541: step 3676, loss 2.0097e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:23.178162: step 3677, loss 0.0217089, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:23.410853: step 3678, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:23.647304: step 3679, loss 1.86264e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:23.881664: step 3680, loss 1.02445e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:24.141264: step 3681, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:24.376019: step 3682, loss 0.10376, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:40:24.610816: step 3683, loss 1.65209e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:24.864302: step 3684, loss 1.11197e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:25.100234: step 3685, loss 2.22757e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:25.357469: step 3686, loss 2.14203e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:25.593591: step 3687, loss 5.90347e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:25.829493: step 3688, loss 0.140056, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:40:26.090615: step 3689, loss 3.38766e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:26.325561: step 3690, loss 0.0643065, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:40:26.560961: step 3691, loss 0.0648699, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:40:26.798357: step 3692, loss 0.000306636, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:27.033954: step 3693, loss 0.0285427, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:27.292080: step 3694, loss 0.160821, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:40:27.546248: step 3695, loss 0.00228658, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:27.749942: step 3696, loss 0.000252324, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:27.984620: step 3697, loss 0.0142809, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:40:28.244843: step 3698, loss 0.16173, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:40:28.496958: step 3699, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:28.730900: step 3700, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:40:29.866104: step 3700, loss 6.1085, acc 0.666348, precision 0.129213, recall 0.541176, F1 0.208617\n",
      "\n",
      "temporal 2020-10-10T22:40:30.110297: step 3701, loss 0.00549979, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:30.372591: step 3702, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:30.619025: step 3703, loss 1.24607e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:30.881022: step 3704, loss 0.0917036, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:40:31.118503: step 3705, loss 0.00181928, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:31.351680: step 3706, loss 5.17807e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:31.587115: step 3707, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:31.843198: step 3708, loss 0.0548894, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:40:32.076294: step 3709, loss 0.000106307, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:32.336637: step 3710, loss 0.000889745, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:32.572553: step 3711, loss 0.370597, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:40:32.808354: step 3712, loss 0.0019359, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:33.066342: step 3713, loss 7.14365e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:33.301805: step 3714, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:33.536859: step 3715, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:33.794023: step 3716, loss 8.89291e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:34.027129: step 3717, loss 8.38188e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:34.260537: step 3718, loss 0.077611, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:40:34.495787: step 3719, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:34.699500: step 3720, loss 0.100796, acc 0.958333, precision 0.95, recall 0.95\n",
      "temporal 2020-10-10T22:40:34.933119: step 3721, loss 2.77696e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:35.168605: step 3722, loss 2.0693e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:35.423802: step 3723, loss 0.176829, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:40:35.658712: step 3724, loss 1.7695e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:35.913121: step 3725, loss 3.1665e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:36.148241: step 3726, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:36.381997: step 3727, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:36.618186: step 3728, loss 0.000142054, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:36.853057: step 3729, loss 8.12398e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:37.088902: step 3730, loss 2.19791e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:37.341832: step 3731, loss 0.0731542, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:40:37.575368: step 3732, loss 0.000127603, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:37.829035: step 3733, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:38.062286: step 3734, loss 6.09553e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:38.301427: step 3735, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:38.540340: step 3736, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:38.795375: step 3737, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:39.033351: step 3738, loss 0.000104337, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:39.267651: step 3739, loss 0.049488, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:40:39.527678: step 3740, loss 2.34303e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:39.780921: step 3741, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:40.037732: step 3742, loss 0.0397222, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:40:40.298502: step 3743, loss 0.060766, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:40:40.502996: step 3744, loss 0.0100241, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:40.738499: step 3745, loss 4.97318e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:40.972113: step 3746, loss 6.70551e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:41.224484: step 3747, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:41.478792: step 3748, loss 8.53234e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:41.714147: step 3749, loss 0.00443452, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:41.948676: step 3750, loss 6.22113e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:42.181988: step 3751, loss 0.107518, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:40:42.416254: step 3752, loss 0.0657463, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:40:42.650173: step 3753, loss 8.31138e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:42.910599: step 3754, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:43.143831: step 3755, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:43.397173: step 3756, loss 0.438626, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:40:43.678428: step 3757, loss 0.413373, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:40:43.977625: step 3758, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:44.216863: step 3759, loss 7.78578e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:44.455580: step 3760, loss 8.77291e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:44.692123: step 3761, loss 0.0773158, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:40:44.949194: step 3762, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:45.185712: step 3763, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:45.420955: step 3764, loss 3.10286e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:45.660265: step 3765, loss 0.000290317, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:45.919156: step 3766, loss 0.0154694, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:46.156799: step 3767, loss 0.116553, acc 0.96875, precision 1, recall 0.945946\n",
      "temporal 2020-10-10T22:40:46.357706: step 3768, loss 2.78153e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:46.595394: step 3769, loss 2.60194e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:46.854865: step 3770, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:47.087217: step 3771, loss 3.31548e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:47.343341: step 3772, loss 0.0972551, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:40:47.588804: step 3773, loss 4.65661e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:47.836597: step 3774, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:48.069776: step 3775, loss 0.0706354, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:40:48.302336: step 3776, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:48.534139: step 3777, loss 0.00351883, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:48.791925: step 3778, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:49.029795: step 3779, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:49.261969: step 3780, loss 9.83446e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:49.508512: step 3781, loss 0.00020559, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:49.743461: step 3782, loss 0.000259209, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:49.979134: step 3783, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:50.241817: step 3784, loss 0.14197, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:40:50.479273: step 3785, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:50.714830: step 3786, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:50.949524: step 3787, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:51.180014: step 3788, loss 6.89177e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:51.414383: step 3789, loss 0.0433511, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:40:51.666571: step 3790, loss 1.04491e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:51.900274: step 3791, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:52.106090: step 3792, loss 0.140713, acc 0.979167, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:40:52.339783: step 3793, loss 3.66154e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:52.576918: step 3794, loss 0.300653, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:40:52.819610: step 3795, loss 6.40742e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:53.054791: step 3796, loss 0.0280433, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:40:53.306265: step 3797, loss 0.0966622, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:40:53.553247: step 3798, loss 0.00218529, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:53.811764: step 3799, loss 7.48049e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:54.088746: step 3800, loss 2.44928e-06, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:40:55.280215: step 3800, loss 5.15241, acc 0.717973, precision 0.137931, recall 0.470588, F1 0.213333\n",
      "\n",
      "temporal 2020-10-10T22:40:55.505688: step 3801, loss 0.00161663, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:55.818852: step 3802, loss 0.0795117, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:40:56.086826: step 3803, loss 5.51333e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:56.333516: step 3804, loss 3.01746e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:56.570994: step 3805, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:56.805635: step 3806, loss 0.282702, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:40:57.064582: step 3807, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:57.299459: step 3808, loss 6.33298e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:57.551873: step 3809, loss 2.99883e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:57.811513: step 3810, loss 0.000112647, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:58.047199: step 3811, loss 0.111522, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:40:58.306699: step 3812, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:58.540815: step 3813, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:58.773597: step 3814, loss 1.28522e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:59.006312: step 3815, loss 2.52927e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:59.234377: step 3816, loss 0.434578, acc 0.979167, precision 1, recall 0.952381\n",
      "temporal 2020-10-10T22:40:59.492869: step 3817, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:40:59.756306: step 3818, loss 2.18659e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:00.007379: step 3819, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:00.244881: step 3820, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:00.482647: step 3821, loss 0.00242712, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:00.737850: step 3822, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:00.977579: step 3823, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:01.209577: step 3824, loss 0.0308603, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:41:01.448357: step 3825, loss 1.0516e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:01.680813: step 3826, loss 4.50798e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:01.920115: step 3827, loss 0.0484248, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:41:02.155024: step 3828, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:02.388584: step 3829, loss 1.52596e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:02.618699: step 3830, loss 0.00282712, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:02.854432: step 3831, loss 0.109166, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:41:03.118258: step 3832, loss 2.11602e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:03.352919: step 3833, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:03.612823: step 3834, loss 0.00189914, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:03.846425: step 3835, loss 5.77419e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:04.100453: step 3836, loss 0.0663502, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:04.335010: step 3837, loss 1.15484e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:04.587719: step 3838, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:04.822579: step 3839, loss 0.0129281, acc 0.984375, precision 1, recall 0.975\n",
      "temporal 2020-10-10T22:41:05.026102: step 3840, loss 2.73188e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:05.259430: step 3841, loss 2.44264e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:05.511872: step 3842, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:05.746444: step 3843, loss 2.57243e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:06.008232: step 3844, loss 8.84732e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:06.243953: step 3845, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:06.505036: step 3846, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:06.738762: step 3847, loss 0.0716189, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:06.971674: step 3848, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:07.204197: step 3849, loss 1.99663e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:07.442701: step 3850, loss 3.74387e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:07.676989: step 3851, loss 0.325034, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:07.928053: step 3852, loss 0.00049543, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:08.161596: step 3853, loss 3.16649e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:08.393659: step 3854, loss 5.2154e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:08.646493: step 3855, loss 2.09971e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:08.901780: step 3856, loss 0.0748126, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:41:09.137999: step 3857, loss 0.192187, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:41:09.372578: step 3858, loss 0.0417259, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:41:09.606488: step 3859, loss 0.000113904, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:09.841006: step 3860, loss 0.0682742, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:10.083385: step 3861, loss 1.35616e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:10.340930: step 3862, loss 0.0955582, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:41:10.576885: step 3863, loss 2.79397e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:10.780291: step 3864, loss 2.48353e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:11.016981: step 3865, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:11.271728: step 3866, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:11.506266: step 3867, loss 0.0028566, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:11.742450: step 3868, loss 6.94751e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:11.977384: step 3869, loss 0.0215853, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:41:12.232771: step 3870, loss 8.00935e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:12.494535: step 3871, loss 0.196989, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:41:12.742750: step 3872, loss 4.09781e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:13.002367: step 3873, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:13.258295: step 3874, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:13.493269: step 3875, loss 0.069489, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:41:13.727216: step 3876, loss 0.000436002, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:13.994708: step 3877, loss 2.70081e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:14.230337: step 3878, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:14.487651: step 3879, loss 0.134154, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:41:14.721964: step 3880, loss 7.7411e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:14.976442: step 3881, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:15.212017: step 3882, loss 1.88847e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:15.446498: step 3883, loss 0.166364, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:41:15.680127: step 3884, loss 0.000342625, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:15.933865: step 3885, loss 0.0673398, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:16.187668: step 3886, loss 1.43423e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:16.442544: step 3887, loss 1.86067e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:16.670685: step 3888, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:16.907796: step 3889, loss 4.52619e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:17.146264: step 3890, loss 0.00329415, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:17.380039: step 3891, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:17.614560: step 3892, loss 3.16432e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:17.848847: step 3893, loss 0.0302417, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:41:18.085169: step 3894, loss 0.0723982, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:41:18.341120: step 3895, loss 6.37585e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:18.578085: step 3896, loss 0.0026342, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:18.811579: step 3897, loss 0.0987321, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:41:19.066018: step 3898, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:19.300962: step 3899, loss 7.77783e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:19.540024: step 3900, loss 1.93714e-07, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:41:20.727746: step 3900, loss 5.86256, acc 0.688336, precision 0.133739, recall 0.517647, F1 0.21256\n",
      "\n",
      "temporal 2020-10-10T22:41:20.930000: step 3901, loss 0.00154988, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:21.159772: step 3902, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:21.380588: step 3903, loss 1.3485e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:21.623577: step 3904, loss 9.12694e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:21.851981: step 3905, loss 4.35853e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:22.076881: step 3906, loss 0.18507, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:41:22.324196: step 3907, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:22.531936: step 3908, loss 1.65775e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:22.749975: step 3909, loss 1.71541e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:22.968796: step 3910, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:23.192601: step 3911, loss 1.49408e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:23.372781: step 3912, loss 3.87427e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:23.611457: step 3913, loss 0.000418156, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:23.831490: step 3914, loss 0.101934, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:41:24.054634: step 3915, loss 4.41893e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:24.294601: step 3916, loss 6.64952e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:24.514825: step 3917, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:24.734287: step 3918, loss 3.93013e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:24.953881: step 3919, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:25.197420: step 3920, loss 0.100882, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:25.420197: step 3921, loss 0.156607, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:41:25.640156: step 3922, loss 4.11641e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:25.855334: step 3923, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:26.079454: step 3924, loss 0.0220949, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:26.298865: step 3925, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:26.520473: step 3926, loss 0.0302166, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:41:26.743779: step 3927, loss 0.00010514, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:26.964417: step 3928, loss 0.000128013, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:27.184466: step 3929, loss 1.65764e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:27.407643: step 3930, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:27.627534: step 3931, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:27.842490: step 3932, loss 0.389171, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:41:28.052619: step 3933, loss 1.08033e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:28.293190: step 3934, loss 6.16228e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:28.514038: step 3935, loss 6.70551e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:28.728789: step 3936, loss 7.94727e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:28.946417: step 3937, loss 8.59552e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:29.167685: step 3938, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:29.387557: step 3939, loss 0.0197277, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:41:29.598410: step 3940, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:29.817145: step 3941, loss 5.51412e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:30.025647: step 3942, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:30.270396: step 3943, loss 1.95009e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:30.490828: step 3944, loss 0.147215, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:41:30.719176: step 3945, loss 3.10195e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:30.938343: step 3946, loss 1.86264e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:31.186490: step 3947, loss 1.6939e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:31.411127: step 3948, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:31.636357: step 3949, loss 1.32397e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:31.859333: step 3950, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:32.088344: step 3951, loss 3.16649e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:32.303866: step 3952, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:32.547345: step 3953, loss 2.6077e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:32.769864: step 3954, loss 4.47034e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:32.991452: step 3955, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:33.239532: step 3956, loss 4.89593e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:33.495147: step 3957, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:33.719291: step 3958, loss 0.000315311, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:33.966815: step 3959, loss 4.29312e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:34.160178: step 3960, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:34.385797: step 3961, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:34.615984: step 3962, loss 0.0765505, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:41:34.854840: step 3963, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:35.084156: step 3964, loss 0.0873355, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:41:35.334912: step 3965, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:35.565255: step 3966, loss 3.53902e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:35.796140: step 3967, loss 0.206529, acc 0.96875, precision 0.9375, recall 1\n",
      "temporal 2020-10-10T22:41:36.051470: step 3968, loss 3.35276e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:36.282745: step 3969, loss 0.020744, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:41:36.539276: step 3970, loss 3.89288e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:36.763628: step 3971, loss 5.08143e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:37.009227: step 3972, loss 8.84135e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:37.255318: step 3973, loss 0.00027282, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:37.485207: step 3974, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:37.706279: step 3975, loss 5.10053e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:37.917051: step 3976, loss 0.110402, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:41:38.141524: step 3977, loss 1.58324e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:38.359769: step 3978, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:38.580657: step 3979, loss 6.77989e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:38.802794: step 3980, loss 1.50681e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:39.031625: step 3981, loss 7.33865e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:39.313110: step 3982, loss 0.00803375, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:39.554717: step 3983, loss 1.37161e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:39.782493: step 3984, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:40.017374: step 3985, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:40.249308: step 3986, loss 0.000908346, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:40.499400: step 3987, loss 0.000191787, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:40.735123: step 3988, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:40.969334: step 3989, loss 2.78261e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:41.226644: step 3990, loss 0.0974091, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:41.482413: step 3991, loss 0.0648711, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:41:41.742044: step 3992, loss 3.72529e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:41.983143: step 3993, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:42.252620: step 3994, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:42.488463: step 3995, loss 0.000566405, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:42.722005: step 3996, loss 0.000252784, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:42.970087: step 3997, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:43.201959: step 3998, loss 4.65285e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:43.436094: step 3999, loss 0.41957, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:41:43.669046: step 4000, loss 8.97634e-05, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:41:44.812947: step 4000, loss 5.33683, acc 0.710325, precision 0.136667, recall 0.482353, F1 0.212987\n",
      "\n",
      "temporal 2020-10-10T22:41:45.017695: step 4001, loss 0.0512437, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:41:45.265512: step 4002, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:45.501021: step 4003, loss 0.042835, acc 0.984375, precision 1, recall 0.956522\n",
      "temporal 2020-10-10T22:41:45.760309: step 4004, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:45.993920: step 4005, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:46.240435: step 4006, loss 7.82309e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:46.493921: step 4007, loss 1.15484e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:46.696890: step 4008, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:46.932695: step 4009, loss 1.17346e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:47.164280: step 4010, loss 0.0279859, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:47.398713: step 4011, loss 0.000360866, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:47.632969: step 4012, loss 2.86085e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:47.865770: step 4013, loss 1.02445e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:48.099749: step 4014, loss 0.188731, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:41:48.370232: step 4015, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:48.625316: step 4016, loss 0.0452116, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:41:48.876336: step 4017, loss 0.000281784, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:49.114452: step 4018, loss 0.0129202, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:41:49.388013: step 4019, loss 5.24619e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:49.629317: step 4020, loss 0.230599, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:41:49.865718: step 4021, loss 0.0817266, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:41:50.122956: step 4022, loss 0.045685, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:41:50.378540: step 4023, loss 0.00067655, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:50.637658: step 4024, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:50.873638: step 4025, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:51.111712: step 4026, loss 0.000662927, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:51.345868: step 4027, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:51.580349: step 4028, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:51.814813: step 4029, loss 0.00950753, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:52.047730: step 4030, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:52.301905: step 4031, loss 0.0203931, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:41:52.529748: step 4032, loss 0.355744, acc 0.979167, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:41:52.766336: step 4033, loss 0.00540622, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:53.022824: step 4034, loss 0.00151549, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:53.262252: step 4035, loss 0.000565805, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:53.496558: step 4036, loss 0.000628975, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:53.751107: step 4037, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:53.985208: step 4038, loss 1.15249e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:54.221638: step 4039, loss 0.000123128, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:54.459394: step 4040, loss 0.203974, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:41:54.694055: step 4041, loss 0.178501, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:54.928908: step 4042, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:55.172717: step 4043, loss 0.458104, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:41:55.407780: step 4044, loss 1.89989e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:55.660751: step 4045, loss 0.0416798, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:41:55.895588: step 4046, loss 4.03979e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:56.132674: step 4047, loss 0.00329443, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:56.368314: step 4048, loss 0.117678, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:41:56.602103: step 4049, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:56.837670: step 4050, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:57.072031: step 4051, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:57.306868: step 4052, loss 1.24059e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:57.542844: step 4053, loss 2.51456e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:57.800306: step 4054, loss 5.26706e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:58.062820: step 4055, loss 0.00220187, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:58.264231: step 4056, loss 1.24176e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:58.499962: step 4057, loss 0.0343986, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:41:58.730259: step 4058, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:58.965937: step 4059, loss 0.00988005, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:59.197358: step 4060, loss 0.0655128, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:41:59.431301: step 4061, loss 0.0235909, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:41:59.663419: step 4062, loss 0.000236663, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:41:59.896853: step 4063, loss 0.0783282, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:42:00.132798: step 4064, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:00.366627: step 4065, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:00.627379: step 4066, loss 2.64493e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:00.860420: step 4067, loss 1.253e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:01.091447: step 4068, loss 0.0197443, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:42:01.323600: step 4069, loss 0.118952, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:42:01.577340: step 4070, loss 0.000577833, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:01.835598: step 4071, loss 6.30263e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:02.068923: step 4072, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:02.304345: step 4073, loss 0.00015765, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:02.538115: step 4074, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:02.770602: step 4075, loss 0.000196724, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:03.028888: step 4076, loss 3.91155e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:03.287799: step 4077, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:03.522743: step 4078, loss 1.03746e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:03.778697: step 4079, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:03.986154: step 4080, loss 3.31123e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:04.219882: step 4081, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:04.479179: step 4082, loss 1.60187e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:04.716454: step 4083, loss 0.226823, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:42:04.974706: step 4084, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:05.237121: step 4085, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:05.474792: step 4086, loss 8.19543e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:05.709107: step 4087, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:05.962770: step 4088, loss 0.0235864, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:42:06.197712: step 4089, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:06.446792: step 4090, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:06.707038: step 4091, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:06.940775: step 4092, loss 0.0914492, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:42:07.175404: step 4093, loss 0.000188443, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:07.434948: step 4094, loss 1.45926e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:07.672042: step 4095, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:07.909208: step 4096, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:08.142709: step 4097, loss 0.0177717, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:42:08.378649: step 4098, loss 0.0644743, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:42:08.614529: step 4099, loss 3.58526e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:08.873900: step 4100, loss 0.0417572, acc 0.984375, precision 0.967742, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:42:09.969966: step 4100, loss 6.90581, acc 0.656788, precision 0.13172, recall 0.576471, F1 0.214442\n",
      "\n",
      "temporal 2020-10-10T22:42:10.175545: step 4101, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:10.410308: step 4102, loss 0.251781, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:42:10.665196: step 4103, loss 0.0556413, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:42:10.870221: step 4104, loss 2.88087e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:11.104935: step 4105, loss 0.0881446, acc 0.984375, precision 0.956522, recall 1\n",
      "temporal 2020-10-10T22:42:11.358681: step 4106, loss 0.000506192, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:11.591747: step 4107, loss 0.000187372, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:11.827012: step 4108, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:12.062022: step 4109, loss 4.06052e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:12.292979: step 4110, loss 8.88788e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:12.529581: step 4111, loss 1.05818e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:12.764549: step 4112, loss 2.4828e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:13.000172: step 4113, loss 1.32056e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:13.236857: step 4114, loss 0.183333, acc 0.96875, precision 0.944444, recall 1\n",
      "temporal 2020-10-10T22:42:13.495910: step 4115, loss 6.27223e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:13.733494: step 4116, loss 2.16011e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:13.969233: step 4117, loss 6.18392e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:14.201745: step 4118, loss 0.00770394, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:14.434845: step 4119, loss 0.00135403, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:14.666815: step 4120, loss 0.0565008, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:42:14.899993: step 4121, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:15.135436: step 4122, loss 0.00428903, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:15.370364: step 4123, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:15.624159: step 4124, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:15.856672: step 4125, loss 0.0852158, acc 0.984375, precision 1, recall 0.958333\n",
      "temporal 2020-10-10T22:42:16.091908: step 4126, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:16.324238: step 4127, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:16.528753: step 4128, loss 6.05655e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:16.762109: step 4129, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:17.007219: step 4130, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:17.239437: step 4131, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:17.493958: step 4132, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:17.728501: step 4133, loss 0.0741714, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:42:17.989955: step 4134, loss 2.42144e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:18.228881: step 4135, loss 5.49039e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:18.489876: step 4136, loss 2.06372e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:18.724455: step 4137, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:18.975843: step 4138, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:19.232876: step 4139, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:19.468391: step 4140, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:19.706762: step 4141, loss 0.00235636, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:19.943048: step 4142, loss 2.57043e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:20.175410: step 4143, loss 1.3485e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:20.409651: step 4144, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:20.647696: step 4145, loss 0.0139714, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:42:20.878578: step 4146, loss 0.127711, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:42:21.113322: step 4147, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:21.345792: step 4148, loss 0.000115716, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:21.602321: step 4149, loss 5.89602e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:21.840814: step 4150, loss 7.59211e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:22.097122: step 4151, loss 0.0812664, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:42:22.327205: step 4152, loss 1.66396e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:22.581873: step 4153, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:22.817548: step 4154, loss 0.00463454, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:23.052595: step 4155, loss 0.00654352, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:23.299588: step 4156, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:23.534215: step 4157, loss 0.000299425, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:23.784623: step 4158, loss 0.0319005, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:42:24.068836: step 4159, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:24.304004: step 4160, loss 1.75088e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:24.536886: step 4161, loss 3.53902e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:24.770755: step 4162, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:25.009319: step 4163, loss 0.0248356, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:42:25.244382: step 4164, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:25.482993: step 4165, loss 9.36882e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:25.740481: step 4166, loss 3.16647e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:25.974587: step 4167, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:26.235169: step 4168, loss 0.190774, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:42:26.469791: step 4169, loss 1.09147e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:26.702769: step 4170, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:26.938666: step 4171, loss 0.0247034, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:42:27.229505: step 4172, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:27.464749: step 4173, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:27.697232: step 4174, loss 0.132744, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:42:27.940859: step 4175, loss 0.000415568, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:28.141010: step 4176, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:28.378568: step 4177, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:28.612025: step 4178, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:28.866930: step 4179, loss 4.09781e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:29.123847: step 4180, loss 1.00393e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:29.360457: step 4181, loss 0.128971, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:42:29.597546: step 4182, loss 0.0878013, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:42:29.832048: step 4183, loss 3.21279e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:30.061640: step 4184, loss 0.000854626, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:30.294713: step 4185, loss 7.63684e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:30.553456: step 4186, loss 0.131643, acc 0.984375, precision 0.97561, recall 1\n",
      "temporal 2020-10-10T22:42:30.794103: step 4187, loss 0.353768, acc 0.953125, precision 0.896552, recall 1\n",
      "temporal 2020-10-10T22:42:31.027726: step 4188, loss 0.119449, acc 0.96875, precision 0.947368, recall 1\n",
      "temporal 2020-10-10T22:42:31.266217: step 4189, loss 0.090835, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:42:31.524663: step 4190, loss 2.48085e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:31.777383: step 4191, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:32.035578: step 4192, loss 0.158632, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:42:32.281365: step 4193, loss 0.210024, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:42:32.518228: step 4194, loss 0.0403032, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:42:32.753575: step 4195, loss 0.00131995, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:32.987107: step 4196, loss 1.34072e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:33.220545: step 4197, loss 2.18101e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:33.453156: step 4198, loss 2.38214e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:33.687444: step 4199, loss 0.186837, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:42:33.892810: step 4200, loss 3.77905e-05, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:42:35.059089: step 4200, loss 5.48812, acc 0.723709, precision 0.133094, recall 0.435294, F1 0.203857\n",
      "\n",
      "temporal 2020-10-10T22:42:35.290509: step 4201, loss 0.149323, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:42:35.506425: step 4202, loss 3.49579e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:35.740363: step 4203, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:35.972494: step 4204, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:36.205824: step 4205, loss 1.99302e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:36.442142: step 4206, loss 4.22817e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:36.678471: step 4207, loss 8.41149e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:36.935617: step 4208, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:37.170120: step 4209, loss 0.00281679, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:37.404204: step 4210, loss 9.3132e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:37.638327: step 4211, loss 1.49564e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:37.870642: step 4212, loss 0.0718631, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:42:38.128155: step 4213, loss 0.00198178, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:38.386963: step 4214, loss 9.55991e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:38.624127: step 4215, loss 7.86016e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:38.873816: step 4216, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:39.133986: step 4217, loss 2.38417e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:39.382251: step 4218, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:39.635948: step 4219, loss 1.56461e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:39.870856: step 4220, loss 5.58793e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:40.103853: step 4221, loss 9.8746e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:40.338422: step 4222, loss 6.70551e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:40.575189: step 4223, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:40.777161: step 4224, loss 0.000510639, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:41.011917: step 4225, loss 0.155188, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:42:41.271388: step 4226, loss 0.235739, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:42:41.503421: step 4227, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:41.760859: step 4228, loss 0.0131457, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:42:41.996834: step 4229, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:42.259194: step 4230, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:42.513504: step 4231, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:42.766958: step 4232, loss 0.0341786, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:42:43.003178: step 4233, loss 3.41498e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:43.264054: step 4234, loss 4.2686e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:43.499668: step 4235, loss 5.76741e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:43.733395: step 4236, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:43.985883: step 4237, loss 7.2643e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:44.220412: step 4238, loss 8.36305e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:44.470268: step 4239, loss 1.7695e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:44.701961: step 4240, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:44.937654: step 4241, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:45.172116: step 4242, loss 4.84287e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:45.405945: step 4243, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:45.641764: step 4244, loss 8.52149e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:45.899230: step 4245, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:46.133832: step 4246, loss 0.111751, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:42:46.367316: step 4247, loss 0.114965, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:42:46.569506: step 4248, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:46.831919: step 4249, loss 9.87199e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:47.066627: step 4250, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:47.305612: step 4251, loss 5.64371e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:47.541129: step 4252, loss 8.21963e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:47.772758: step 4253, loss 0.215813, acc 0.984375, precision 0.970588, recall 1\n",
      "temporal 2020-10-10T22:42:48.002569: step 4254, loss 0.00168581, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:48.233958: step 4255, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:48.487699: step 4256, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:48.720863: step 4257, loss 1.67818e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:48.955841: step 4258, loss 0.250591, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:42:49.214279: step 4259, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:49.464628: step 4260, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:49.697567: step 4261, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:49.931457: step 4262, loss 0.183718, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:42:50.164519: step 4263, loss 1.67638e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:50.394430: step 4264, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:50.626706: step 4265, loss 2.21653e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:50.860685: step 4266, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:51.113097: step 4267, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:51.347880: step 4268, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:51.605799: step 4269, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:51.839508: step 4270, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:52.072356: step 4271, loss 0.000176134, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:52.299150: step 4272, loss 6.45716e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:52.530818: step 4273, loss 0.00270237, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:52.810121: step 4274, loss 0.0018345, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:53.070933: step 4275, loss 1.32989e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:53.305836: step 4276, loss 0.194841, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:42:53.554911: step 4277, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:53.793625: step 4278, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:54.052824: step 4279, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:54.288447: step 4280, loss 0.0138536, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:42:54.520609: step 4281, loss 0.0052111, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:54.751052: step 4282, loss 9.90896e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:54.982721: step 4283, loss 8.92182e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:55.216746: step 4284, loss 0.0195887, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:42:55.450185: step 4285, loss 6.68676e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:55.681638: step 4286, loss 5.02913e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:55.938152: step 4287, loss 0.328723, acc 0.96875, precision 0.964286, recall 0.964286\n",
      "temporal 2020-10-10T22:42:56.173979: step 4288, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:56.421763: step 4289, loss 2.60768e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:56.675772: step 4290, loss 0.0764159, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:42:56.910388: step 4291, loss 3.14384e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:57.148339: step 4292, loss 0.212224, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:42:57.381745: step 4293, loss 3.21627e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:57.613510: step 4294, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:57.845668: step 4295, loss 0.0135016, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:58.073007: step 4296, loss 1.43791e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:58.308022: step 4297, loss 8.75442e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:58.540932: step 4298, loss 0.00015568, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:58.772448: step 4299, loss 0.00214879, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:42:59.003910: step 4300, loss 8.56814e-08, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:43:00.156391: step 4300, loss 10.2195, acc 0.59847, precision 0.130243, recall 0.694118, F1 0.219331\n",
      "\n",
      "temporal 2020-10-10T22:43:00.366209: step 4301, loss 9.3502e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:00.599428: step 4302, loss 3.81838e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:00.863320: step 4303, loss 6.72401e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:01.120388: step 4304, loss 3.63212e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:01.377371: step 4305, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:01.612205: step 4306, loss 5.62419e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:01.896319: step 4307, loss 0.000450872, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:02.150164: step 4308, loss 0.000358569, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:02.385522: step 4309, loss 8.6761e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:02.617871: step 4310, loss 1.81411e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:02.869715: step 4311, loss 0.00332978, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:03.105996: step 4312, loss 0.00177735, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:03.337815: step 4313, loss 2.62631e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:03.586217: step 4314, loss 7.07804e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:03.824756: step 4315, loss 0.00126592, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:04.082499: step 4316, loss 0.00338414, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:04.336336: step 4317, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:04.577583: step 4318, loss 1.79791e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:04.813042: step 4319, loss 0.000324576, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:05.015358: step 4320, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:05.250624: step 4321, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:05.510787: step 4322, loss 1.04118e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:05.768373: step 4323, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:06.002271: step 4324, loss 0.00114733, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:06.237140: step 4325, loss 0.0585774, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:43:06.497196: step 4326, loss 4.2468e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:06.732400: step 4327, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:06.991953: step 4328, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:07.250157: step 4329, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:07.505445: step 4330, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:07.741301: step 4331, loss 1.3411e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:07.983193: step 4332, loss 1.18646e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:08.241023: step 4333, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:08.479383: step 4334, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:08.735835: step 4335, loss 4.54481e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:08.970889: step 4336, loss 0.00100431, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:09.228055: step 4337, loss 0.000106021, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:09.462910: step 4338, loss 0.185507, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:43:09.702890: step 4339, loss 2.27226e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:09.935697: step 4340, loss 0.211196, acc 0.96875, precision 1, recall 0.95\n",
      "temporal 2020-10-10T22:43:10.166275: step 4341, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:10.434772: step 4342, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:10.692318: step 4343, loss 5.53667e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:10.895770: step 4344, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:11.148613: step 4345, loss 0.0830715, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:43:11.382941: step 4346, loss 1.19499e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:11.617586: step 4347, loss 1.2144e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:11.852268: step 4348, loss 0.0576663, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:43:12.108803: step 4349, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:12.351096: step 4350, loss 0.000699887, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:12.606696: step 4351, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:12.864489: step 4352, loss 3.55174e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:13.157359: step 4353, loss 0.0566312, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:43:13.401163: step 4354, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:13.637160: step 4355, loss 0.0789138, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:43:13.869523: step 4356, loss 0.00290055, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:14.101502: step 4357, loss 0.0046006, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:14.353404: step 4358, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:14.588005: step 4359, loss 0.121745, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:43:14.822864: step 4360, loss 0.0478539, acc 0.984375, precision 0.967742, recall 1\n",
      "temporal 2020-10-10T22:43:15.079636: step 4361, loss 0.0412769, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:43:15.312425: step 4362, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:15.547327: step 4363, loss 2.06264e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:15.803101: step 4364, loss 2.94295e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:16.036417: step 4365, loss 0.654863, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:43:16.272761: step 4366, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:16.507053: step 4367, loss 0.0401501, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:43:16.718422: step 4368, loss 0.115196, acc 0.979167, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:43:16.972023: step 4369, loss 2.23823e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:17.206650: step 4370, loss 7.63666e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:17.441824: step 4371, loss 5.9231e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:17.698863: step 4372, loss 0.00098676, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:17.958347: step 4373, loss 0.00210805, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:18.194127: step 4374, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:18.428406: step 4375, loss 1.49012e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:18.665095: step 4376, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:18.898325: step 4377, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:19.137659: step 4378, loss 1.3932e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:19.371312: step 4379, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:19.610113: step 4380, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:19.844922: step 4381, loss 0.0344918, acc 0.984375, precision 1, recall 0.965517\n",
      "temporal 2020-10-10T22:43:20.071882: step 4382, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:20.309681: step 4383, loss 0.236626, acc 0.984375, precision 0.96, recall 1\n",
      "temporal 2020-10-10T22:43:20.543088: step 4384, loss 0.0172096, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:43:20.771766: step 4385, loss 0.000751755, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:21.027478: step 4386, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:21.258446: step 4387, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:21.494950: step 4388, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:21.752910: step 4389, loss 0.356893, acc 0.96875, precision 0.96875, recall 0.96875\n",
      "temporal 2020-10-10T22:43:21.986418: step 4390, loss 1.86935e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:22.223042: step 4391, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:22.493258: step 4392, loss 0.0841697, acc 0.979167, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:43:22.725996: step 4393, loss 4.09356e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:22.962935: step 4394, loss 0.0244311, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:43:23.210302: step 4395, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:23.464981: step 4396, loss 0.0928251, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:43:23.699465: step 4397, loss 0.0891945, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:43:23.932375: step 4398, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:24.162773: step 4399, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:24.394375: step 4400, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:43:25.552543: step 4400, loss 11.9403, acc 0.564054, precision 0.134122, recall 0.8, F1 0.22973\n",
      "\n",
      "temporal 2020-10-10T22:43:25.759460: step 4401, loss 3.61684e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:25.993172: step 4402, loss 6.82324e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:26.231398: step 4403, loss 0.0816598, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:43:26.482851: step 4404, loss 0.052454, acc 0.984375, precision 0.975, recall 1\n",
      "temporal 2020-10-10T22:43:26.744430: step 4405, loss 2.51455e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:26.982116: step 4406, loss 0.0971816, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:43:27.215825: step 4407, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:27.451270: step 4408, loss 6.54354e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:27.687492: step 4409, loss 1.63159e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:27.922084: step 4410, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:28.155439: step 4411, loss 4.65661e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:28.388764: step 4412, loss 0.216812, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:43:28.619074: step 4413, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:28.854878: step 4414, loss 1.97111e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:29.112954: step 4415, loss 1.18963e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:29.315681: step 4416, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:29.569524: step 4417, loss 2.04891e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:29.840241: step 4418, loss 0.0458616, acc 0.984375, precision 0.961538, recall 1\n",
      "temporal 2020-10-10T22:43:30.078429: step 4419, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:30.310702: step 4420, loss 0.217105, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:43:30.545327: step 4421, loss 1.68563e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:30.779005: step 4422, loss 0.00037434, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:31.013169: step 4423, loss 7.97754e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:31.247021: step 4424, loss 0.27704, acc 0.96875, precision 1, recall 0.941176\n",
      "temporal 2020-10-10T22:43:31.480881: step 4425, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:31.716552: step 4426, loss 0.0021729, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:31.978976: step 4427, loss 9.3132e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:32.214773: step 4428, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:32.453572: step 4429, loss 0.29607, acc 0.984375, precision 1, recall 0.974359\n",
      "temporal 2020-10-10T22:43:32.704872: step 4430, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:32.960338: step 4431, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:33.190872: step 4432, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:33.448470: step 4433, loss 7.79025e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:33.697960: step 4434, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:33.930293: step 4435, loss 2.1043e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:34.185486: step 4436, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:34.419334: step 4437, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:34.651668: step 4438, loss 0.000839737, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:34.887606: step 4439, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:35.094928: step 4440, loss 0.0783582, acc 0.979167, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:43:35.327895: step 4441, loss 8.56814e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:35.584959: step 4442, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:35.848384: step 4443, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:36.082866: step 4444, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:36.315210: step 4445, loss 0.000775708, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:36.550764: step 4446, loss 9.49946e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:36.787542: step 4447, loss 1.464e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:37.032135: step 4448, loss 0.107527, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:43:37.265248: step 4449, loss 0.000292729, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:37.496838: step 4450, loss 0.000237755, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:37.729231: step 4451, loss 1.10451e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:37.988154: step 4452, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:38.247668: step 4453, loss 0.138723, acc 0.984375, precision 1, recall 0.978723\n",
      "temporal 2020-10-10T22:43:38.501789: step 4454, loss 4.09242e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:38.759850: step 4455, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:39.019356: step 4456, loss 0.00087308, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:39.277729: step 4457, loss 0.0774851, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:43:39.519316: step 4458, loss 0.138796, acc 0.984375, precision 1, recall 0.973684\n",
      "temporal 2020-10-10T22:43:39.776265: step 4459, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:40.010997: step 4460, loss 8.54931e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:40.246223: step 4461, loss 1.50874e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:40.502171: step 4462, loss 0.000327108, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:40.764149: step 4463, loss 0.599978, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:43:40.990386: step 4464, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:41.226874: step 4465, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:41.484750: step 4466, loss 0.121493, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:43:41.736471: step 4467, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:41.992606: step 4468, loss 0.205901, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:43:42.249380: step 4469, loss 3.72857e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:42.483328: step 4470, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:42.741774: step 4471, loss 3.42723e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:42.981962: step 4472, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:43.218166: step 4473, loss 3.01908e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:43.475314: step 4474, loss 0.00661895, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:43.707101: step 4475, loss 0.00436347, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:43.951504: step 4476, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:44.224682: step 4477, loss 0.191358, acc 0.984375, precision 1, recall 0.972973\n",
      "temporal 2020-10-10T22:43:44.457828: step 4478, loss 0.201501, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:43:44.692889: step 4479, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:44.929914: step 4480, loss 1.58131e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:45.199444: step 4481, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:45.434676: step 4482, loss 0.154751, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:43:45.665814: step 4483, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:45.900044: step 4484, loss 0.0737606, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:43:46.131899: step 4485, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:46.364915: step 4486, loss 0.27458, acc 0.96875, precision 1, recall 0.935484\n",
      "temporal 2020-10-10T22:43:46.599990: step 4487, loss 1.52546e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:46.826610: step 4488, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:47.058669: step 4489, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:47.290836: step 4490, loss 7.17102e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:47.517913: step 4491, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:47.751741: step 4492, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:47.986014: step 4493, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:48.238826: step 4494, loss 2.63175e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:48.473753: step 4495, loss 2.51455e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:48.706758: step 4496, loss 0.00424362, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:48.940648: step 4497, loss 0.118265, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:43:49.175919: step 4498, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:49.428635: step 4499, loss 0.0289634, acc 0.984375, precision 1, recall 0.964286\n",
      "temporal 2020-10-10T22:43:49.683135: step 4500, loss 0, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:43:50.876418: step 4500, loss 11.2189, acc 0.592734, precision 0.130152, recall 0.705882, F1 0.21978\n",
      "\n",
      "temporal 2020-10-10T22:43:51.099259: step 4501, loss 0.321715, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:43:51.333831: step 4502, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:51.567344: step 4503, loss 0.193392, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:43:51.804432: step 4504, loss 4.43304e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:52.037651: step 4505, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:52.273767: step 4506, loss 4.24439e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:52.511731: step 4507, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:52.769444: step 4508, loss 3.16649e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:53.004964: step 4509, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:53.238787: step 4510, loss 0.000960296, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:53.472182: step 4511, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:53.673283: step 4512, loss 1.8378e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:53.908289: step 4513, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:54.142120: step 4514, loss 0.0110616, acc 0.984375, precision 1, recall 0.962963\n",
      "temporal 2020-10-10T22:43:54.373854: step 4515, loss 0.00452285, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:54.648562: step 4516, loss 0.106737, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:43:54.885524: step 4517, loss 3.96134e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:55.119704: step 4518, loss 0.0864311, acc 0.984375, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:43:55.356932: step 4519, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:55.590093: step 4520, loss 0.000589021, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:55.848675: step 4521, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:56.102653: step 4522, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:56.335902: step 4523, loss 5.21533e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:56.594265: step 4524, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:56.851517: step 4525, loss 0.00196187, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:57.085299: step 4526, loss 2.01153e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:57.341050: step 4527, loss 0.020649, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:43:57.577411: step 4528, loss 1.05795e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:57.837782: step 4529, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:58.086438: step 4530, loss 0.267644, acc 0.96875, precision 0.941176, recall 1\n",
      "temporal 2020-10-10T22:43:58.344457: step 4531, loss 0.097522, acc 0.984375, precision 0.971429, recall 1\n",
      "temporal 2020-10-10T22:43:58.577893: step 4532, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:58.810665: step 4533, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:59.041342: step 4534, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:59.278470: step 4535, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:59.480940: step 4536, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:59.716188: step 4537, loss 3.10232e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:43:59.948596: step 4538, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:00.200740: step 4539, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:00.434028: step 4540, loss 0.00227225, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:00.693129: step 4541, loss 0.00124414, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:00.949787: step 4542, loss 3.23327e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:01.183228: step 4543, loss 7.43181e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:01.414421: step 4544, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:01.657011: step 4545, loss 2.08382e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:01.890717: step 4546, loss 0.00119087, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:02.144221: step 4547, loss 2.53318e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:02.380578: step 4548, loss 0.00121268, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:02.626042: step 4549, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:02.866187: step 4550, loss 1.80676e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:03.102991: step 4551, loss 0.00807429, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:03.356190: step 4552, loss 0.849254, acc 0.953125, precision 1, recall 0.918919\n",
      "temporal 2020-10-10T22:44:03.615080: step 4553, loss 0.362619, acc 0.96875, precision 0.971429, recall 0.971429\n",
      "temporal 2020-10-10T22:44:03.874198: step 4554, loss 0.00442124, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:04.112807: step 4555, loss 4.39578e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:04.342789: step 4556, loss 4.53826e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:04.576229: step 4557, loss 1.30385e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:04.814284: step 4558, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:05.055357: step 4559, loss 2.79397e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:05.285483: step 4560, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:05.518354: step 4561, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:05.776512: step 4562, loss 7.18965e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:06.025237: step 4563, loss 0.490289, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:44:06.282526: step 4564, loss 0.245812, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:44:06.541501: step 4565, loss 0.000449109, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:06.776494: step 4566, loss 0.08758, acc 0.984375, precision 0.972973, recall 1\n",
      "temporal 2020-10-10T22:44:07.009816: step 4567, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:07.287735: step 4568, loss 4.28722e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:07.520973: step 4569, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:07.754394: step 4570, loss 0.269287, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:44:07.985750: step 4571, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:08.215302: step 4572, loss 0.0842283, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:44:08.446895: step 4573, loss 0.0907684, acc 0.96875, precision 1, recall 0.939394\n",
      "temporal 2020-10-10T22:44:08.679566: step 4574, loss 2.62983e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:08.911164: step 4575, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:09.146489: step 4576, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:09.404395: step 4577, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:09.656707: step 4578, loss 3.53902e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:09.889922: step 4579, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:10.122720: step 4580, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:10.355145: step 4581, loss 4.88005e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:10.589936: step 4582, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:10.823566: step 4583, loss 6.29567e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:11.025632: step 4584, loss 9.93411e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:11.257179: step 4585, loss 9.36882e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:11.487015: step 4586, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:11.745356: step 4587, loss 7.59221e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:12.034256: step 4588, loss 1.43116e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:12.270047: step 4589, loss 6.55644e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:12.522958: step 4590, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:12.779606: step 4591, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:13.014768: step 4592, loss 2.79397e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:13.251338: step 4593, loss 1.3411e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:13.483861: step 4594, loss 5.33387e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:13.719305: step 4595, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:13.974987: step 4596, loss 0.082393, acc 0.984375, precision 0.973684, recall 1\n",
      "temporal 2020-10-10T22:44:14.208716: step 4597, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:14.439473: step 4598, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:14.691580: step 4599, loss 0.31253, acc 0.96875, precision 0.972973, recall 0.972973\n",
      "temporal 2020-10-10T22:44:14.926676: step 4600, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:44:16.186096: step 4600, loss 9.55501, acc 0.632887, precision 0.130864, recall 0.623529, F1 0.216327\n",
      "\n",
      "temporal 2020-10-10T22:44:16.433783: step 4601, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:16.691576: step 4602, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:16.928676: step 4603, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:17.162893: step 4604, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:17.391521: step 4605, loss 7.45056e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:17.626266: step 4606, loss 1.35973e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:17.879970: step 4607, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:18.079745: step 4608, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:18.310174: step 4609, loss 0.000138007, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:18.560542: step 4610, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:18.815028: step 4611, loss 0.0020598, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:19.049314: step 4612, loss 0.0676447, acc 0.984375, precision 1, recall 0.971429\n",
      "temporal 2020-10-10T22:44:19.279933: step 4613, loss 0.192441, acc 0.96875, precision 1, recall 0.923077\n",
      "temporal 2020-10-10T22:44:19.512037: step 4614, loss 0.0403246, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:44:19.766277: step 4615, loss 4.45918e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:20.003044: step 4616, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:20.258682: step 4617, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:20.495238: step 4618, loss 1.21072e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:20.751555: step 4619, loss 0.269532, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:44:21.004713: step 4620, loss 0.00482526, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:21.259507: step 4621, loss 1.11759e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:21.492876: step 4622, loss 0.0625684, acc 0.984375, precision 0.974359, recall 1\n",
      "temporal 2020-10-10T22:44:21.747924: step 4623, loss 3.02651e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:22.022889: step 4624, loss 0.120615, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:44:22.257520: step 4625, loss 0.072044, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:44:22.489485: step 4626, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:22.720268: step 4627, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:23.031524: step 4628, loss 0.000159161, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:23.265196: step 4629, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:23.498670: step 4630, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:23.733003: step 4631, loss 0.0788494, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:44:23.936997: step 4632, loss 4.96705e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:24.170625: step 4633, loss 2.44005e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:24.403506: step 4634, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:24.639789: step 4635, loss 5.65017e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:24.879863: step 4636, loss 6.79851e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:25.117624: step 4637, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:25.349697: step 4638, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:25.581195: step 4639, loss 0.000382934, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:25.833847: step 4640, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:26.088889: step 4641, loss 4.28408e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:26.349950: step 4642, loss 0.000567068, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:26.608925: step 4643, loss 2.23517e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:26.842451: step 4644, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:27.099988: step 4645, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:27.337691: step 4646, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:27.579108: step 4647, loss 0.0041747, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:27.833937: step 4648, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:28.064277: step 4649, loss 0.00459672, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:28.311927: step 4650, loss 0.00182982, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:28.570753: step 4651, loss 1.27707e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:28.813911: step 4652, loss 0.0290113, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:44:29.047431: step 4653, loss 5.8486e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:29.280870: step 4654, loss 0.019109, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:44:29.551889: step 4655, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:29.778602: step 4656, loss 0.0130275, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:30.060730: step 4657, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:30.293801: step 4658, loss 3.05445e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:30.531326: step 4659, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:30.762980: step 4660, loss 2.96158e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:30.995826: step 4661, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:31.233900: step 4662, loss 3.91155e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:31.488302: step 4663, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:31.747013: step 4664, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:31.981350: step 4665, loss 5.96045e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:32.213039: step 4666, loss 2.80862e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:32.469826: step 4667, loss 5.3582e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:32.704327: step 4668, loss 0.0163005, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:44:32.936367: step 4669, loss 1.07843e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:33.170572: step 4670, loss 0.00284646, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:33.405555: step 4671, loss 3.54798e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:33.661842: step 4672, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:33.895332: step 4673, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:34.151702: step 4674, loss 0.00385078, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:34.402886: step 4675, loss 1.10451e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:34.634265: step 4676, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:34.868395: step 4677, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:35.119193: step 4678, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:35.376723: step 4679, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:35.602918: step 4680, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:35.836569: step 4681, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:36.064202: step 4682, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:36.295977: step 4683, loss 1.01853e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:36.528324: step 4684, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:36.777746: step 4685, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:37.015952: step 4686, loss 0.13007, acc 0.984375, precision 0.969697, recall 1\n",
      "temporal 2020-10-10T22:44:37.244820: step 4687, loss 2.53672e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:37.477837: step 4688, loss 6.29562e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:37.730947: step 4689, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:37.987988: step 4690, loss 0.0763245, acc 0.984375, precision 1, recall 0.97561\n",
      "temporal 2020-10-10T22:44:38.246781: step 4691, loss 0.000274813, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:38.502351: step 4692, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:38.750528: step 4693, loss 0.0579018, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:44:39.004481: step 4694, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:39.249603: step 4695, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:39.497210: step 4696, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:39.750324: step 4697, loss 0.23593, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:44:39.997615: step 4698, loss 0.0310572, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:44:40.229450: step 4699, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:40.460438: step 4700, loss 0, acc 1, precision 1, recall 1\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:44:41.641745: step 4700, loss 10.2274, acc 0.631931, precision 0.137681, recall 0.670588, F1 0.228457\n",
      "\n",
      "temporal 2020-10-10T22:44:41.832159: step 4701, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:42.045385: step 4702, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:42.275736: step 4703, loss 9.31322e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:42.501733: step 4704, loss 0.267425, acc 0.979167, precision 1, recall 0.961538\n",
      "temporal 2020-10-10T22:44:42.758624: step 4705, loss 4.89799e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:43.013147: step 4706, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:43.246856: step 4707, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:43.480700: step 4708, loss 0.115592, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:44:43.735012: step 4709, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:43.991770: step 4710, loss 0.39325, acc 0.96875, precision 0.945946, recall 1\n",
      "temporal 2020-10-10T22:44:44.223364: step 4711, loss 0.622479, acc 0.96875, precision 0.928571, recall 1\n",
      "temporal 2020-10-10T22:44:44.454898: step 4712, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:44.708108: step 4713, loss 0.00145423, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:44.942404: step 4714, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:45.175937: step 4715, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:45.408295: step 4716, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:45.640934: step 4717, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:45.873145: step 4718, loss 2.8274e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:46.109315: step 4719, loss 1.00743e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:46.362245: step 4720, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:46.597393: step 4721, loss 0.00174515, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:46.831536: step 4722, loss 0.127338, acc 0.984375, precision 1, recall 0.972222\n",
      "temporal 2020-10-10T22:44:47.060745: step 4723, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:47.321903: step 4724, loss 0.0594359, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:44:47.556669: step 4725, loss 0.411892, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:44:47.809450: step 4726, loss 0.00146278, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:48.044807: step 4727, loss 0.111817, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:44:48.247257: step 4728, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:48.483664: step 4729, loss 0.200954, acc 0.984375, precision 1, recall 0.966667\n",
      "temporal 2020-10-10T22:44:48.733501: step 4730, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:48.989605: step 4731, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:49.223722: step 4732, loss 0.0310767, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:44:49.458746: step 4733, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:49.692990: step 4734, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:49.928546: step 4735, loss 0.199935, acc 0.984375, precision 0.96875, recall 1\n",
      "temporal 2020-10-10T22:44:50.170127: step 4736, loss 2.57068e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:50.406003: step 4737, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:50.642296: step 4738, loss 0.427251, acc 0.96875, precision 0.935484, recall 1\n",
      "temporal 2020-10-10T22:44:50.941301: step 4739, loss 6.14672e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:51.192660: step 4740, loss 0.025125, acc 0.984375, precision 1, recall 0.96\n",
      "temporal 2020-10-10T22:44:51.426111: step 4741, loss 0.0290172, acc 0.984375, precision 0.962963, recall 1\n",
      "temporal 2020-10-10T22:44:51.663949: step 4742, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:51.912665: step 4743, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:52.146333: step 4744, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:52.384534: step 4745, loss 3.72529e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:52.620079: step 4746, loss 6.51925e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:52.852716: step 4747, loss 2.67825e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:53.101898: step 4748, loss 2.50883e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:53.332512: step 4749, loss 1.65775e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:53.584401: step 4750, loss 0.232061, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:44:53.819046: step 4751, loss 0.0362235, acc 0.984375, precision 1, recall 0.970588\n",
      "temporal 2020-10-10T22:44:54.097440: step 4752, loss 2.25244e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:54.333320: step 4753, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:54.564726: step 4754, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:54.798896: step 4755, loss 7.45058e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:55.066700: step 4756, loss 3.60387e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:55.313208: step 4757, loss 1.97391e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:55.570206: step 4758, loss 0.00629472, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:55.804254: step 4759, loss 5.58793e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:56.036734: step 4760, loss 1.15484e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:56.271975: step 4761, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:56.501608: step 4762, loss 2.45867e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:56.756231: step 4763, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:56.989199: step 4764, loss 0.331378, acc 0.984375, precision 0.972222, recall 1\n",
      "temporal 2020-10-10T22:44:57.221331: step 4765, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:57.468168: step 4766, loss 2.98023e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:57.703713: step 4767, loss 2.0485e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:57.935423: step 4768, loss 0.0345271, acc 0.984375, precision 1, recall 0.96875\n",
      "temporal 2020-10-10T22:44:58.167897: step 4769, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:58.422708: step 4770, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:58.654647: step 4771, loss 1.32247e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:58.886725: step 4772, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:59.118863: step 4773, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:59.348562: step 4774, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:59.583221: step 4775, loss 1.12445e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:44:59.811356: step 4776, loss 8.22878e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:00.042717: step 4777, loss 2.76972e-05, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:00.272710: step 4778, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:00.506443: step 4779, loss 0.0982458, acc 0.984375, precision 1, recall 0.967742\n",
      "temporal 2020-10-10T22:45:00.744010: step 4780, loss 5.58794e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:00.974621: step 4781, loss 1.86265e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:01.208242: step 4782, loss 5.87927e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:01.457419: step 4783, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:01.692141: step 4784, loss 7.2643e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:01.927232: step 4785, loss 0.014136, acc 0.984375, precision 0.966667, recall 1\n",
      "temporal 2020-10-10T22:45:02.165329: step 4786, loss 0.00477287, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:02.423891: step 4787, loss 0.287034, acc 0.984375, precision 0.964286, recall 1\n",
      "temporal 2020-10-10T22:45:02.682345: step 4788, loss 1.25537e-06, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:02.941714: step 4789, loss 0.000171416, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:03.174495: step 4790, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:03.428345: step 4791, loss 0.0784843, acc 0.984375, precision 0.965517, recall 1\n",
      "temporal 2020-10-10T22:45:03.682654: step 4792, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:03.917553: step 4793, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:04.146296: step 4794, loss 0.0253284, acc 0.984375, precision 1, recall 0.969697\n",
      "temporal 2020-10-10T22:45:04.376778: step 4795, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:04.625019: step 4796, loss 0, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:04.857817: step 4797, loss 2.10478e-07, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:05.088690: step 4798, loss 5.58793e-09, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:05.346830: step 4799, loss 6.33298e-08, acc 1, precision 1, recall 1\n",
      "temporal 2020-10-10T22:45:05.549901: step 4800, loss 0.0161996, acc 0.979167, precision 1, recall 0.961538\n",
      "\n",
      "Evaluation:\n",
      "temporal 2020-10-10T22:45:06.729824: step 4800, loss 7.90673, acc 0.708413, precision 0.133333, recall 0.470588, F1 0.207792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "# tf.app.flags.DEFINE_string('f', '', 'kernel')     # 只定义一次，如果下面代码报错，就运行这个\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement,\n",
    "      gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.8))\n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length_arg1 = x_train_arg1_word.shape[1],\n",
    "            sequence_length_arg2 = x_train_arg2_word.shape[1],\n",
    "            word_vocab_size = pretrained_word_embedding.shape[0],\n",
    "            word_embedding_size = pretrained_word_embedding.shape[1],\n",
    "            num_classes = np.array(y_train).shape[1],\n",
    "            filter_sizes = list(map(int, FLAGS.filter_sizes.split(','))),\n",
    "            num_filters = FLAGS.num_filters,\n",
    "            l2_reg_lambda = FLAGS.l2_reg_lambda\n",
    "        )\n",
    "        print(\"sequence_length_arg1 :{}\\nsequence_length_arg2:{}\\nnum_classes:{}\\nword_vocab_size:{}\\nword_embedding_size:{}\\nfilter_sizes:{}\\n\".\n",
    "              format(x_train_arg1_word.shape[1], x_train_arg2_word.shape[1], np.array(y_train).shape[1], \n",
    "                     len(word_vocab), word_embedding_dim, \n",
    "                     list(map(int, FLAGS.filter_sizes.split(\",\")))))\n",
    "        if FLAGS.use_pretrain:\n",
    "            cnn.assign_embedding(sess, pretrained_word_embedding)\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        \n",
    "        # Adam optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name='train_op')\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        # 绘图\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram('{}/grad/hist'.format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar('{}/grad/sparsity'.format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        # 当前路径的绝对路径 + './runs/explict_timestamp'\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, 'runs', 'explicit_' + timestamp))\n",
    "        print('Writing to {}\\n'.format(out_dir))\n",
    "        \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar('loss', cnn.loss)\n",
    "        acc_summary = tf.summary.scalar('accuracy', cnn.accuracy)\n",
    "        \n",
    "        # Train summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summary\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, 'summaries', 'dev')\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Checkpoint directory, Tensorflow assumes this directory already exists so we need to create it \n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"explicit-model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        \n",
    "        # Write vocabulary\n",
    "        # vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def train_step(x_batch_arg1_word, x_batch_arg2_word, y_batch):\n",
    "            '''\n",
    "            A single training step\n",
    "            '''\n",
    "            feed_dict = {\n",
    "                cnn.input_x_arg1_word: x_batch_arg1_word,\n",
    "                cnn.input_x_arg2_word: x_batch_arg2_word,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy, precision, recall = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.precision, cnn.recall],\n",
    "                feed_dict\n",
    "            )\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"temporal {}: step {}, loss {:g}, acc {:g}, precision {:g}, recall {:g}\"\n",
    "                  .format(time_str, step, loss, accuracy, precision, recall))\n",
    "            \n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch_arg1_word, x_batch_arg2_word, y_batch, writer=None):\n",
    "            '''\n",
    "            Evaluate model on a dev set\n",
    "            '''\n",
    "            feed_dict = {\n",
    "              cnn.input_x_arg1_word: x_batch_arg1_word,\n",
    "              cnn.input_x_arg2_word: x_batch_arg2_word,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy, precision, recall = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.precision, cnn.recall],\n",
    "                feed_dict)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            print(\"temporal {}: step {}, loss {:g}, acc {:g}, precision {:g}, recall {:g}, F1 {:g}\"\n",
    "                  .format(time_str, step, loss, accuracy, precision, recall, f1))\n",
    "            with open(result_path, \"a+\") as w_f:\n",
    "                w_f.write(\"temporal {}: step {}, loss {:g}, acc {:g}, precision {:g}, recall {:g}, F1 {:g}\"\n",
    "                          .format(time_str, step, loss, accuracy, precision, recall, f1)+\"\\n\")\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return f1\n",
    "            \n",
    "        # Generate batches\n",
    "        batches = batch_iter(list(zip(x_train_arg1_word, x_train_arg2_word, y_train)), \n",
    "                             FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        \n",
    "        max_f1 = 0.0\n",
    "        for batch in batches:\n",
    "            \n",
    "            x_batch_arg1_word, x_batch_arg2_word, y_batch = zip(*batch)\n",
    "            \n",
    "            train_step(x_batch_arg1_word,x_batch_arg2_word, y_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                f1 = dev_step(x_test_arg1_word, x_test_arg2_word, y_test, writer=dev_summary_writer)\n",
    "                if f1 > max_f1:\n",
    "                    max_f1 = f1\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "                    with open(result_path, \"a+\") as w_f:\n",
    "                        w_f.write(\"max_f1=\" + str(max_f1))\n",
    "                        \n",
    "                print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T10:28:21.821387Z",
     "start_time": "2020-10-10T10:28:21.738723Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T12:34:45.510116Z",
     "start_time": "2020-10-09T12:34:45.503823Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "DiscourseRelation/CNN_Text_Classification2_base/CNN_Text_Classification2_base/CNN_Text_Classification2_base.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [conda env:TF1]",
   "language": "python",
   "name": "conda-env-TF1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "142.997px",
    "width": "193.991px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.1px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "497px",
    "left": "946px",
    "right": "20px",
    "top": "87px",
    "width": "581px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
